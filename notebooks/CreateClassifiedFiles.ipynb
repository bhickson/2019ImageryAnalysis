{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LULC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chime extension is already loaded. To reload it, use:\n",
      "  %reload_ext chime\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext chime\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "import chime\n",
    "\n",
    "import rasterio as rio\n",
    "import gdal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from skimage.morphology import binary_closing, binary_opening, square, remove_small_holes, disk, square\n",
    "#from Functions import readResampledWindow, getSentinelBandFile\n",
    "\n",
    "from skimage.filters.rank import modal, mean_bilateral\n",
    "from datetime import datetime, timedelta, time\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import psutil\n",
    "import nbimporter\n",
    "#from KeyFunctions import *\n",
    "import chime\n",
    "from utils import *\n",
    "from skimage.morphology import binary_closing, binary_opening, square, remove_small_holes, disk, square, remove_small_objects\n",
    "\n",
    "import richdem as rd\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "def throttleProcessors(workDayStart, workDayEnd):\n",
    "    startPause = time(*(map(int, workDayStart.split(':'))))\n",
    "    endPause = time(*(map(int, workDayEnd.split(':'))))\n",
    "    nowTime = datetime.today().time()\n",
    "    if nowTime < endPause or nowTime > startPause:\n",
    "        print(f\"Current time is {nowTime}. Setting processor/thread use to 4\")\n",
    "        return 4\n",
    "    else:\n",
    "        return 8\n",
    "\n",
    "\n",
    "def createClassifiedRaster(classification_model, ortho_file, classifiedFiles_loc, suffix=None, binaryClass=None, overwrite=False):\n",
    "        try:\n",
    "\n",
    "            start = datetime.now()\n",
    "            if suffix == None:\n",
    "                suffix = datetime.now().strftime(\"%Y%m%d\")\n",
    "            daynum = 1\n",
    "            output_image = os.path.join(classifiedFiles_loc, os.path.basename(ortho_file))#_{day}-{daynum}.tif\"))\n",
    "            if binaryClass:\n",
    "                #print(f\"Setting binary output ({output_image})\")\n",
    "                #output_image = output_image.replace(\".tif\", f\"_{binaryClass}BinaryLGBNewishGBLM.tif\")\n",
    "                output_image = output_image.replace(\".tif\", f\"_{binaryClass}BinaryOptunaTunerGBLM_{suffix}.tif\")\n",
    "            else:\n",
    "                output_image = output_image.replace(\".tif\", f\"_MultiClassOptunaTunerGBLM_{suffix}_v2.tif\")\n",
    "\n",
    "            if os.path.exists(output_image) and not overwrite:\n",
    "                #print(f\"File exists ({output_image})\")\n",
    "                return output_image\n",
    "            #else:\n",
    "            #    shouldPause()\n",
    "\n",
    "            try:\n",
    "                with rio.open(ortho_file) as src:\n",
    "                    kwargs = src.profile\n",
    "                    data = src.read()\n",
    "                    descs = list(src.descriptions)\n",
    "            except:\n",
    "                print(f\"ERROR: Unable to open {os.path.basename(ortho_file)}. Skipping\")\n",
    "                return None\n",
    "\n",
    "            if binaryClass:\n",
    "                feature_names = classification_model.feature_name()\n",
    "                dtype = rio.float32\n",
    "            else:\n",
    "                feature_names = classification_model.feature_name_\n",
    "                dtype = rio.uint8\n",
    "\n",
    "            bands = {desc:data[ib] for ib, desc in enumerate(descs)}\n",
    "\n",
    "            #This is a training dataset not created with the others, but may be in the classifier. Create and add\n",
    "            if \"RGBNmean\" in feature_names:\n",
    "                #print(\"Adding RGBNmean\")\n",
    "                rgbnMean = np.nanmean(data[:4], axis=0).astype(data.dtype)\n",
    "                bands[\"RGBNmean\"] = rgbnMean\n",
    "\n",
    "            if \"RED_LHE\" in feature_names or \"GREEN_LHE\" in feature_names or \"BLUE_LHE\" in feature_names or \"NIR_LHE\" in feature_names:\n",
    "                path = os.path.basename(ortho_file).split(\"_\")[0]\n",
    "                row = os.path.basename(ortho_file).split(\"_\")[1]\n",
    "                print(f\"PATH: {path}, ROW: {row}\")\n",
    "                localHisto_file = findFile(path, row, histoEqual_loc)\n",
    "                with rio.open(localHisto_file) as src:\n",
    "                    for i, desc in enumerate(src.descriptions):\n",
    "                        bands[desc] = src.read(i+1)\n",
    "\n",
    "            if \"REDnessNorm\" in feature_names or \"GREENnessNorm\" in feature_names or \"BLUEnessNorm\" in feature_names or \"NIRnessNorm\" in feature_names:\n",
    "                bands[\"REDnessNorm\"] = calcNessNorm(ortho_file, target_band=\"RED\")\n",
    "                bands[\"GREENnessNorm\"] = calcNessNorm(ortho_file, target_band=\"GREEN\")\n",
    "                bands[\"BLUEnessNorm\"] = calcNessNorm(ortho_file, target_band=\"BLUE\")\n",
    "                bands[\"NIRnessNorm\"] = calcNessNorm(ortho_file, target_band=\"NIR\")\n",
    "\n",
    "            for name in feature_names:\n",
    "                #if \"Sentinel2_\" in name:\n",
    "                if name.startswith(\"Sentinel2_\"):\n",
    "                    sentBand = name.replace(\"Sentinel2_\", \"\")\n",
    "                    #sentBand = name.split(\"_\")[-1]\n",
    "                    #print(f\"Getting sentinel {name} ({sentBand})\")\n",
    "                    sentFile = getSentinelBandFile(sentBand, sentinelData_dir, suffix=\"_2868\")\n",
    "                    bands[name] = readResampledWindow(sentFile, ortho_file, returnData=True)\n",
    "            #return bands\n",
    "            output = createClassificationFromModel(classification_model, bands, feature_names)\n",
    "\n",
    "            #output = np.rint(output)\n",
    "            # create our final mask\n",
    "            #mask = (~m.mask[:,0]).reshape(*img_swp.shape[:-1])\n",
    "\n",
    "            kwargs.update(dtype=dtype, count=1)\n",
    "            with rio.open(output_image, 'w', **kwargs) as dst: \n",
    "                # write to the final file\n",
    "                #dst.write(output.astype(rio.uint8), 1)\n",
    "                dst.write(output.astype(dtype), 1)\n",
    "                #dst.write_mask(mask)\n",
    "                if not binaryClass:\n",
    "                    colors = {\n",
    "                        1: (12,42,235, 255),\n",
    "                        2: (41, 210, 219,255),\n",
    "                        3: (255, 214, 117, 255),\n",
    "                        4: (171, 224, 85, 255),\n",
    "                        5: (12, 100, 1, 255),\n",
    "                        6: (0, 192, 32, 255),\n",
    "                        7: (62, 62, 62, 255),\n",
    "                        8: (160, 160, 160, 255),\n",
    "                        9: (160, 37, 6, 255)\n",
    "                        }\n",
    "                    dst.write_colormap(1, colors)\n",
    "\n",
    "            #print(\"WROTE IT\")\n",
    "            \"\"\"if binaryClass:\n",
    "                cleaned_loc = os.path.abspath(r\"..\\EPCExtent_30cm\\Orthos_Segmentedv3_Classifiedv3_cleaned\")\n",
    "                cleanedOutput_loc = os.path.join(cleaned_loc, os.path.basename(ortho_file).replace(\".tif\",\"_cleaned.tif\"))\n",
    "                cleanedOutput = cleanIt(output, inData=bands, inClass=binaryClass, inprofile=kwargs)\n",
    "                kwargs.update(nodata=0, dtype=rio.float32)\n",
    "                with rio.open(cleanedOutput_loc, 'w', **kwargs) as dst:\n",
    "                    #dst.write(cleanedOutput.astype(rio.uint8), 1)\n",
    "                    dst.write(cleanedOutput.astype(rio.float32), 1)\n",
    "            \"\"\"\n",
    "            end = datetime.now()\n",
    "            print(f\"Classified to {os.path.abspath(output_image)}. \\nClassification took {end-start} -\\t{end}\")\n",
    "\n",
    "            return output_image\n",
    "        except:\n",
    "            print(f\"Failed for {output_image}\")\n",
    "        \n",
    "\n",
    "def createBinaryClassifiedRaster(models, ortho_file, classifiedFiles_loc, roundConfidence=False, cleanPixels=False, overwrite=False):\n",
    "    try:\n",
    "        start = datetime.now()\n",
    "        day = datetime.now().strftime(\"%Y%m%d\")\n",
    "        daynum = 1\n",
    "        output_file = os.path.basename(ortho_file).replace(\".tif\", f\"_BinaryOptunaTunerGBLM.tif\")\n",
    "        output_path = os.path.join(classifiedFiles_loc, output_file)#_{day}-{daynum}.tif\"))\n",
    "        cleaned_loc = os.path.abspath(r\"..\\EPCExtent_30cm\\Orthos_Segmentedv3_Classifiedv3_cleaned\")\n",
    "        cleanedOutput_path = os.path.join(cleaned_loc, output_file.replace(\".tif\",\"_cleaned.tif\"))\n",
    "\n",
    "        if os.path.exists(output_path) and not overwrite:\n",
    "            #print(f\"File exists ({output_path})\")\n",
    "            return output_path\n",
    "        #else:\n",
    "            #shouldPause()\n",
    "\n",
    "        try:\n",
    "            with rio.open(ortho_file) as src:\n",
    "                kwargs = src.profile\n",
    "                kwargs.update(\n",
    "                    dtype= rio.uint8,\n",
    "                    count= 1,\n",
    "                )\n",
    "                data = src.read()\n",
    "                descs = list(src.descriptions)\n",
    "        except:\n",
    "            print(f\"ERROR: Unable to open {os.path.basename(ortho_file)}. Skipping\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            all_feature_names = set( fn for model_path in models.values() for fn in pickle.load(open(model_path, 'rb')).feature_name_ )\n",
    "            #feature_names = classification_model.feature_name_\n",
    "        except:\n",
    "            all_feature_names = set( fn for model_path in models.values() for fn in pickle.load(open(model_path, 'rb')).feature_name() )\n",
    "            #feature_names = classification_model.feature_name()\n",
    "\n",
    "        bands = {desc:data[ib] for ib, desc in enumerate(descs)}\n",
    "\n",
    "        #This is a training dataset not created with the others, but may be in the classifier. Create and add\n",
    "        if \"RGBNmean\" in all_feature_names:\n",
    "            #print(\"Adding RGBNmean\")\n",
    "            rgbnMean = np.nanmean(data[:4], axis=0).astype(data.dtype)\n",
    "            bands[\"RGBNmean\"] = rgbnMean\n",
    "\n",
    "        if \"RED_LHE\" in all_feature_names or \"GREEN_LHE\" in all_feature_names or \"BLUE_LHE\" in all_feature_names or \"NIR_LHE\" in feature_names:\n",
    "            path = os.path.basename(ortho_file).split(\"_\")[0]\n",
    "            row = os.path.basename(ortho_file).split(\"_\")[1]\n",
    "            print(f\"PATH: {path}, ROW: {row}\")\n",
    "            localHisto_file = findFile(path, row, histoEqual_loc)\n",
    "            with rio.open(localHisto_file) as src:\n",
    "                for i, desc in enumerate(src.descriptions):\n",
    "                    bands[desc] = src.read(i+1)\n",
    "\n",
    "        if \"REDnessNorm\" in all_feature_names or \"GREENnessNorm\" in all_feature_names or \"BLUEnessNorm\" in all_feature_names or \"NIRnessNorm\" in all_feature_names:\n",
    "            bands[\"REDnessNorm\"] = calcNessNorm(ortho_file, target_band=\"RED\")\n",
    "            bands[\"GREENnessNorm\"] = calcNessNorm(ortho_file, target_band=\"GREEN\")\n",
    "            bands[\"BLUEnessNorm\"] = calcNessNorm(ortho_file, target_band=\"BLUE\")\n",
    "            bands[\"NIRnessNorm\"] = calcNessNorm(ortho_file, target_band=\"NIR\")\n",
    "\n",
    "        for name in all_feature_names:\n",
    "            #if \"Sentinel2_\" in name:\n",
    "            if name.startswith(\"Sentinel2_\"):\n",
    "                sentBand = name.replace(\"Sentinel2_\", \"\")\n",
    "                #sentBand = name.split(\"_\")[-1]\n",
    "                #print(f\"Getting sentinel {name} ({sentBand})\")\n",
    "                sentFile = getSentinelBandFile(sentBand, sentinelData_dir, suffix=\"_2868\")\n",
    "                bands[name] = readResampledWindow(sentFile, ortho_file, returnData=True)\n",
    "\n",
    "        if roundConfidence:\n",
    "            dtype = np.uint8\n",
    "        else:\n",
    "            dtype = np.float32\n",
    "        kwargs.update(\n",
    "            dtype = dtype,\n",
    "            count = 9,\n",
    "            nodata = 0\n",
    "        )\n",
    "        #output_path = f\"C:/Users/BenJames/Downloads/{os.path.basename(output_path)}\"\n",
    "        with rio.open(output_path, 'w', **kwargs) as dst: \n",
    "            for i, class_name in enumerate(models):\n",
    "                #if class_name == \"Asphault\" or class_name == \"Pool\" or class_name == \"Barren\" or class_name == \"PondLake\" or class_name == \"Impervious\":\n",
    "                #    continue\n",
    "                #print(f\"on model {class_name}\")\n",
    "                model = pickle.load(open(models[class_name], 'rb'))\n",
    "                confidenceClassification = createClassificationFromModel(model, all_bands=bands)\n",
    "                #if class_name == \"DenseVeg\":\n",
    "                #    return confidenceClassification\n",
    "                # Model provides confidence values from 0 to 1, round to binary if specifid\n",
    "                if roundConfidence:\n",
    "                    confidenceClassification = np.rint(confidenceClassification)\n",
    "                # write to the final file\n",
    "                dst.write(confidenceClassification.astype(dtype), i+1)\n",
    "                dst.set_band_description(i+1, class_name)\n",
    "\n",
    "                if cleanPixels and roundConfidence:\n",
    "                    cleanedOutput = cleanIt(binaryClassification==1, inData=bands, inClass=class_name)\n",
    "                    with rio.open(cleanedOutput_path, \"w\", **kwargs) as cdst:\n",
    "                        cdst.write(cleanedOutput.astype(dtype), i+1)\n",
    "                        cdst.set_band_description(i+1, class_name)\n",
    "\n",
    "        end = datetime.now()\n",
    "        print(f\"Classified to {os.path.abspath(output_path)}. \\nClassification took {end-start} -\\t{end}\")\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        print(f\"Failed on {output_path}. Returning None\")\n",
    "        try:\n",
    "            os.remove(output_path)\n",
    "        except:\n",
    "            print(f\"\\tUnable to remove {output_path}\")\n",
    "        output_path = None\n",
    "            \n",
    "    return output_path\n",
    "\n",
    "def createClassificationFromModel(model, all_bands, model_features=None):\n",
    "    for name, band in all_bands.items():\n",
    "        numnans = len(band[np.isnan(band)])\n",
    "        if numnans != 0:\n",
    "            print(name, numnans)\n",
    "            \n",
    "    if model_features == None:\n",
    "        model_features = model.feature_name()\n",
    "        \n",
    "    features = {fn: all_bands[fn] for fn in model_features}# if fn in descs}\n",
    "    featureArrays = list(features.values())\n",
    "    \n",
    "    trainingFeatures = np.stack(featureArrays)\n",
    "    \n",
    "    #print(\"Input Data shape\", data.shape)\n",
    "    \n",
    "    # read the image into the proper format, adding indices if necessary\n",
    "    img_swp = np.moveaxis(trainingFeatures, 0, -1)\n",
    "    img_flat = img_swp.reshape(-1, img_swp.shape[-1])\n",
    "    \n",
    "    #flatten bands along axis\n",
    "    img_flat = img_swp.reshape(-1, img_swp.shape[-1])\n",
    "    \n",
    "    # remove no data values, store the indices for later use\n",
    "    # a later cell makes the assumption that all bands have identical no-data value arrangements\n",
    "    m = np.ma.masked_invalid(img_flat)\n",
    "    #print(\"Mask Values\", np.unique(m.mask, return_counts=True))\n",
    "    to_predict = img_flat[~m.mask].reshape(-1, img_flat.shape[-1])\n",
    "    #print(\"TO PREDICT SHAPE\", to_predict.shape)\n",
    "    \n",
    "    # predict\n",
    "    #print(\"Beginning prediction...\", datetime.now())\n",
    "    img_preds = model.predict(to_predict)\n",
    "    \n",
    "    #img_preds = [np.argmax(x) for x in img_preds]\n",
    "    \n",
    "    # add the prediction back to the valid pixels (using only the first band of the mask to decide on validity)\n",
    "    # resize to the original image dimensions\n",
    "    output = np.zeros(img_flat.shape[0])\n",
    "    output[~m.mask[:,0]] = img_preds#.flatten()\n",
    "    output = output.reshape(*img_swp.shape[:-1])\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def cleanupStructures(array, value):\n",
    "    structure = array == value\n",
    "    array[structure] = 0\n",
    "    for i in range(3):\n",
    "        structure = binary_opening(structure, square(3))\n",
    "        structure = binary_closing(structure, square(5))\n",
    "    \n",
    "    array[structure] = value\n",
    "    array = fillHolesInClass(array, value, 90)\n",
    "    array = np.where(structure==1, value, array)\n",
    "    \n",
    "    #for i in range(5):\n",
    "    #    print(f\"{i} iteration\"\n",
    "    #non_structure_mode = modal(array, square(21), mask=array==9)\n",
    "    #array = np.where(array==255, non_structure_mode, array)\n",
    "    \n",
    "    return array\n",
    "\n",
    "    \n",
    "def cleanIt(inarray, inData, inClass, inprofile):\n",
    "    if inClass==\"Pool\":\n",
    "        outarray = cleanupPool(inarray, inData)\n",
    "    elif inClass==\"Asphault\":\n",
    "        outarray = cleanupAsphault(inarray, inprofile)\n",
    "    elif inClass==\"DenseVeg\":\n",
    "        outarray = cleanupDenseVeg(inarray, inData)\n",
    "    elif inClass==\"SparseVeg\":\n",
    "        outarray = cleanupSparseVeg(inarray, inData)\n",
    "    elif inClass==\"PondLake\":\n",
    "        outarray = cleanupPondLake(inarray, inData)\n",
    "    else:\n",
    "        outarray = inarray\n",
    "        \n",
    "    return outarray\n",
    "\n",
    "\n",
    "def fillHolesInClass(array, class_num, size_max):\n",
    "    class_bool = array == class_num\n",
    "    array[class_bool] = 255\n",
    "    filled = remove_small_holes(class_bool, area_threshold=size_max, connectivity=1)\n",
    "    array[filled] = class_num\n",
    "    return array\n",
    "\n",
    "\n",
    "def smoothValue(array, value, selem=square(5)):\n",
    "    onlys = array == value\n",
    "    for i in range(2):\n",
    "        onlys = binary_closing(onlys, selem)\n",
    "    array[onlys] = value\n",
    "    #array[~asphault] = 255\n",
    "    return array\n",
    "\n",
    "\n",
    "def cleanupAsphault(a, inprofile, slope):\n",
    "    # 1. Drop where any slope percentage is less than 10 degrees\n",
    "    slope_thres = np.where(slope<10, 0, 1)\n",
    "    slope_thres = remove_small_objects(slope_thres, min_size=1000, connectivity=1)\n",
    "    a[slope_thres==1] = 0\n",
    "    \n",
    "    # 2. Burn in mix-used paths\n",
    "    \n",
    "    # 3. Remove groups of asphault pixels smaller than 1000\n",
    "    a = remove_small_objects(a==1, min_size=1000, connectivity=1)\n",
    "    \n",
    "    # 4. Fill in holes\n",
    "    a = fillHolesInClass(a, 9, 90)\n",
    "    \n",
    "    # 5. Open and Dilate and to smooth edges\n",
    "    for i in range(5):\n",
    "        a = binary_opening(a, square(3))\n",
    "        a = binary_closing(a, square(5))\n",
    "        \n",
    "    return a \n",
    "\n",
    "\n",
    "def cleanupImpervious(a, indata):\n",
    "    a[indata[\"MSAVI\"] > 30000] = 0\n",
    "    \n",
    "    return a\n",
    "\n",
    "\n",
    "def cleanupPool(a, indata):\n",
    "    a = remove_small_objects(a==1, min_size=50, connectivity=1)\n",
    "    a = remove_small_holes(a==1, area_threshold=5, connectivity=1)\n",
    "    for i in range(2):\n",
    "        a = binary_opening(a, disk(2))\n",
    "    \n",
    "    a[indata[\"HAG\"] > 8] = 0\n",
    "    a[indata[\"NIR\"] > 10000] = 0\n",
    "    a[indata[\"BLUEness\"] < 35000] = 0\n",
    "    a[indata[\"Slope\"] > 5] = 0\n",
    "    \n",
    "    return a\n",
    "\n",
    "\n",
    "def cleanupPondLake(a, indata):\n",
    "    # drop ponds/lakes smaller than x?\n",
    "    #a = removeClassSmaller(a, 1, 10000) \n",
    "    ndvi_lim = indata[\"NDVI\"]<12000\n",
    "    ndpi_lim = indata[\"NDPI\"]>26000\n",
    "    blueness_lim = indata[\"BLUEness\"]<=40000\n",
    "    a[ndvi_lim & ndpi_lim & blueness_lim] = 1\n",
    "    a[a==1 & (~ndvi_lim | ~ndpi_lim | ~blueness_lim)] = 0\n",
    "    a[indata[\"Slope\"] >5 ] = 0\n",
    "    a = remove_small_objects(a==1, min_size=5000, connectivity=1)\n",
    "    # smooth ponds\n",
    "    a = smoothValue(a, 1, selem=disk(5))\n",
    "    \n",
    "    return a\n",
    "\n",
    "\n",
    "def cleanupIrrigated(a, value, indata=None):\n",
    "     # set very green veg with high index and low brightness to irrigated\n",
    "    a = remove_small_holes(a==1, area_threshold=5, connectivity=1)\n",
    "    a = smoothValue(a, value, selem=disk(5))\n",
    "    a[indata[\"MSAVI\"]<=30000] = 0\n",
    "    \n",
    "    return a\n",
    "        \n",
    "def cleanupDenseVeg(a, value):\n",
    "    a = smoothValue(a, value, selem=disk(2)) # smooth dense veg\n",
    "    return a\n",
    "\n",
    "\n",
    "def cleanupSparseVeg(a, value):\n",
    "    a = smoothValue(a, value, selem=square(5)) # smooth sparse veg\n",
    "    return a\n",
    "\n",
    "\n",
    "def getSlopeArray(tprofile):\n",
    "    with rio.open(r\"R:\\ProjectData\\PAG2019\\2ftDEMs_PAG\\EPC_DEM_2015.vrt\") as src:\n",
    "        intrans = tprofile[\"transform\"]\n",
    "        inheight = tprofile[\"height\"]\n",
    "        inwidth = tprofile[\"width\"]\n",
    "        inres = intrans.a\n",
    "        win = from_bounds(intrans.c, intrans.f-(inheight*inres), intrans.c+(inwidth*inres), intrans.f, src.transform).round_offsets().round_lengths()\n",
    "        \n",
    "        if win.col_off < src.width and win.row_off < src.height:\n",
    "            dem = src.read(1, window=win, out_shape=(tprofile[\"height\"], tprofile[\"width\"]))\n",
    "            slope = rd.TerrainAttribute(rd.rdarray(dem, no_data=src.nodata), attrib=\"slope_degrees\")\n",
    "            # if the read window includes an area with no-data, that means that it's outside of the region for the 2015 2ft DEM. Fill in with NED data\n",
    "            if src.nodata in dem:\n",
    "                slope10m = get10mNEDSlope(intrans,inheight,inwidth)\n",
    "                slope = np.where(dem==src.nodata, slope10m, slope)\n",
    "        else:\n",
    "            # the entire window is outside of the extent of the 2ft DEM coverage; use NED\n",
    "            slope = get10mNEDSlope(intrans,inheight,inwidth)\n",
    "            \n",
    "    return slope\n",
    "\n",
    "\n",
    "def get10mNEDSlope(intrans, inheight, inwidth):\n",
    "    inres = intrans.a\n",
    "    with rio.open(rasterDataDir + r\"/10mDEMs/DEM10mNED_slope.tif\") as src10m:\n",
    "        win = from_bounds(intrans.c, intrans.f-(inheight*inres), intrans.c+(inwidth*inres), intrans.f, src10m.transform).round_offsets().round_lengths()\n",
    "        slope = src10m.read(1, window=win, out_shape=(inheight, inwidth))\n",
    "        \n",
    "    return slope\n",
    "\n",
    "\n",
    "def burnShadows(a, indata, rednessNorm):\n",
    "    # burn in shadows where low values, but not pool or pond\n",
    "    rgbnMean = np.nanmean([indata[\"RED\"], indata[\"GREEN\"], indata[\"BLUE\"], indata[\"NIR\"]], axis=0)\n",
    "    \n",
    "    #a[(rgbnMean<10282) & (a!=1) & (a!=2)] = 10\n",
    "    \n",
    "    # if classed as pool, NIR is below 5k and rednessNorm > 0.15\n",
    "    a[(a==1) & (indata[\"NIR\"]<=5000) & (rednessNorm > 0.15)] = 10\n",
    "    \n",
    "    return a\n",
    "\n",
    "\n",
    "def getBandByDescription(file, bandDesc):\n",
    "    if type(bandDesc) == list:\n",
    "        getBands = bandDesc[:]\n",
    "    elif type(bandDesc) == str:\n",
    "        getBands = [bandDesc]\n",
    "        \n",
    "    bandDict = {}\n",
    "    with rio.open(file) as src:\n",
    "        for band in getBands:\n",
    "            try:\n",
    "                index = src.descriptions.index(band)\n",
    "            except:\n",
    "                raise ValueError(f\"Unable to find band {band} in {file}\")    \n",
    "            # have to increment index since tiff indicies start at 1 and not 0\n",
    "            bandDict[band] = src.read(index+1)\n",
    "            \n",
    "    if type(bandDesc) != list:\n",
    "        return bandDict[bandDesc]\n",
    "    \n",
    "    return bandDict\n",
    "\n",
    "\n",
    "def calcNessNorm(file, target_band):\n",
    "    bands = getBandByDescription(file, bandDesc=[\"RED\", \"GREEN\", \"BLUE\", \"NIR\"])\n",
    "    #return bands\n",
    "    tband = bands[target_band]\n",
    "    obands = np.stack([array for name, array in bands.items() if name != target_band])\n",
    "    nessNorm = (tband-np.nanmean(obands,axis=0))/(tband+np.nanmean(obands,axis=0))\n",
    "    mask = np.all(np.stack(bands.values())==0, axis=0)\n",
    "    nessNorm[mask]=0\n",
    "    \n",
    "    return nessNorm\n",
    "\n",
    "\n",
    "def calcRGDiff(file):\n",
    "    bands = getBandByDescription(file, bandDesc=[\"RED\", \"GREEN\"])\n",
    "    rgDiff = bands[\"RED\"]-bands[\"GREEN\"]\n",
    "    \n",
    "    return rgDiff\n",
    "\n",
    "\n",
    "def splitToBinary(array, limit):\n",
    "    data = np.where(array>limit, 1, 0)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\"\"\"\n",
    "def getCleanedBinary(file, orthoFile, outdir, lcClass, probLimit=0.01, overwrite=False, writeOut=False):\n",
    "    ofile = os.path.join(outdir, os.path.basename(file).replace(\".tif\", \"_clean.tif\"))\n",
    "    #if os.path.exists(ofile) and not overwrite and not returnData:\n",
    "    #    return ofile\n",
    "    if os.path.exists(ofile) and not overwrite:\n",
    "        data_clean = rio.open(ofile).read(1)\n",
    "    else:\n",
    "        with rio.open(file) as src:\n",
    "            data = src.read(1)\n",
    "            kwargs = src.profile\n",
    "         \n",
    "        data = splitToBinary(data, probLimit)\n",
    "    \n",
    "        if lcClass == \"Asphault\":\n",
    "            data_clean = cleanupAsphault(data, kwargs)\n",
    "        elif lcClass == \"Pool\":\n",
    "            #rint(\"Cleaning Pool\")\n",
    "            trainingData = getBandByDescription(orthoFile, [\"HAG\",\"NIR\",\"BLUEness\"])\n",
    "            data_clean = cleanupPool(data, trainingData)\n",
    "            #print(\"Cleaned Pool\")\n",
    "        elif lcClass == \"PondLake\":\n",
    "            trainingData = getBandByDescription(orthoFile, [\"NDVI\",\"NDPI\",\"BLUEness\"])\n",
    "            data_clean = cleanupPondLake(data, trainingData)\n",
    "        elif lcClass == \"Irrigated\":\n",
    "            trainingData = getBandByDescription(orthoFile, [\"MSAVI\"])\n",
    "            data_clean = cleanupIrrigated(data, trainingData)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown class type {lcClass}\")\n",
    "    \n",
    "        if writeOut:\n",
    "            kwargs.update(dtype=np.uint8, nodata=0)\n",
    "            with rio.open(ofile, \"w\", nbits=1, **kwargs) as dst:\n",
    "                dst.write(data_clean.astype(np.uint8), 1)\n",
    "            return ofile\n",
    "    \n",
    "    return data_clean.astype(np.uint8)\n",
    "\"\"\"\n",
    "                      \n",
    "def buildCleanLULC(rowIndex, r, outdir, overwrite=False):\n",
    "    try:\n",
    "        #shouldPause()\n",
    "        \n",
    "        start = datetime.now()\n",
    "        ofile = f\"{r.path}_{r.row}_TrainingStackV3_BinaryStack_cleaned.tif\"\n",
    "        output_lulc = os.path.join(outdir, ofile)\n",
    "        if os.path.exists(output_lulc) and not overwrite:\n",
    "            return output_lulc\n",
    "        \n",
    "        with rio.open(r.OrthoFile) as src:\n",
    "            kwargs = src.profile\n",
    "            \n",
    "        #asphault = getCleanedBinary(r.AsphaultFile, r.OrthoFile, cleanBinaryDir, \"Asphault\", writeOut=False)\n",
    "        #pool = getCleanedBinary(r.PoolFile, r.OrthoFile, cleanBinaryDir, \"Pool\", writeOut=False)\n",
    "        #pondLake = getCleanedBinary(r.PondLakeFile, r.OrthoFile, cleanBinaryDir, \"PondLake\", probLimit=0.5, writeOut=False)\n",
    "        \n",
    "        binaryFile = r.BinaryStackFile\n",
    "        \n",
    "        binaryClassifications = getBandByDescription(binaryFile, list(values_2019_lookup.keys()))\n",
    "        \n",
    "        pondLake = splitToBinary(binaryClassifications[\"PondLake\"], limit=0.5)\n",
    "        pool = splitToBinary(binaryClassifications[\"Pool\"], limit=0.1)\n",
    "        sparseVeg = splitToBinary(binaryClassifications[\"SparseVeg\"], limit=0.5)\n",
    "        denseVeg = splitToBinary(binaryClassifications[\"DenseVeg\"], limit=0.5)\n",
    "        irrigatedLand = splitToBinary(binaryClassifications[\"IrrigatedLand\"], limit=0.5)\n",
    "        asphault = splitToBinary(binaryClassifications[\"Asphault\"], limit=0.1)\n",
    "        structures = splitToBinary(binaryClassifications[\"Structure\"], limit=0.5)\n",
    "        \n",
    "        impervious = binaryClassifications[\"Impervious\"]\n",
    "        barren = binaryClassifications[\"Barren\"]\n",
    "        \n",
    "        inputData = getBandByDescription(r.OrthoFile, [\"RED\", \"GREEN\", \"BLUE\", \"NIR\", \"MSAVI\", \"HAG\", \"BLUEness\", \"NDVI\", \"NDPI\"])\n",
    "        inputData[\"Slope\"] = getSlopeArray(kwargs)\n",
    "        \n",
    "        asphault = cleanupAsphault(asphault, kwargs, inputData[\"Slope\"])\n",
    "        pool = cleanupPool(pool, inputData)\n",
    "        pondLake = cleanupPondLake(pondLake, inputData)\n",
    "        irrigatedLand = cleanupIrrigated(irrigatedLand, 1, inputData)\n",
    "        structures = cleanupStructures(structures, 1)\n",
    "        sparseVeg = cleanupSparseVeg(sparseVeg, 1)\n",
    "        denseVeg = cleanupDenseVeg(denseVeg, 1)\n",
    "        impervious = cleanupImpervious(impervious, inputData)\n",
    "        \n",
    "        lulc_array = np.zeros(asphault.shape, dtype=np.uint8)\n",
    "        \n",
    "        lulc_array[structures == 1] = values_2019_lookup[\"Structure\"]\n",
    "        lulc_array[pool == 1] = values_2019_lookup[\"Pool\"]\n",
    "        lulc_array[pondLake == 1] = values_2019_lookup[\"PondLake\"]\n",
    "        lulc_array[irrigatedLand == 1] = values_2019_lookup[\"IrrigatedLand\"]\n",
    "        lulc_array[sparseVeg == 1] = values_2019_lookup[\"SparseVeg\"]\n",
    "        lulc_array[asphault == 1] = values_2019_lookup[\"Asphault\"]\n",
    "        lulc_array[denseVeg == 1] = values_2019_lookup[\"DenseVeg\"]\n",
    "        \n",
    "        #with rio.open(r.ImperviousFile) as src:\n",
    "        #    impervious = src.read(1)\n",
    "        #with rio.open(r.BarrenFile) as src:\n",
    "        #barren = src.read(1)\n",
    "            \n",
    "        # stack barren and impervious and pick class with best confidence (highest/argmax)\n",
    "        barrenImp = np.stack([impervious, barren])\n",
    "        bestIndex = np.argmax(barrenImp, axis=0)\n",
    "        lulc_array[(lulc_array==0) & (bestIndex==0)] = values_2019_lookup[\"Impervious\"]\n",
    "        lulc_array[(lulc_array==0) & (bestIndex==1)] = values_2019_lookup[\"Barren\"]\n",
    "        \n",
    "        # cleanup Barren and Impervious\n",
    "        redNessNorm = calcNessNorm(r.OrthoFile, target_band=\"RED\")\n",
    "        blueNessNorm = calcNessNorm(r.OrthoFile, target_band=\"BLUE\")\n",
    "        rbNessDiff = redNessNorm - blueNessNorm\n",
    "        rgDiff = calcRGDiff(r.OrthoFile)\n",
    "        #  These cutoffs are kinda arbitrary based on limited sampling\n",
    "        lulc_array[(lulc_array==values_2019_lookup[\"Impervious\"]) & (rbNessDiff>=0.1) & (rgDiff>=2500)] = values_2019_lookup[\"Barren\"]\n",
    "        lulc_array[(lulc_array==values_2019_lookup[\"Impervious\"]) & (rbNessDiff>=0.06) & (rgDiff<2500)] = values_2019_lookup[\"Barren\"]\n",
    "        lulc_array[(lulc_array==values_2019_lookup[\"Barren\"]) & (rbNessDiff<=0.1) & (rgDiff<2500)] = values_2019_lookup[\"Impervious\"]\n",
    "        \n",
    "        # burn in shadows where low values, but not pool or pond\n",
    "        lulc_array = burnShadows(lulc_array, inputData, redNessNorm)\n",
    "        \n",
    "        \n",
    "        \n",
    "        kwargs.update(count=1, dtype=np.uint8, nodata=0)\n",
    "        with rio.open(output_lulc, 'w', **kwargs) as dst: \n",
    "            # write to the final file\n",
    "            dst.write(lulc_array, 1)\n",
    "            colors = {\n",
    "                1: (12,42,235, 255),\n",
    "                2: (41, 210, 219,255),\n",
    "                3: (255, 214, 117, 255),\n",
    "                4: (171, 224, 85, 255),\n",
    "                5: (12, 100, 1, 255),\n",
    "                6: (0, 192, 32, 255),\n",
    "                7: (62, 62, 62, 255),\n",
    "                8: (160, 160, 160, 255),\n",
    "                9: (160, 37, 6, 255),\n",
    "                10: (0, 0, 0, 255)\n",
    "                }\n",
    "            dst.write_colormap(1, colors)\n",
    "        print(f\"Finished with #{rowIndex} - {output_lulc}\")\n",
    "        return output_lulc\n",
    "    \n",
    "    except ValueError as e:\n",
    "        print(f\"Failed for {r.OrthoFile}\\n{e}\")\n",
    "        \n",
    "def shouldPause(pauseTime_sec=60, memLimit_perc=50, cpuLimit_perc=80, workStart_hr=9, workEnd_hr=17):\n",
    "    wait = True\n",
    "    while wait:\n",
    "        now_hr=datetime.now().hour\n",
    "        memUsage = psutil.virtual_memory()[2]\n",
    "        cpuUsage = psutil.cpu_percent(interval=0.1)\n",
    "        if (memUsage > memLimit_perc or cpuUsage > cpuLimit_perc) and (now_hr >= workStart_hr or now_hr <=workEnd_hr):\n",
    "            print(f\"sleeping ~{pauseTime_sec}\")\n",
    "            sleeptime = randint(pauseTime_sec-5, pauseTime_sec+5)\n",
    "            sleep(sleeptime)\n",
    "        else:\n",
    "            wait=False\n",
    "            \n",
    "            \n",
    "def getnjobs():\n",
    "    now = datetime.now()\n",
    "    if now.hour >= 9 and now.hour < 17:\n",
    "        njobs = 1\n",
    "    else:\n",
    "        njobs = 2\n",
    "    return njobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BenJames\\anaconda3\\envs\\geospatial\\lib\\site-packages\\geopandas\\geodataframe.py:294: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for f in features_lst:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4846 total tile indicies\n"
     ]
    }
   ],
   "source": [
    "rasterDataDir = os.path.abspath(\"R:/ProjectData/PAG2019\")\n",
    "classifiedFiles_loc = os.path.join(rasterDataDir, \"EPCExtent_30cm/Orthos_Segmented_Classifiedv3\")\n",
    "binaryClassifiedFiles_loc = os.path.join(rasterDataDir, \"EPCExtent_30cm/Orthos_Segmented_Classifiedv3_Binary\")\n",
    "histoEqual_loc = os.path.join(rasterDataDir, \"EPCExtent_30cm/Orthos_LocalHistogramEqualized\")\n",
    "cleanBinaryDir = os.path.join(rasterDataDir, r\"EPCExtent_30cm/Orthos_Segmented_Classifiedv3_Binary_cleaned\")\n",
    "v3TrainingStack_dir = os.path.join(rasterDataDir, r\"EPCExtent_30cm\\Orthos_Segmentedv3\")\n",
    "sentinelData_dir = os.path.join(rasterDataDir, r\"Sentinel2Data\")\n",
    "\n",
    "ortho30cmvrt_loc = os.path.join(rasterDataDir, \"EPCExtent_30cm/Orthos/EPC_30cmOrtho_2019.vrt\")\n",
    "hagvrt_loc = os.path.join(rasterDataDir, \"EPCExtent_30cm/Elevation_80cmNPS/HAG_2015/HAG_2015.vrt\")\n",
    "\n",
    "orthosDir = os.path.join(rasterDataDir, r\"EPCExtent_30cm/Orthos_Segmentedv3\")\n",
    "os.makedirs(classifiedFiles_loc, exist_ok=True)\n",
    "os.makedirs(binaryClassifiedFiles_loc, exist_ok=True)\n",
    "os.makedirs(cleanBinaryDir, exist_ok=True)\n",
    "\n",
    "values_2019_lookup = {'PondLake': 1, 'Pool': 2, 'Barren': 3, 'SparseVeg': 4, 'DenseVeg': 5,\n",
    "                      'IrrigatedLand': 6, 'Asphault': 7, 'Impervious': 8, 'Structure': 9}\n",
    "\n",
    "tindex = gpd.read_file(\"../vectors/Ortho_5kSubIndex.gpkg\")\n",
    "#to_process = gpd.read_file(\"../temp/ToProcess.gpkg\")\n",
    "\n",
    "print(f\"{len(tindex)} total tile indicies\")\n",
    "tindex[\"OrthoFile\"] = tindex.apply(lambda r: findFile(path=r.path, row=r.row, directory=orthosDir), axis=1)\n",
    "#tindex[\"PropsFile\"] = tindex.apply(lambda r: findFile(path=r.path, row=r.row, files=propsDir), axis=1)\n",
    "\n",
    "# ignore tiles which don't have input variables created\n",
    "#tindex = tindex[(~pd.isnull(tindex.OrthoFile))]\n",
    "#print(f\"{len(tindex)} tile indicies with training data already built\")\n",
    "\n",
    "\n",
    "#prioritize central tucson and work out from there\n",
    "tindex[\"centroid\"] = tindex.geometry.centroid\n",
    "central_tile = tindex[(tindex.path == \"W1004789\") & (tindex.row == \"N449850\")]\n",
    "central_point = central_tile.centroid.values[0]\n",
    "tindex[\"DistToCenter\"] = tindex.centroid.apply(lambda c: int(c.distance(central_point)))\n",
    "tindex.sort_values(by=\"DistToCenter\", inplace=True)\n",
    "\n",
    "classifiers = {mp.split(\"_\")[0].split(\"Binary\")[-1]:mp for mp in glob(\"../Models/*20220103.sav\")}\n",
    "\n",
    "#tt = tindex[((tindex.path == \"W989789\") & (tindex.row == \"W439850\")) |\n",
    "#            ((tindex.path == \"W1004789\") & (tindex.row == \"W449850\")) |\n",
    "#            ((tindex.path == \"W1004789\") & (tindex.row == \"W344850\")) |\n",
    "#            ((tindex.path == \"W979789\") & (tindex.row == \"W419850\")) |\n",
    "#            ((tindex.path == \"W1004789\") & (tindex.row == \"W444850\")) |\n",
    "#            ((tindex.path == \"W919789\") & (tindex.row == \"W404850\")) \n",
    "#           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.63011363636363"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3245 tiles within 32 miles\n",
      "254 tiles with LULC files to be created\n",
      "250 tiles (98.43%) that need training data created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x21cfa139c10>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARwAAAEJCAYAAABG75jIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAZH0lEQVR4nO3df5BdZZ3n8fdHogzFCCYY2JjAoBJlgS0zpjcya5U1kqkkM1oTnAJJ1c6QsrJGKVT+GNcC16pYIFOwMw6jrlAbB9aEWQYzqWHMOiBmYC3drRDSWVF+CEuvyUg2WRLpgPiHDAmf/eM8bW56bt++6R9P35v+vKpu3XO/5zzPec7tW99+nuece49sExFRw+tmugERMXsk4URENUk4EVFNEk5EVJOEExHVJOFERDVdJRxJb5K0VdLTkn4s6bckzZO0XdKz5Xluy/Y3SBqS9IyklS3xpZIeL+u+LEklfqqkb5T4Tknnt5RZW/bxrKS1U3foEVFbtz2cLwHftn0h8C7gx8D1wEO2FwMPlddIughYA1wMrAJul3RKqecOYD2wuDxWlfg64LDtC4DbgFtLXfOADcB7gGXAhtbEFhH9ZdyEI+kM4H3AnQC2/8n2i8BqYFPZbBNweVleDdxr+xXbe4AhYJmkBcAZtne4udpw86gyI3VtBZaX3s9KYLvtYduHge0cS1IR0WfmdLHN24BDwH+R9C5gN3AdcI7tAwC2D0g6u2y/EHikpfy+Enu1LI+Oj5R5rtR1RNJLwFmt8TZlfkXSepqeE6effvrSCy+8sIvDiojpsHv37p/Znt9uXTcJZw7wbuCTtndK+hJl+DQGtYm5Q3yiZY4F7I3ARoCBgQEPDg52aF5ETCdJ/zjWum7mcPYB+2zvLK+30iSg58swifJ8sGX7c1vKLwL2l/iiNvHjykiaA5wJDHeoKyL60LgJx/b/A56T9M4SWg48BWwDRs4arQW+WZa3AWvKmae30kwOP1qGXy9LurTMz1w9qsxIXVcAD5d5ngeBFZLmlsniFSUWEX2omyEVwCeB/yrpDcBPgI/QJKstktYBPwWuBLD9pKQtNEnpCHCt7aOlnmuArwOnAQ+UBzQT0ndLGqLp2awpdQ1LugnYVba70fbwBI81ImaYTrafp8gcTsTMkrTb9kC7dbnSOCKqScKJiGqScCKimiSciKim27NUMYPOv/7vT7jM3ls+MA0tiekwm/6+6eFERDVJOBFRTYZUPWoi3eyYPVo/H/00vErC6WP99EGLsY31dzwZ/+lkSBUR1SThREQ1+S5Vj+i2+5xh1OzUT5+PfJcqInpCEk5EVJMh1Qzqp25y9JZuPjsz9bnJkCoiekISTkRUkyFVZRlGxVTrtc9UhlQR0ROScCKimnyXqoKT8Tsx0X964QufSTg9InM2MVGtn51e/+eWIVVEVJOEExHVdHVaXNJe4GXgKHDE9oCkzwMfBQ6VzT5r+/6y/Q3AurL9p2w/WOJLOXbnzfuB62xb0qnAZmAp8AJwle29pcxa4HNlH1+wvalTW3vhtHivnaaM2WmmPoedToufyBzO+23/bFTsNtt/NmpnF9Hcqvdi4C3AP0h6R7nd7x3AeuARmoSziuZ2v+uAw7YvkLQGuBW4StI8YAMwABjYLWmb7cMn0O6I6BHTMaRaDdxr+xXbe4AhYJmkBcAZtne46VZtBi5vKTPSc9kKLJckYCWw3fZwSTLbaZJURPShbns4Br4jycB/tr2xxD8h6WpgEPjjkhQW0vRgRuwrsVfL8ug45fk5ANtHJL0EnNUab1PmVyStp+k5cd5553V5SFMrw6joNaM/a2N9RmueLu+2h/Ne2+8Gfhe4VtL7aIZHbweWAAeAL5Zt1aa8O8QnWuZYwN5oe8D2wPz58zseSETMnK4Sju395fkgcB+wzPbzto/afg34GrCsbL4POLel+CJgf4kvahM/roykOcCZwHCHuiKiD407pJJ0OvA62y+X5RXAjZIW2D5QNvsQ8ERZ3gbcI+nPaSaNFwOP2j4q6WVJlwI7gauBr7SUWQvsAK4AHi5nrx4E/kTS3LLdCuCGSR5zdRlGRS/ohQsEu5nDOQe4r5nDZQ5wj+1vS7pb0hKaIc5e4GMAtp+UtAV4CjgCXFvOUAFcw7HT4g+UB8CdwN2Shmh6NmtKXcOSbgJ2le1utD088cONiJk0bsKx/RPgXW3if9ShzM3AzW3ig8AlbeK/BK4co667gLvGa2dE9L78Hs4kjNUtzRAq+kmn4dVEPsv5PZyI6AlJOBFRTRJORFST38OJiDFN9VXI6eFERDVJOBFRTYZUUySnwqNf1bwCOT2ciKgmCSciqknCiYhqknAiopoknIioJmepJiFnpuJk0+3Pkk5UejgRUU0STkRUk4QTEdUk4URENUk4EVFNEk5EVJPT4hExpqm+9CM9nIioJgknIqrpKuFI2ivpcUmPSRossXmStkt6tjzPbdn+BklDkp6RtLIlvrTUMyTpyyp315N0qqRvlPhOSee3lFlb9vGspLVTdeARUd+J9HDeb3tJy/1mrgcesr0YeKi8RtJFNHfOvBhYBdwu6ZRS5g5gPc3tfxeX9QDrgMO2LwBuA24tdc0DNgDvobl3+YbWxBYR/WUyQ6rVwKayvAm4vCV+r+1XbO8BhoBlkhYAZ9je4ebue5tHlRmpayuwvPR+VgLbbQ/bPgxs51iSiog+023CMfAdSbslrS+xc2wfACjPZ5f4QuC5lrL7SmxhWR4dP66M7SPAS8BZHeo6jqT1kgYlDR46dKjLQ4qI2ro9Lf5e2/slnQ1sl/R0h23VJuYO8YmWORawNwIbobnVb4e2RcQM6qqHY3t/eT4I3Eczn/J8GSZRng+WzfcB57YUXwTsL/FFbeLHlZE0BzgTGO5QV0T0oXETjqTTJb1xZBlYATwBbANGzhqtBb5ZlrcBa8qZp7fSTA4/WoZdL0u6tMzPXD2qzEhdVwAPl3meB4EVkuaWyeIVJRYRfaibIdU5wH3lDPYc4B7b35a0C9giaR3wU+BKANtPStoCPAUcAa61fbTUdQ3wdeA04IHyALgTuFvSEE3PZk2pa1jSTcCust2NtocncbwRMYPUdCROHgMDAx4cHJzpZkTMWpJ2t1w+c5xcaRwR1SThREQ1STgRUU0STkRUk4QTEdUk4URENUk4EVFNEk5EVJOEExHVJOFERDVJOBFRTRJORFSThBMR1SThREQ1STgRUU0STkRUk4QTEdUk4URENUk4EVFNEk5EVJOEExHVJOFERDVJOBFRTdcJR9Ipkn4g6Vvl9ecl/V9Jj5XH77Vse4OkIUnPSFrZEl8q6fGy7svlDpyUu3R+o8R3Sjq/pcxaSc+Wx1oiom+dSA/nOuDHo2K32V5SHvcDSLqI5s6ZFwOrgNslnVK2vwNYT3P738VlPcA64LDtC4DbgFtLXfOADcB7aO5nvqHc8jci+lBXCUfSIuADwF92sflq4F7br9jeAwwByyQtAM6wvaPcN3wzcHlLmU1leSuwvPR+VgLbbQ/bPgxs51iSiog+020P5y+AzwCvjYp/QtKPJN3V0vNYCDzXss2+EltYlkfHjytj+wjwEnBWh7oiog+Nm3AkfRA4aHv3qFV3AG8HlgAHgC+OFGlTjTvEJ1qmtY3rJQ1KGjx06FCbIhHRC7rp4bwX+H1Je4F7gcsk/ZXt520ftf0a8DWaORZoeiHntpRfBOwv8UVt4seVkTQHOBMY7lDXcWxvtD1ge2D+/PldHFJEzIRxE47tG2wvsn0+zWTww7b/sMzJjPgQ8ERZ3gasKWee3kozOfyo7QPAy5IuLfMzVwPfbCkzcgbqirIPAw8CKyTNLUO2FSUWEX1oziTK/kdJS2iGOHuBjwHYflLSFuAp4Ahwre2jpcw1wNeB04AHygPgTuBuSUM0PZs1pa5hSTcBu8p2N9oenkSbI2IGqelInDwGBgY8ODg4082ImLUk7bY90G5drjSOiGomM6Tqe+df//dt43tv+UDllkTMDunhREQ1STgRUU0STkRUk4QTEdUk4URENUk4EVFNEk5EVJOEExHVJOFERDVJOBFRTRJORFSThBMR1SThREQ1STgRUU0STkRUM6t/D2csrb+Tk9/GiZg66eFERDVJOBFRTRJORFQzq+dwWudnxvp944iYOunhREQ1STgRUU3XCUfSKZJ+IOlb5fU8SdslPVue57Zse4OkIUnPSFrZEl8q6fGy7svllr+U2wJ/o8R3Sjq/pczaso9nJa0lIvrWifRwrgN+3PL6euAh24uBh8prJF1Ec6vei4FVwO2STill7gDW09xvfHFZD7AOOGz7AuA24NZS1zxgA/AeYBmwoTWxRUR/6SrhSFoEfAD4y5bwamBTWd4EXN4Sv9f2K7b3AEPAMkkLgDNs73Bzf+HNo8qM1LUVWF56PyuB7baHbR8GtnMsSUVEn+m2h/MXwGeA11pi59g+AFCezy7xhcBzLdvtK7GFZXl0/Lgyto8ALwFndajrOJLWSxqUNHjo0KEuDykiahs34Uj6IHDQ9u4u61SbmDvEJ1rmWMDeaHvA9sD8+fO7bGZE1NZND+e9wO9L2gvcC1wm6a+A58swifJ8sGy/Dzi3pfwiYH+JL2oTP66MpDnAmcBwh7oiog+pmU7pcmPpt4FP2/6gpD8FXrB9i6TrgXm2PyPpYuAemknet9BMKC+2fVTSLuCTwE7gfuArtu+XdC3wr2x/XNIa4A9sf7hMGu8G3l2a8L+ApbaHx2rjwMCABwcHT+xdGKXTRYD5MmdEZ5J22x5ot24yVxrfAmyRtA74KXAlgO0nJW0BngKOANfaPlrKXAN8HTgNeKA8AO4E7pY0RNOzWVPqGpZ0E7CrbHdjp2QTEb3thBKO7e8C3y3LLwDLx9juZuDmNvFB4JI28V9SElabdXcBd51IOyOiN+VK44ioJgknIqpJwomIapJwIqKaWf17OBOR3zuO2WSqP+/p4URENUk4EVFNhlRt5KdHI6ZHejgRUU0STkRUkyHVJOSMVZxspnsKIT2ciKgmCSciqknCiYhqMoczjtFzMzlNHjFx6eFERDVJOBFRTYZUUySnyKNf1ZwmSA8nIqpJwomIajKkOkH5YmfMJlM9PZAeTkRUk4QTEdWMe+dNSb8GfA84lWYIttX2BkmfBz4KHCqbftb2/aXMDcA64CjwKdsPlvhSjt0I737gOtuWdCqwGVgKvABcZXtvKbMW+FzZxxdsb+rU3qm48+ZE5G6d0U/G+rxOxWd1snfefAW4zPYvJL0e+B+SRu6YeZvtPxu1s4to7px5Mc2tfv9B0jvK3TfvANYDj9AknFU0d99cBxy2fUG51e+twFXlVr8bgAHAwG5J22wfPpE3ICJ6w7hDKjd+UV6+vjw6dYtWA/fafsX2HmAIWCZpAXCG7R1uulWbgctbyoz0XLYCyyUJWAlstz1cksx2miQVEX2oq7NUkk4BdgMXAF+1vVPS7wKfkHQ1MAj8cUkKC2l6MCP2ldirZXl0nPL8HIDtI5JeAs5qjbcp09q+9TQ9J84777xuDqmqXBQYvaAXzqp2NWls+6jtJcAimt7KJTTDo7cDS4ADwBfL5mpXRYf4RMu0tm+j7QHbA/Pnz+94LBExc07oLJXtF4HvAqtsP18S0WvA14BlZbN9wLktxRYB+0t8UZv4cWUkzQHOBIY71BURfWjchCNpvqQ3leXTgN8Bni5zMiM+BDxRlrcBaySdKumtwGLgUdsHgJclXVrmZ64GvtlSZm1ZvgJ4uMzzPAiskDRX0lxgRYlFRB/qZg5nAbCpzOO8Dthi+1uS7pa0hGaIsxf4GIDtJyVtAZ4CjgDXljNUANdw7LT4A+UBcCdwt6Qhmp7NmlLXsKSbgF1luxttD0/ieKdNt1cgZz4naul2zqbm53DchGP7R8Bvton/UYcyNwM3t4kPApe0if8SuHKMuu4C7hqvnRHR+3KlcURUM+6Vxv1mpq407qSbrm2GVzEVemEY1elK4/RwIqKaJJyIqCZDqsp6ocsbJ5de+0xlSBURPSEJJyKqyU+M9qjZcoHgbDnOqdYLX8SciCScyvKbyFFLLybwDKkiopoknIioJqfFe0SvndqM3tJPn4+cFo+InpCEExHVZEjVo/KFz+inYVSrDKkioick4URENRlS9YGJXCDYa93sGNvJ9vfNkCoiekISTkRUkyFVHzvZuuIns9n0t8qQKiJ6QhJORFSThBMR1Yw7hyPp14DvAafS/H7OVtsbJM0DvgGcT3PnzQ/bPlzK3ACsA44Cn7L9YIkv5didN+8HrrNtSacCm4GlwAvAVbb3ljJrgc+V5nzB9qZO7Z1NczitpuK3dfp1zqCXTPbvcDL8DSY7h/MKcJntdwFLgFWSLgWuBx6yvRh4qLxG0kU0t+q9GFgF3F5uEwxwB7Ce5n7ji8t6aJLTYdsXALcBt5a65gEbgPcAy4AN5R7jEdGHxk04bvyivHx9eRhYDYz0NjYBl5fl1cC9tl+xvQcYApZJWgCcYXuHm27V5lFlRuraCiyXJGAlsN32cOk9bedYkoqIPtPVT4yWHspu4ALgq7Z3SjrH9gEA2wcknV02Xwg80lJ8X4m9WpZHx0fKPFfqOiLpJeCs1nibMq3tW0/Tc+K8887r5pBOOp264t1288farttu/mz5feIMmyauq0lj20dtLwEW0fRWLumwudpV0SE+0TKt7dtoe8D2wPz58zs0LSJm0gmdpbL9IvBdmmHN82WYRHk+WDbbB5zbUmwRsL/EF7WJH1dG0hzgTGC4Q10R0YfGHVJJmg+8avtFSacBv0MzqbsNWAvcUp6/WYpsA+6R9OfAW2gmhx+1fVTSy2XCeSdwNfCVljJrgR3AFcDD5ezVg8CftEwUrwBumOxBzzZjdeEnO9Q6GUznsc3modNYupnDWQBsKvM4rwO22P6WpB3AFknrgJ8CVwLYflLSFuAp4Ahwre2jpa5rOHZa/IHyALgTuFvSEE3PZk2pa1jSTcCust2Ntocnc8ARMXPGTTi2fwT8Zpv4C8DyMcrcDNzcJj4I/LP5H9u/pCSsNuvuAu4ar50R0fvy5c0Y18k8pJqIDJU6y5c3I6InJOFERDVJOBFRTVdXGsfsNpErjftV5memV3o4EVFNEk5EVJMhVUyZDEdiPOnhREQ1STgRUU0STkRUk4QTEdUk4URENUk4EVFNEk5EVJOEExHVJOFERDUn3Q9wSToE/OMkq3kz8LMpaE4/t2G2778X2jDT+59oG37Ddtvbp5x0CWcqSBoc6xfLZksbZvv+e6ENM73/6WhDhlQRUU0STkRUk4TT3saZbgAz34bZvn+Y+TbM9P5hituQOZyIqCY9nIioJgknIqqZdQlH0ipJz0gaknR9m/VnSvpvkn4o6UlJH2lZt1fS45IekzShu+11sf+5ku6T9CNJj0q6pNuyFfY/Fcd/l6SDkp4YY70kfbm070eS3t1t2yu1ocZ7cKGkHZJekfTpUetqvQed2jDx98D2rHkApwD/B3gb8Abgh8BFo7b5LHBrWZ5Pc6/zN5TXe4E3T/P+/xTYUJYvBB7qtux07n8qjr/U8T7g3cATY6z/PZp7zgu4FNg5Vcc/2TZUfA/OBv41ze2yP30if7/pbsNk34PZ1sNZBgzZ/ontfwLuBVaP2sbAGyUJ+HWahHOk4v4vAh4CsP00cL6kc7osO537nxK2v0fzno5lNbDZjUeAN0la0GXbp7sNU2K8/ds+aHsX8OqoVdXegw5tmJTZlnAWAs+1vN5XYq3+E/Avgf3A48B1tl8r6wx8R9JuSeunaf8/BP4AQNIy4DeARV2Wnc79w+SPfzJtnIrjn2wboM57MJF21TTh92C23bVBbWKjrwtYCTwGXAa8Hdgu6fu2fw681/Z+SWeX+NPlP8VU7v8W4EuSHqNJeD+g6WF1U3Y69w+TP/7JtHEqjn+ybYA678FE2lXThN+D2dbD2Qec2/J6EU1PptVHgL8t3ekhYA/NXAa295fng8B9NF3cKd2/7Z/b/ojtJcDVNPNIe7ps+3TufyqOfzJtnIrjn2wbar0HJ9yumibzHsy2hLMLWCzprZLeAKwBto3a5qfAcoAyd/FO4CeSTpf0xhI/HVgBtJ3hn8z+Jb2prAP4d8D3Su+qm7ZP2/6n6Pi7sQ24upwpuhR4yfaBbto+3W2o+B6MpeZ70Nak34PJzLb344PmDMT/ppnt/w8l9nHg42X5LcB3aIYTTwB/WOJvo5nf+CHw5EjZadj/bwHPAk8DfwvM7VS21v6n8Pj/GjhAMxm5D1g3av8Cvlra9zgwMJXHP5k2VHwP/kWJ/xx4sSyfUfk9aNuGyb4H+WpDRFQz24ZUETGDknAiopoknIioJgknIqpJwokIYPwvdLbZ/sOSnlLzJed7uiqTs1QRASDpfcAvaL5Hdsk42y4GtgCX2T4s6Ww3FwJ2lB5ORADtv9Ap6e2Svl2+N/V9SReWVR8Fvmr7cCk7brKBJJyI6Gwj8EnbS4FPA7eX+DuAd0j6n5IekbSqm8pm25c3I6JLkn4d+DfA3zS/1gLAqeV5DrAY+G2a73R9X9Iltl/sVGcSTkSM5XXAi26+yDvaPuAR268CeyQ9Q5OAdo1XYUTEP+PmS8N7JF0Jv/rp1XeV1X8HvL/E30wzxPrJeHUm4UQEAJL+GtgBvFPSPknrgH8LrJM08mXNkV8YfBB4QdJTwH8H/r3tF8bdR06LR0Qt6eFERDVJOBFRTRJORFSThBMR1SThREQ1STgRUU0STkRU8/8BKTueFVZmMycAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "targetDistance_miles = 32\n",
    "#cleanedLULC_dir = os.path.join(rasterDataDir, r\"EPCExtent_30cm\\LULC\")\n",
    "\n",
    "tindex_target = tindex[tindex.DistToCenter <= targetDistance_miles*5280].copy()\n",
    "print(f\"{len(tindex_target)} tiles within {targetDistance_miles} miles\")\n",
    "tindex_target[\"CleanedLULCFile\"] = tindex_target.apply(lambda r: cleanBinaryDir+f\"/{r.path}_{r.row}_TrainingStackV3_BinaryStack_cleaned.tif\", axis=1)\n",
    "tindex_target[\"CleanedLULCFile\"] = tindex_target[\"CleanedLULCFile\"].apply(lambda f: f if os.path.exists(f) else None)\n",
    "tindex_toDo = tindex_target[tindex_target[\"CleanedLULCFile\"].isnull()].copy()\n",
    "tindex_toDo[\"OrthoFile\"] = tindex_toDo.apply(lambda r: glob(f\"{orthosDir}/{r.path}_{r.row}_*.tif\"), axis=1)\n",
    "tindex_toDo[\"OrthoFile\"] = tindex_toDo[\"OrthoFile\"].apply(lambda f: f[0] if len(f) == 1 else None)\n",
    "print(f\"{len(tindex_toDo)} tiles with LULC files to be created\")\n",
    "trainingToCreate = tindex_toDo[tindex_toDo.OrthoFile.isnull()]\n",
    "print(f\"{len(trainingToCreate)} tiles ({round(100*len(tindex_toDo[tindex_toDo.OrthoFile.isnull()])/len(tindex_toDo),2)}%) that need training data created\")\n",
    "tindex_toDo.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting orthos at 2022-01-25 10:04:08.184092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=5)]: Done  62 tasks      | elapsed: 109.2min\n",
      "[Parallel(n_jobs=5)]: Done 152 tasks      | elapsed: 259.3min\n",
      "C:\\Users\\BenJames\\anaconda3\\envs\\geospatial\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:703: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Done 254 out of 254 | elapsed: 430.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished orthos at 2022-01-25 17:14:28.421683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed: 89.2min\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed: 226.9min\n",
      "[Parallel(n_jobs=8)]: Done 254 out of 254 | elapsed: 368.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED\n",
      "Wall time: 13h 18min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f\"Starting orthos at {datetime.now()}\")\n",
    "toDo_OrthoFiles = Parallel(n_jobs=5, verbose=5, backend=\"loky\")(delayed(segmentWindowV3)(boxrow,\n",
    "                                                                                         v3TrainingStack_dir,\n",
    "                                                                                         ortho30cmvrt_loc,\n",
    "                                                                                         hagvrt_loc,\n",
    "                                                                                         returnArray=False,\n",
    "                                                                                         writeOutStack=True,\n",
    "                                                                                         overwrite=False)\n",
    "                                                                for i, boxrow in tindex_toDo.iterrows())\n",
    "print(f\"Finished orthos at {datetime.now()}\")\n",
    "toDo_LHEFiles = Parallel(n_jobs=8, verbose=5, backend=\"loky\")(delayed(createLocalHistogramOrtho)(boxrow,\n",
    "                                                                                                 histoEqual_loc,\n",
    "                                                                                                 ortho30cmvrt_loc,\n",
    "                                                                                                 overwrite=False)\n",
    "                                                              for i, boxrow in tindex_toDo.iterrows())\n",
    "\n",
    "tindex_toDo[\"OrthoFile\"] = tindex_toDo.apply(lambda r: glob(f\"{orthosDir}/{r.path}_{r.row}_*.tif\"), axis=1)\n",
    "tindex_toDo[\"OrthoFile\"] = tindex_toDo[\"OrthoFile\"].apply(lambda f: f[0] if len(f) == 1 else None)\n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Group Parallelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH: W989789, ROW: N329850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BenJames\\anaconda3\\envs\\geospatial\\lib\\site-packages\\joblib\\parallel.py:255: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  return [func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed on R:\\ProjectData\\PAG2019\\EPCExtent_30cm/Orthos_Segmented_Classifiedv3\\W989789_N329850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. Returning None\n",
      "PATH: W969789, ROW: N299850\n",
      "Failed on R:\\ProjectData\\PAG2019\\EPCExtent_30cm/Orthos_Segmented_Classifiedv3\\W969789_N299850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. Returning None\n",
      "PATH: W1079789, ROW: N309850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1079789_N309850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:21.304167 -\t2022-01-26 09:54:30.390595\n",
      "PATH: W1144789, ROW: N524850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1144789_N524850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:36.215109 -\t2022-01-26 09:59:06.922694\n",
      "PATH: W929789, ROW: N309850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W929789_N309850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:21.389326 -\t2022-01-26 10:03:28.600021\n",
      "PATH: W864789, ROW: N524850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BenJames\\AppData\\Local\\Temp/ipykernel_14896/3591094708.py:515: RuntimeWarning: invalid value encountered in true_divide\n",
      "  nessNorm = (tband-np.nanmean(obands,axis=0))/(tband+np.nanmean(obands,axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W864789_N524850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:23.685577 -\t2022-01-26 10:07:52.593146\n",
      "PATH: W1144789, ROW: N374850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1144789_N374850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:48.230324 -\t2022-01-26 10:12:41.090488\n",
      "PATH: W864789, ROW: N374850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W864789_N374850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:46.602604 -\t2022-01-26 10:17:27.952093\n",
      "PATH: W1039789, ROW: N294850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1039789_N294850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:32.096802 -\t2022-01-26 10:22:00.278895\n",
      "PATH: W1149789, ROW: N384850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1149789_N384850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:25.718933 -\t2022-01-26 10:26:26.230735\n",
      "PATH: W1149789, ROW: N514850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1149789_N514850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:03:54.302611 -\t2022-01-26 10:30:20.752345\n",
      "PATH: W969789, ROW: N294850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W969789_N294850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:21.444182 -\t2022-01-26 10:34:42.405509\n",
      "PATH: W859789, ROW: N384850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W859789_N384850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:23.698887 -\t2022-01-26 10:39:06.290396\n",
      "PATH: W939789, ROW: N304850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W939789_N304850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:10.933344 -\t2022-01-26 10:43:17.405714\n",
      "PATH: W1069789, ROW: N304850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1069789_N304850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:06.872185 -\t2022-01-26 10:47:24.486802\n",
      "PATH: W849789, ROW: N414850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W849789_N414850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:12.629069 -\t2022-01-26 10:51:37.294872\n",
      "PATH: W889789, ROW: N339850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W889789_N339850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:30.200244 -\t2022-01-26 10:56:07.673209\n",
      "PATH: W1119789, ROW: N559850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1119789_N559850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:02.799554 -\t2022-01-26 11:00:10.687760\n",
      "PATH: W1119789, ROW: N339850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1119789_N339850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:13.046162 -\t2022-01-26 11:04:23.935920\n",
      "PATH: W1114789, ROW: N334850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1114789_N334850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:12.965389 -\t2022-01-26 11:08:37.059339\n",
      "PATH: W894789, ROW: N334850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W894789_N334850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:23.963184 -\t2022-01-26 11:13:01.180550\n",
      "PATH: W1114789, ROW: N564850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1114789_N564850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:01.511341 -\t2022-01-26 11:17:02.857891\n",
      "PATH: W889789, ROW: N559850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W889789_N559850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:02.022836 -\t2022-01-26 11:21:05.045757\n",
      "PATH: W894789, ROW: N564850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W894789_N564850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:02.883837 -\t2022-01-26 11:25:08.093589\n",
      "PATH: W1109789, ROW: N569850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1109789_N569850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:03:53.412595 -\t2022-01-26 11:29:01.667212\n",
      "PATH: W884789, ROW: N344850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W884789_N344850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:10.442561 -\t2022-01-26 11:33:12.271760\n",
      "PATH: W1124789, ROW: N554850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1124789_N554850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:00.301844 -\t2022-01-26 11:37:12.747635\n",
      "PATH: W1109789, ROW: N329850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1109789_N329850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:08.101862 -\t2022-01-26 11:41:21.081496\n",
      "PATH: W1124789, ROW: N344850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1124789_N344850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:11.938043 -\t2022-01-26 11:45:33.175544\n",
      "PATH: W899789, ROW: N329850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W899789_N329850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:03:59.568482 -\t2022-01-26 11:49:32.917030\n",
      "PATH: W899789, ROW: N569850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W899789_N569850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:03:55.063837 -\t2022-01-26 11:53:28.143843\n",
      "PATH: W884789, ROW: N554850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W884789_N554850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:03:54.154367 -\t2022-01-26 11:57:22.477207\n",
      "PATH: W1139789, ROW: N534850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1139789_N534850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:13.417956 -\t2022-01-26 12:01:36.078153\n",
      "PATH: W1089789, ROW: N314850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1089789_N314850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:06.176448 -\t2022-01-26 12:05:42.422603\n",
      "PATH: W869789, ROW: N364850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W869789_N364850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:11.401190 -\t2022-01-26 12:09:53.999808\n",
      "PATH: W919789, ROW: N584850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W919789_N584850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:08.170712 -\t2022-01-26 12:14:02.330490\n",
      "PATH: W1089789, ROW: N584850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1089789_N584850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:17.069586 -\t2022-01-26 12:18:19.600195\n",
      "PATH: W919789, ROW: N314850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W919789_N314850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:32.620964 -\t2022-01-26 12:22:52.406160\n",
      "PATH: W869789, ROW: N534850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W869789_N534850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:14.571786 -\t2022-01-26 12:27:07.181049\n",
      "PATH: W1139789, ROW: N364850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1139789_N364850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:03.689324 -\t2022-01-26 12:31:11.039391\n",
      "PATH: W854789, ROW: N394850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W854789_N394850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:11.454053 -\t2022-01-26 12:35:22.658443\n",
      "PATH: W1059789, ROW: N299850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1059789_N299850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:03:52.587814 -\t2022-01-26 12:39:15.410253\n",
      "PATH: W949789, ROW: N299850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W949789_N299850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:07.651744 -\t2022-01-26 12:43:23.241987\n",
      "PATH: W1004789, ROW: N289850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W1004789_N289850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:03:53.817760 -\t2022-01-26 12:47:17.237735\n",
      "PATH: W999789, ROW: N289850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W999789_N289850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:03:55.903082 -\t2022-01-26 12:51:13.300045\n",
      "PATH: W849789, ROW: N409850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W849789_N409850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:13.178034 -\t2022-01-26 12:55:26.639165\n",
      "PATH: W904789, ROW: N574850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W904789_N574850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:01.086677 -\t2022-01-26 12:59:27.890830\n",
      "PATH: W904789, ROW: N324850\n",
      "Classified to R:\\ProjectData\\PAG2019\\EPCExtent_30cm\\Orthos_Segmented_Classifiedv3\\W904789_N324850_TrainingStackV3_BinaryOptunaTunerGBLM.tif. \n",
      "Classification took 0:04:03.342954 -\t2022-01-26 13:03:31.430801\n",
      "PATH: W879789, ROW: N549850\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#tday = datetime.now().strftime(\"%Y%m%d\")\n",
    "tday = \"20220103\"\n",
    "\n",
    "\"\"\"\n",
    "asphaultFiles = Parallel(n_jobs=getnjobs())(delayed(createClassifiedRaster)(pickle.load(open(classifiers[\"Asphault\"], \"rb\")),\n",
    "                                                                   ortho_file= row.OrthoFile,\n",
    "                                                                   classifiedFiles_loc= binaryClassifiedFiles_loc,\n",
    "                                                                   binaryClass= \"Asphault\",\n",
    "                                                                   suffix = tday,\n",
    "                                                                   overwrite= False) for i, row in tindex_toDo.iterrows())\n",
    "\n",
    "poolFiles = Parallel(n_jobs=getnjobs())(delayed(createClassifiedRaster)(pickle.load(open(classifiers[\"Pool\"], \"rb\")),\n",
    "                                                               ortho_file= row.OrthoFile,\n",
    "                                                               classifiedFiles_loc= binaryClassifiedFiles_loc,\n",
    "                                                               binaryClass= \"Pool\",\n",
    "                                                               suffix = tday,\n",
    "                                                               overwrite= False) for i, row in tindex_toDo.iterrows())\n",
    "\n",
    "imperviousFiles = Parallel(n_jobs=getnjobs())(delayed(createClassifiedRaster)(pickle.load(open(classifiers[\"Impervious\"], \"rb\")),\n",
    "                                                                     ortho_file= row.OrthoFile,\n",
    "                                                                     classifiedFiles_loc= binaryClassifiedFiles_loc,\n",
    "                                                                     binaryClass= \"Impervious\",\n",
    "                                                                     suffix= tday,\n",
    "                                                                     overwrite= False) for i, row in tindex_toDo.iterrows())\n",
    "\n",
    "barrenFiles = Parallel(n_jobs=getnjobs())(delayed(createClassifiedRaster)(pickle.load(open(classifiers[\"Barren\"], \"rb\")),\n",
    "                                                                          ortho_file= row.OrthoFile,\n",
    "                                                                          classifiedFiles_loc= binaryClassifiedFiles_loc,\n",
    "                                                                          binaryClass= \"Barren\",\n",
    "                                                                          suffix = tday,\n",
    "                                                                          overwrite= False) for i, row in tindex_toDo.iterrows())\n",
    "\n",
    "pondLakeFiles = Parallel(n_jobs=getnjobs())(delayed(createClassifiedRaster)(pickle.load(open(classifiers[\"PondLake\"], \"rb\")),\n",
    "                                                                            ortho_file= row.OrthoFile,\n",
    "                                                                            classifiedFiles_loc= binaryClassifiedFiles_loc,\n",
    "                                                                            binaryClass= \"PondLake\",\n",
    "                                                                            suffix = tday,\n",
    "                                                                            overwrite= False) for i, row in tindex_toDo.iterrows())\n",
    "\"\"\"\n",
    "files = Parallel(n_jobs=1)(delayed(createBinaryClassifiedRaster)(models=classifiers,\n",
    "                                                                 ortho_file=row.OrthoFile,\n",
    "                                                                 classifiedFiles_loc=classifiedFiles_loc,\n",
    "                                                                 overwrite=False,\n",
    "                                                                 roundConfidence=False) for i, row in tindex_toDo.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tindex_toDo[\"BinaryStackFile\"] = tindex_toDo.apply(lambda r: glob(classifiedFiles_loc + f\"/{r.path}_{r.row}_TrainingStackV3_BinaryOptunaTunerGBLM.tif\"), axis=1)\n",
    "#tindex_toDo[\"AsphaultFile\"] = tindex_toDo.apply(lambda r: glob(binaryClassifiedFiles_loc + f\"/{r.path}_{r.row}_TrainingStackV3_AsphaultBinaryOptunaTunerGBLM_*.tif\"), axis=1)\n",
    "#tindex_toDo[\"PoolFile\"] = tindex_toDo.apply(lambda r: glob(binaryClassifiedFiles_loc + f\"/{r.path}_{r.row}_TrainingStackV3_PoolBinaryOptunaTunerGBLM_*.tif\"), axis=1)\n",
    "#tindex_toDo[\"PondLakeFile\"] = tindex_toDo.apply(lambda r: glob(binaryClassifiedFiles_loc + f\"/{r.path}_{r.row}_TrainingStackV3_PondLakeBinaryOptunaTunerGBLM_*.tif\"), axis=1)\n",
    "#tindex_toDo[\"ImperviousFile\"] = tindex_toDo.apply(lambda r: glob(binaryClassifiedFiles_loc + f\"/{r.path}_{r.row}_TrainingStackV3_ImperviousBinaryOptunaTunerGBLM_*.tif\"), axis=1)\n",
    "#tindex_toDo[\"BarrenFile\"] = tindex_toDo.apply(lambda r: glob(binaryClassifiedFiles_loc + f\"/{r.path}_{r.row}_TrainingStackV3_BarrenBinaryOptunaTunerGBLM_*.tif\"), axis=1)\n",
    "\n",
    "tindex_toDo[\"BinaryStackFile\"] = tindex_toDo[\"BinaryStackFile\"].apply(lambda f: f[0] if len(f) == 1 else None)\n",
    "#tindex_toDo[\"AsphaultFile\"] = tindex_toDo[\"AsphaultFile\"].apply(lambda f: f[0] if len(f) == 1 else None)\n",
    "#tindex_toDo[\"PoolFile\"] = tindex_toDo[\"PoolFile\"].apply(lambda f: f[0] if len(f) == 1 else None)\n",
    "#tindex_toDo[\"PondLakeFile\"] = tindex_toDo[\"PondLakeFile\"].apply(lambda f: f[0] if len(f) == 1 else None)\n",
    "#tindex_toDo[\"ImperviousFile\"] = tindex_toDo[\"ImperviousFile\"].apply(lambda f: f[0] if len(f) == 1 else None)\n",
    "#tindex_toDo[\"BarrenFile\"] = tindex_toDo[\"BarrenFile\"].apply(lambda f: f[0] if len(f) == 1 else None)\n",
    "tindex_toDo = tindex_toDo[tindex_toDo.BinaryStackFile.notnull()].reset_index() #  tindex_target.dropna().reset_index()\n",
    "print(tindex_toDo.shape)\n",
    "%chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cleanedLULC_dir = os.path.join(rasterDataDir, r\"EPCExtent_30cm\\LULC\")\n",
    "os.makedirs(cleanedLULC_dir, exist_ok=True)\n",
    "\n",
    "tindex_toDo[\"CleanedLULCFile\"] = tindex_toDo.apply(lambda r: cleanedLULC_dir + f\"/{r.path}_{r.row}_LULC2019.tif\", axis=1)\n",
    "tindex_toDo[\"CleanedLULCFile\"] = tindex_toDo[\"CleanedLULCFile\"].apply(lambda f: f if os.path.exists(f) else None)\n",
    "lulc_toDo = tindex_toDo[tindex_toDo[\"CleanedLULCFile\"].isnull()].copy()\n",
    "print(f\"{len(lulc_toDo)} LULCs to be build\")\n",
    "\n",
    "cleanLULCFiles = Parallel(n_jobs=5)(delayed(buildCleanLULC)(i, row, cleanBinaryDir, overwrite=False) for i, row in lulc_toDo.iterrows())\n",
    "\n",
    "tindex_toDo[\"CleanedLULCFile\"] = tindex_toDo.apply(lambda r: cleanBinaryDir + f\"\\\\{r.path}_{r.row}_TrainingStackV3_BinaryStack_cleaned.tif\", axis=1)\n",
    "tindex_toDo[\"CleanedLULCFile\"] = tindex_toDo[\"CleanedLULCFile\"].apply(lambda f: f if os.path.exists(f) else None)\n",
    "tindex_toDo_comp = tindex_toDo[tindex_toDo[\"CleanedLULCFile\"].notnull()].copy()\n",
    "print(tindex_toDo_comp.shape)\n",
    "\n",
    "%chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.mask import mask\n",
    "\n",
    "railroads = gpd.read_file(\"../vectors/PC_Railroad/railroad.shp\").unary_union\n",
    "railroads = railroads.buffer(20)\n",
    "washes = gpd.read_file(\"../vectors/PC_MajorWashes/mash_maj.shp\").unary_union\n",
    "bridges = gpd.read_file(\"../vectors/BurnIns.gpkg\", layer=\"Bridges_paved\").unary_union\n",
    "barren = gpd.read_file(\"../vectors/BurnIns.gpkg\", layer=\"Barren\").unary_union\n",
    "\n",
    "\n",
    "def getGeometryMask(geometry, raster):\n",
    "    if geometry.type == \"Polygon\":\n",
    "        geometry = [geometry]\n",
    "    tds, tds_trans = mask(raster, geometry, all_touched=False, crop=False, filled=False)\n",
    "    \n",
    "    return ~tds.mask\n",
    "\n",
    "def burnInValuesOverLULC(rowIndex, row):\n",
    "    \n",
    "    lulc_file = row.CleanedLULCFile\n",
    "    ortho_file = row.OrthoFile\n",
    "    \n",
    "    hag = getBandByDescription(ortho_file, \"HAG\")\n",
    "    with rio.open(lulc_file) as src:\n",
    "        lulc = src.read(1)\n",
    "        kwargs = src.profile\n",
    "        bnds = src.bounds\n",
    "        railroad_mask = getGeometryMask(railroads, src)[0]\n",
    "        washes_mask = getGeometryMask(washes, src)[0]\n",
    "        bridges_mask = getGeometryMask(bridges, src)[0]\n",
    "        mines_mask = getGeometryMask(minesLand, src)[0]\n",
    "    print(lulc.shape)\n",
    "    print(mines_mask.shape)\n",
    "    \n",
    "    \n",
    "    # burn in railroads where not structure or road\n",
    "    lulc[(railroad_mask) & \n",
    "         (lulc != values_2019_lookup[\"Structure\"]) &\n",
    "         (lulc != values_2019_lookup[\"Asphault\"])] = values_2019_lookup[\"Barren\"]\n",
    "    \n",
    "    #burn in bridges as asphault\n",
    "    lulc[(bridges_mask)] = values_2019_lookup[\"Asphault\"]\n",
    "    #burn in Major Wash polygons as barren where major wash, 6ft above ground and classified as impervious or structure\n",
    "    lulc[(washes_mask) \n",
    "         & (hag < 10) \n",
    "         & ((lulc == values_2019_lookup[\"Impervious\"]) | (lulc == values_2019_lookup[\"Structure\"]))] = values_2019_lookup[\"Barren\"]\n",
    "    \n",
    "    lulc[(lulc!=0)\n",
    "             & (mines_mask==1)\n",
    "             & (lulc != values_2019_lookup[\"PondLake\"])\n",
    "             & (lulc != values_2019_lookup[\"Pool\"])\n",
    "             & (lulc != values_2019_lookup[\"SparseVeg\"])\n",
    "             & (lulc != values_2019_lookup[\"DenseVeg\"])] = values_2019_lookup[\"Barren\"]\n",
    "    \n",
    "    \n",
    "    with rio.open(lulc_file, \"r+\") as dst:\n",
    "        dst.write(lulc, 1)\n",
    "        \n",
    "    print(f\"Finished burn in for #{rowIndex} - {row.path}, {row.row}\")\n",
    "    \n",
    "    return lulc_file\n",
    "\n",
    "\n",
    "relevantTiles = tindex_target[(tindex_target.intersects(railroads))\n",
    "                   | (tindex_target.intersects(washes))\n",
    "                   | (tindex_target.intersects(bridges))\n",
    "                   | (tindex_target.intersects(barren))].reset_index(drop=True)\n",
    "\n",
    "cleanedRelevantTiles = Parallel(n_jobs=10)(delayed(burnInValuesOverLULC)(i, row) for i, row in relevantTiles.iterrows())\n",
    "\n",
    "%chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import chime\n",
    "\n",
    "tindex_target = tindex.dropna().reset_index()\n",
    "\n",
    "lulcVRT_loc = \"../EPCExtent_30cm/Cleaned2019LULC.vrt\"\n",
    "outTiff_loc = \"../EPCExtent_30cm/EPC_LULC_2019.tif\"\n",
    "lulcVRT = gdal.BuildVRT(lulcVRT_loc, tt.CleanedLULCFile.values.tolist())\n",
    "del lulcVRT\n",
    "\n",
    "start = datetime.now()\n",
    "lulcVRT_loc = \"../EPCExtent_30cm/Cleaned2019LULC.vrt\"\n",
    "outTiff_loc = \"../EPCExtent_30cm/EPC_LULC_2019.tif\"\n",
    "translateOptions = gdal.TranslateOptions(creationOptions=[\"NUM_THREADS=ALL_CPUS\",\n",
    "                                                          \"TILED=YES\",\n",
    "                                                          \"COMPRESS=LZW\"])\n",
    "lulc2019Tiff = gdal.Translate(outTiff_loc, lulcVRT_loc, options=translateOptions)\n",
    "del lulc2019Tiff\n",
    "end = datetime.now()\n",
    "print(f\"Translate took {end-start}\")\n",
    "start_addo = datetime.now()\n",
    "gdaladdo_cmd  = f\"gdaladdo -ro --config COMPRESS_OVERVIEW LZW {outTiff_loc}\"\n",
    "end_addo = datetime.now()\n",
    "print(f\"Overviews took {end_addo-start_addo}\")\n",
    "chime.theme(\"zelda\")\n",
    "chime.success()\n",
    "Translate took 0:26:12.219672\n",
    "print(os.path.abspath(tindex[(tindex.path==\"W994789\") &(tindex.row==\"W454850\")].OrthoFile.values[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
