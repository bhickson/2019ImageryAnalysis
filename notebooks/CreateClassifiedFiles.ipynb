{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LULC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chime extension is already loaded. To reload it, use:\n",
      "  %reload_ext chime\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext chime\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "import chime\n",
    "\n",
    "import rasterio as rio\n",
    "import gdal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from skimage.morphology import binary_closing, binary_opening, square, remove_small_holes, disk, square\n",
    "#from Functions import readResampledWindow, getSentinelBandFile\n",
    "\n",
    "from skimage.filters.rank import modal, mean_bilateral\n",
    "from datetime import datetime, timedelta, time\n",
    "from time import sleep\n",
    "import nbimporter\n",
    "#from KeyFunctions import *\n",
    "import chime\n",
    "from utils import *\n",
    "from skimage.morphology import binary_closing, binary_opening, square, remove_small_holes, disk, square, remove_small_objects\n",
    "\n",
    "import richdem as rd\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import psutil\n",
    "\n",
    "\n",
    "def throttleProcessors(workDayStart, workDayEnd):\n",
    "    startPause = time(*(map(int, workDayStart.split(':'))))\n",
    "    endPause = time(*(map(int, workDayEnd.split(':'))))\n",
    "    nowTime = datetime.today().time()\n",
    "    if nowTime < endPause or nowTime > startPause:\n",
    "        print(f\"Current time is {nowTime}. Setting processor/thread use to 4\")\n",
    "        return 4\n",
    "    else:\n",
    "        return 8\n",
    "\n",
    "def wait_start(workDayStart=\"8:00\", workDayEnd=\"19:00\", force=False):\n",
    "    startPause = time(*(map(int, workDayStart.split(':'))))\n",
    "    endPause = time(*(map(int, workDayEnd.split(':'))))\n",
    "    nowTime = datetime.today().time()\n",
    "    if nowTime < endPause and nowTime > startPause and datetime.today().isoweekday() <= 5 and not force:\n",
    "        waitTimeSeconds = datetime.combine(datetime.today(), endPause) - datetime.combine(datetime.today(), nowTime)\n",
    "    else:\n",
    "        waitTimeSeconds = False\n",
    "    return waitTimeSeconds\n",
    "\n",
    "\n",
    "def createClassifiedRaster(classification_model, ortho_file, classifiedFiles_loc, suffix=None, binaryClass=None, overwrite=False):\n",
    "        #try:\n",
    "        start = datetime.now()\n",
    "        if suffix == None:\n",
    "            suffix = datetime.now().strftime(\"%Y%m%d\")\n",
    "        daynum = 1\n",
    "        output_image = os.path.join(classifiedFiles_loc, os.path.basename(ortho_file))#_{day}-{daynum}.tif\"))\n",
    "        if binaryClass:\n",
    "            #print(f\"Setting binary output ({output_image})\")\n",
    "            #output_image = output_image.replace(\".tif\", f\"_{binaryClass}BinaryLGBNewishGBLM.tif\")\n",
    "            output_image = output_image.replace(\".tif\", f\"_{binaryClass}BinaryOptunaTunerGBLM_{suffix}.tif\")\n",
    "        else:\n",
    "            output_image = output_image.replace(\".tif\", f\"_MultiClassOptunaTunerGBLM_{suffix}_v2.tif\")\n",
    "            \n",
    "        if os.path.exists(output_image) and not overwrite:\n",
    "            print(f\"File exists ({output_image})\")\n",
    "            return output_image\n",
    "        try:\n",
    "            with rio.open(ortho_file) as src:\n",
    "                kwargs = src.profile\n",
    "                data = src.read()\n",
    "                descs = list(src.descriptions)\n",
    "        except:\n",
    "            print(f\"ERROR: Unable to open {os.path.basename(ortho_file)}. Skipping\")\n",
    "            return None\n",
    "        \n",
    "        if binaryClass:\n",
    "            feature_names = classification_model.feature_name()\n",
    "            dtype = rio.float32\n",
    "        else:\n",
    "            feature_names = classification_model.feature_name_\n",
    "            dtype = rio.uint8\n",
    "            \n",
    "        bands = {desc:data[ib] for ib, desc in enumerate(descs)}\n",
    "        \n",
    "        #This is a training dataset not created with the others, but may be in the classifier. Create and add\n",
    "        if \"RGBNmean\" in feature_names:\n",
    "            #print(\"Adding RGBNmean\")\n",
    "            rgbnMean = np.nanmean(data[:4], axis=0).astype(data.dtype)\n",
    "            bands[\"RGBNmean\"] = rgbnMean\n",
    "            \n",
    "        if \"RED_LHE\" in feature_names or \"GREEN_LHE\" in feature_names or \"BLUE_LHE\" in feature_names or \"NIR_LHE\" in feature_names:\n",
    "            path = os.path.basename(ortho_file).split(\"_\")[0]\n",
    "            row = os.path.basename(ortho_file).split(\"_\")[1]\n",
    "            print(f\"PATH: {path}, ROW: {row}\")\n",
    "            localHisto_file = findFile(path, row, histoEqual_loc)\n",
    "            with rio.open(localHisto_file) as src:\n",
    "                for i, desc in enumerate(src.descriptions):\n",
    "                    bands[desc] = src.read(i+1)\n",
    "                    \n",
    "        if \"REDnessNorm\" in feature_names or \"GREENnessNorm\" in feature_names or \"BLUEnessNorm\" in feature_names or \"NIRnessNorm\" in feature_names:\n",
    "            bands[\"REDnessNorm\"] = calcNessNorm(ortho_file, target_band=\"RED\")\n",
    "            bands[\"GREENnessNorm\"] = calcNessNorm(ortho_file, target_band=\"GREEN\")\n",
    "            bands[\"BLUEnessNorm\"] = calcNessNorm(ortho_file, target_band=\"BLUE\")\n",
    "            bands[\"NIRnessNorm\"] = calcNessNorm(ortho_file, target_band=\"NIR\")\n",
    "            \n",
    "        for name in feature_names:\n",
    "            if \"Sentinel2_\" in name:\n",
    "                sentBand = name.split(\"_\")[-1]\n",
    "                sentFile = getSentinelBandFile(sentBand, \"../Sentinel2Data/\")\n",
    "                bands[name] = readResampledWindow(sentFile, ortho_file, returnData=True)\n",
    "        #return bands\n",
    "        output = createClassificationFromModel(classification_model, bands, feature_names)\n",
    "        \n",
    "        #output = np.rint(output)\n",
    "        # create our final mask\n",
    "        #mask = (~m.mask[:,0]).reshape(*img_swp.shape[:-1])\n",
    "        \n",
    "        kwargs.update(dtype=dtype, count=1)\n",
    "        with rio.open(output_image, 'w', **kwargs) as dst: \n",
    "            # write to the final file\n",
    "            #dst.write(output.astype(rio.uint8), 1)\n",
    "            dst.write(output.astype(dtype), 1)\n",
    "            #dst.write_mask(mask)\n",
    "            if not binaryClass:\n",
    "                colors = {\n",
    "                    1: (12,42,235, 255),\n",
    "                    2: (41, 210, 219,255),\n",
    "                    3: (255, 214, 117, 255),\n",
    "                    4: (171, 224, 85, 255),\n",
    "                    5: (12, 100, 1, 255),\n",
    "                    6: (0, 192, 32, 255),\n",
    "                    7: (62, 62, 62, 255),\n",
    "                    8: (160, 160, 160, 255),\n",
    "                    9: (160, 37, 6, 255)\n",
    "                    }\n",
    "                dst.write_colormap(1, colors)\n",
    "        \n",
    "        #print(\"WROTE IT\")\n",
    "        \"\"\"if binaryClass:\n",
    "            cleaned_loc = os.path.abspath(r\"..\\EPCExtent_30cm\\Orthos_Segmentedv3_Classifiedv3_cleaned\")\n",
    "            cleanedOutput_loc = os.path.join(cleaned_loc, os.path.basename(ortho_file).replace(\".tif\",\"_cleaned.tif\"))\n",
    "            cleanedOutput = cleanIt(output, inData=bands, inClass=binaryClass, inprofile=kwargs)\n",
    "            kwargs.update(nodata=0, dtype=rio.float32)\n",
    "            with rio.open(cleanedOutput_loc, 'w', **kwargs) as dst:\n",
    "                #dst.write(cleanedOutput.astype(rio.uint8), 1)\n",
    "                dst.write(cleanedOutput.astype(rio.float32), 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Classified to {os.path.abspath(output_image)}. \\nClassification took {datetime.now()-start}\")\n",
    "        \n",
    "        return output_image\n",
    "        #except:\n",
    "        #    print(f\"Failed for {output_image}\")\n",
    "        \n",
    "\n",
    "def createBinaryClassifiedRaster(models, ortho_file, classifiedFiles_loc, roundConfidence=False, cleanPixels=False, overwrite=False):\n",
    "    \"\"\"memoryUsage = psutil.virtual_memory().percent\n",
    "    waitStart = wait_start()\n",
    "    if memoryUsage>50 and waitStart:\n",
    "        print(f\"Current time is {datetime.now().time()} on day # {datetime.today().weekday()} and memory usage at {memoryUsage}%. Waiting {round(waitStart.seconds/60/60,1)} hours\")\n",
    "        sleep(waitStart.seconds)\n",
    "        print(f\"Current time is {datetime.now().time()}. Resuming...\")\n",
    "    \"\"\"\n",
    "        \n",
    "    start = datetime.now()\n",
    "    day = datetime.now().strftime(\"%Y%m%d\")\n",
    "    daynum = 1\n",
    "    output_file = os.path.basename(ortho_file).replace(\".tif\", f\"_BinaryOptunaTunerGBLM.tif\")\n",
    "    output_path = os.path.join(classifiedFiles_loc, output_file)#_{day}-{daynum}.tif\"))\n",
    "    cleaned_loc = os.path.abspath(r\"..\\EPCExtent_30cm\\Orthos_Segmentedv3_Classifiedv3_cleaned\")\n",
    "    cleanedOutput_path = os.path.join(cleaned_loc, output_file.replace(\".tif\",\"_cleaned.tif\"))\n",
    "    \n",
    "    if os.path.exists(output_path) and not overwrite:\n",
    "        #print(f\"File exists ({output_image})\")\n",
    "        return output_path\n",
    "    \n",
    "    try:\n",
    "        with rio.open(ortho_file) as src:\n",
    "            kwargs = src.profile\n",
    "            kwargs.update(\n",
    "                dtype= rio.uint8,\n",
    "                count= 1,\n",
    "            )\n",
    "            data = src.read()\n",
    "            descs = list(src.descriptions)\n",
    "    except:\n",
    "        print(f\"ERROR: Unable to open {os.path.basename(ortho_file)}. Skipping\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        all_feature_names = set( fn for model_path in models.values() for fn in pickle.load(open(model_path, 'rb')).feature_name_ )\n",
    "        #feature_names = classification_model.feature_name_\n",
    "    except:\n",
    "        all_feature_names = set( fn for model_path in models.values() for fn in pickle.load(open(model_path, 'rb')).feature_name() )\n",
    "        #feature_names = classification_model.feature_name()\n",
    "    \n",
    "    bands = {desc:data[ib] for ib, desc in enumerate(descs)}\n",
    "    \n",
    "    #This is a training dataset not created with the others, but may be in the classifier. Create and add\n",
    "    if \"RGBNmean\" in all_feature_names:\n",
    "        #print(\"Adding RGBNmean\")\n",
    "        rgbnMean = np.nanmean(data[:4], axis=0).astype(data.dtype)\n",
    "        bands[\"RGBNmean\"] = rgbnMean\n",
    "        \n",
    "    if \"RED_LHE\" in all_feature_names:\n",
    "        path = os.path.basename(ortho_file).split(\"_\")[0]\n",
    "        row = os.path.basename(ortho_file).split(\"_\")[1]\n",
    "        print(f\"PATH: {path}, ROW: {row}\")\n",
    "        localHisto_file = findFile(path, row, histoEqual_loc)\n",
    "        with rio.open(localHisto_file) as src:\n",
    "            for i, desc in enumerate(src.descriptions):\n",
    "                bands[desc] = src.read(i+1)\n",
    "    \n",
    "    for name in all_feature_names:\n",
    "        if \"Sentinel2_\" in name:\n",
    "            sentBand = name.split(\"_\")[-1]\n",
    "            sentFile = getSentinelBandFile(sentBand, \"../Sentinel2Data/\")\n",
    "            bands[name] = readResampledWindow(sentFile, ortho_file, returnData=True)\n",
    "    #try:\n",
    "    if roundConfidence:\n",
    "        dtype = np.uint8\n",
    "    else:\n",
    "        dtype = np.float32\n",
    "    kwargs.update(\n",
    "        dtype = dtype,\n",
    "        count = 9,\n",
    "        nodata = 0\n",
    "    )\n",
    "    #output_path = f\"C:/Users/BenJames/Downloads/{os.path.basename(output_path)}\"\n",
    "    with rio.open(output_path, 'w', **kwargs) as dst: \n",
    "        for i, class_name in enumerate(models):\n",
    "            if class_name == \"Asphault\" or class_name == \"Pool\" or class_name == \"Barren\" or class_name == \"PondLake\" or class_name == \"Impervious\":\n",
    "                continue\n",
    "            #print(f\"on model {class_name}\")\n",
    "            model = pickle.load(open(models[class_name], 'rb'))\n",
    "            confidenceClassification = createClassificationFromModel(model, all_bands=bands)\n",
    "            #if class_name == \"DenseVeg\":\n",
    "            #    return confidenceClassification\n",
    "            # Model provides confidence values from 0 to 1, round to binary if specifid\n",
    "            if roundConfidence:\n",
    "                confidenceClassification = np.rint(confidenceClassification)\n",
    "            # write to the final file\n",
    "            dst.write(confidenceClassification.astype(dtype), i+1)\n",
    "            dst.set_band_description(i+1, class_name)\n",
    "            \n",
    "            if cleanPixels and roundConfidence:\n",
    "                cleanedOutput = cleanIt(binaryClassification==1, inData=bands, inClass=class_name)\n",
    "                with rio.open(cleanedOutput_path, \"w\", **kwargs) as cdst:\n",
    "                    cdst.write(cleanedOutput.astype(dtype), i+1)\n",
    "                    cdst.set_band_description(i+1, class_name)\n",
    "                    \n",
    "                    \n",
    "    print(f\"Classified to {os.path.abspath(output_path)}. \\nClassification took {datetime.now()-start}\")\n",
    "    #except:\n",
    "    #    print(f\"Failed on {output_path}. Returning None\")\n",
    "    #    try:\n",
    "    #        os.remove(output_path)\n",
    "    #    except:\n",
    "    #        print(f\"Unable to remove {output_path}\")\n",
    "    #        \n",
    "    #    output_path = None\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "\n",
    "def createClassificationFromModel(model, all_bands, model_features=None):\n",
    "    for name, band in all_bands.items():\n",
    "        numnans = len(band[np.isnan(band)])\n",
    "        if numnans != 0:\n",
    "            print(name, numnans)\n",
    "            \n",
    "    if model_features == None:\n",
    "        model_features = model.feature_name()\n",
    "        \n",
    "    features = {fn: all_bands[fn] for fn in model_features}# if fn in descs}\n",
    "    featureArrays = list(features.values())\n",
    "    \n",
    "    trainingFeatures = np.stack(featureArrays)\n",
    "    \n",
    "    #print(\"Input Data shape\", data.shape)\n",
    "    \n",
    "    # read the image into the proper format, adding indices if necessary\n",
    "    img_swp = np.moveaxis(trainingFeatures, 0, -1)\n",
    "    img_flat = img_swp.reshape(-1, img_swp.shape[-1])\n",
    "    \n",
    "    #flatten bands along axis\n",
    "    img_flat = img_swp.reshape(-1, img_swp.shape[-1])\n",
    "    \n",
    "    # remove no data values, store the indices for later use\n",
    "    # a later cell makes the assumption that all bands have identical no-data value arrangements\n",
    "    m = np.ma.masked_invalid(img_flat)\n",
    "    #print(\"Mask Values\", np.unique(m.mask, return_counts=True))\n",
    "    to_predict = img_flat[~m.mask].reshape(-1, img_flat.shape[-1])\n",
    "    #print(\"TO PREDICT SHAPE\", to_predict.shape)\n",
    "    \n",
    "    # predict\n",
    "    #print(\"Beginning prediction...\", datetime.now())\n",
    "    img_preds = model.predict(to_predict)\n",
    "    \n",
    "    #img_preds = [np.argmax(x) for x in img_preds]\n",
    "    \n",
    "    # add the prediction back to the valid pixels (using only the first band of the mask to decide on validity)\n",
    "    # resize to the original image dimensions\n",
    "    output = np.zeros(img_flat.shape[0])\n",
    "    output[~m.mask[:,0]] = img_preds#.flatten()\n",
    "    output = output.reshape(*img_swp.shape[:-1])\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def cleanupStructures(array, value):\n",
    "    structure = array == value\n",
    "    array[structure] = 0\n",
    "    for i in range(3):\n",
    "        structure = binary_opening(structure, square(3))\n",
    "        structure = binary_closing(structure, square(5))\n",
    "    \n",
    "    array[structure] = value\n",
    "    array = fillHolesInClass(array, value, 90)\n",
    "    array = np.where(structure==1, value, array)\n",
    "    \n",
    "    #for i in range(5):\n",
    "    #    print(f\"{i} iteration\"\n",
    "    #non_structure_mode = modal(array, square(21), mask=array==9)\n",
    "    #array = np.where(array==255, non_structure_mode, array)\n",
    "    \n",
    "    return array\n",
    "\n",
    "    \n",
    "def cleanIt(inarray, inData, inClass, inprofile):\n",
    "    if inClass==\"Pool\":\n",
    "        outarray = cleanupPool(inarray, inData)\n",
    "    elif inClass==\"Asphault\":\n",
    "        outarray = cleanupAsphault(inarray, inprofile)\n",
    "    elif inClass==\"DenseVeg\":\n",
    "        outarray = cleanupDenseVeg(inarray, inData)\n",
    "    elif inClass==\"SparseVeg\":\n",
    "        outarray = cleanupSparseVeg(inarray, inData)\n",
    "    elif inClass==\"PondLake\":\n",
    "        outarray = cleanupPondLake(inarray, inData)\n",
    "    else:\n",
    "        outarray = inarray\n",
    "        \n",
    "    return outarray\n",
    "\n",
    "\n",
    "def classifyTiles(df, model, forceStart=False, overwrite=False, binaryClass=None):\n",
    "    print(f\"\\n\\nStarting processing for {len(df)} tiles\\n\")\n",
    "    classifiedFiles = []\n",
    "    for i, r in df.iterrows():\n",
    "        waitTime = wait_start(force=forceStart)\n",
    "        if waitTime:\n",
    "            buildVRT(classifiedFiles_loc, \"EPC_30cmOrthoSegmented_Classified.vrt\")\n",
    "            pause_until = datetime.now() + waitTime\n",
    "            print(f\"waiting {timedelta(seconds=waitTime.seconds)} until {pause_until.strftime('%H:%m')}\")\n",
    "            sleep(waitTime.seconds)\n",
    "            \n",
    "        print(f\"Starting {r.path}_{r.row} @ {datetime.now()}\")\n",
    "        #try:\n",
    "        result = createClassifiedRaster(classification_model=model,\n",
    "                                            ortho_file=r.OrthoFile,\n",
    "                                            classifiedFiles_loc=classifiedFiles_loc,\n",
    "                                            binaryClass=binaryClass,\n",
    "                                            overwrite=overwrite)\n",
    "        classifiedFiles.append(result)\n",
    "        #except:\n",
    "        #    print(f\"FAILED FOR {r.OrthoFile}\")\n",
    "        \n",
    "    return classifiedFiles\n",
    "\n",
    "\n",
    "def fillHolesInClass(array, class_num, size_max):\n",
    "    class_bool = array == class_num\n",
    "    array[class_bool] = 255\n",
    "    filled = remove_small_holes(class_bool, area_threshold=size_max, connectivity=1)\n",
    "    array[filled] = class_num\n",
    "    return array\n",
    "\n",
    "\n",
    "def smoothValue(array, value, selem=square(5)):\n",
    "    onlys = array == value\n",
    "    for i in range(2):\n",
    "        onlys = binary_closing(onlys, selem)\n",
    "    array[onlys] = value\n",
    "    #array[~asphault] = 255\n",
    "    return array\n",
    "\n",
    "\n",
    "def cleanupAsphault(a, inprofile):\n",
    "    # 1. Drop where any slope percentage is less than 10 degrees\n",
    "    slope = getSlopeArray(inprofile)\n",
    "    slope_thres = np.where(slope<10, 0, 1)\n",
    "    slope_thres = remove_small_objects(slope_thres, min_size=1000, connectivity=1)\n",
    "    a[slope_thres==1] = 0\n",
    "    \n",
    "    # 2. Burn in mix-used paths\n",
    "    \n",
    "    # 3. Remove groups of asphault pixels smaller than 1000\n",
    "    a = remove_small_objects(a==1, min_size=1000, connectivity=1)\n",
    "    \n",
    "    # 4. Fill in holes\n",
    "    a = fillHolesInClass(a, 9, 90)\n",
    "    \n",
    "    # 5. Open and Dilate and to smooth edges\n",
    "    for i in range(5):\n",
    "        a = binary_opening(a, square(3))\n",
    "        a = binary_closing(a, square(5))\n",
    "        \n",
    "    return a \n",
    "\n",
    "\n",
    "def cleanupImpervious(a, indata):\n",
    "    a[indata[\"MSAVI\"] > 30000] = 0\n",
    "    \n",
    "    return a\n",
    "\n",
    "\n",
    "def cleanupPool(a, indata):\n",
    "    a = remove_small_objects(a==1, min_size=50, connectivity=1)\n",
    "    a = remove_small_holes(a==1, area_threshold=5, connectivity=1)\n",
    "    for i in range(2):\n",
    "        a = binary_opening(a, disk(2))\n",
    "    \n",
    "    a[indata[\"HAG\"] > 8] = 0\n",
    "    a[indata[\"NIR\"] > 10000] = 0\n",
    "    a[indata[\"BLUEness\"] < 35000] = 0\n",
    "    \n",
    "    return a\n",
    "\n",
    "\n",
    "def cleanupPondLake(a, indata):\n",
    "    # drop ponds/lakes smaller than x?\n",
    "    #a = removeClassSmaller(a, 1, 10000) \n",
    "    ndvi_lim = indata[\"NDVI\"]<12000\n",
    "    ndpi_lim = indata[\"NDPI\"]>26000\n",
    "    blueness_lim = indata[\"BLUEness\"]<=40000\n",
    "    a[ndvi_lim & ndpi_lim & blueness_lim] = 1\n",
    "    a[a==1 & (~ndvi_lim | ~ndpi_lim | ~blueness_lim)] = 0\n",
    "    a = remove_small_objects(a==1, min_size=5000, connectivity=1)\n",
    "    # smooth ponds\n",
    "    a = smoothValue(a, 1, selem=disk(5))\n",
    "    \n",
    "    return a\n",
    "\n",
    "\n",
    "def cleanupIrrigated(a, value, indata=None):\n",
    "     # set very green veg with high index and low brightness to irrigated\n",
    "    a = remove_small_holes(a==1, area_threshold=5, connectivity=1)\n",
    "    a = smoothValue(a, value, selem=disk(5))\n",
    "    a[indata[\"MSAVI\"]<=30000] = 0\n",
    "    \n",
    "    return a\n",
    "        \n",
    "def cleanupDenseVeg(a, value):\n",
    "    a = smoothValue(a, value, selem=disk(2)) # smooth dense veg\n",
    "    return a\n",
    "\n",
    "\n",
    "def cleanupSparseVeg(a, value):\n",
    "    a = smoothValue(a, value, selem=square(5)) # smooth sparse veg\n",
    "    return a\n",
    "\n",
    "\n",
    "def getSlopeArray(tprofile):\n",
    "    with rio.open(r\"D:/EPC_DEM_2015.vrt\") as src:\n",
    "        intrans = tprofile[\"transform\"]\n",
    "        inheight = tprofile[\"height\"]\n",
    "        inwidth = tprofile[\"width\"]\n",
    "        inres = intrans.a\n",
    "        win = from_bounds(intrans.c, intrans.f-(inheight*inres), intrans.c+(inwidth*inres), intrans.f, src.transform).round_offsets().round_lengths()\n",
    "        \n",
    "        if win.col_off < src.width and win.row_off < src.height:\n",
    "            dem = src.read(1, window=win, out_shape=(tprofile[\"height\"], tprofile[\"width\"]))\n",
    "            slope = rd.TerrainAttribute(rd.rdarray(dem, no_data=src.nodata), attrib=\"slope_degrees\")\n",
    "            # if the read window includes an area with no-data, that means that it's outside of the region for the 2015 2ft DEM. Fill in with NED data\n",
    "            if src.nodata in dem:\n",
    "                slope10m = get10mNEDSlope(intrans,inheight,inwidth)\n",
    "                slope = np.where(dem==src.nodata, slope10m, slope)\n",
    "        else:\n",
    "            # the entire window is outside of the extent of the 2ft DEM coverage; use NED\n",
    "            slope = get10mNEDSlope(intrans,inheight,inwidth)\n",
    "            \n",
    "    return slope\n",
    "\n",
    "\n",
    "def get10mNEDSlope(intrans, inheight, inwidth):\n",
    "    inres = intrans.a\n",
    "    with rio.open(\"../OtherData/10mDEMs/DEM10mNED_slope.tif\") as src10m:\n",
    "        win = from_bounds(intrans.c, intrans.f-(inheight*inres), intrans.c+(inwidth*inres), intrans.f, src10m.transform).round_offsets().round_lengths()\n",
    "        slope = src10m.read(1, window=win, out_shape=(inheight, inwidth))\n",
    "        \n",
    "    return slope\n",
    "\n",
    "\n",
    "def burnShadows(a, indata, rednessNorm):\n",
    "    # burn in shadows where low values, but not pool or pond\n",
    "    rgbnMean = np.nanmean([indata[\"RED\"], indata[\"GREEN\"], indata[\"BLUE\"], indata[\"NIR\"]], axis=0)\n",
    "    \n",
    "    #a[(rgbnMean<10282) & (a!=1) & (a!=2)] = 10\n",
    "    \n",
    "    # if classed as pool, NIR is below 5k and rednessNorm > 0.15\n",
    "    a[(a==1) & (indata[\"NIR\"]<=5000) & (rednessNorm > 0.15)] = 10\n",
    "    \n",
    "    return a\n",
    "\n",
    "\n",
    "def getBandByDescription(file, bandDesc):\n",
    "    if type(bandDesc) == list:\n",
    "        getBands = bandDesc[:]\n",
    "    elif type(bandDesc) == str:\n",
    "        getBands = [bandDesc]\n",
    "        \n",
    "    bandDict = {}\n",
    "    with rio.open(file) as src:\n",
    "        for band in getBands:\n",
    "            try:\n",
    "                index = src.descriptions.index(band)\n",
    "            except:\n",
    "                raise ValueError(f\"Unable to find band {band} in {file}\")    \n",
    "            # have to increment index since tiff indicies start at 1 and not 0\n",
    "            bandDict[band] = src.read(index+1)\n",
    "            \n",
    "    if type(bandDesc) != list:\n",
    "        return bandDict[bandDesc]\n",
    "    \n",
    "    return bandDict\n",
    "\n",
    "\n",
    "def calcNessNorm(file, target_band):\n",
    "    bands = getBandByDescription(file, bandDesc=[\"RED\", \"GREEN\", \"BLUE\", \"NIR\"])\n",
    "    #return bands\n",
    "    tband = bands[target_band]\n",
    "    obands = np.stack([array for name, array in bands.items() if name != target_band])\n",
    "    nessNorm = (tband-np.nanmean(obands,axis=0))/(tband+np.nanmean(obands,axis=0))\n",
    "    mask = np.all(np.stack(bands.values())==0, axis=0)\n",
    "    nessNorm[mask]=0\n",
    "    \n",
    "    return nessNorm\n",
    "\n",
    "\n",
    "def calcRGDiff(file):\n",
    "    bands = getBandByDescription(file, bandDesc=[\"RED\", \"GREEN\"])\n",
    "    rgDiff = bands[\"RED\"]-bands[\"GREEN\"]\n",
    "    \n",
    "    return rgDiff\n",
    "\n",
    " \n",
    "def getCleanedBinary(file, orthoFile, outdir, lcClass, probLimit=0.01, overwrite=False, writeOut=False):\n",
    "    ofile = os.path.join(outdir, os.path.basename(file).replace(\".tif\", \"_clean.tif\"))\n",
    "    #if os.path.exists(ofile) and not overwrite and not returnData:\n",
    "    #    return ofile\n",
    "    if os.path.exists(ofile) and not overwrite:\n",
    "        data_clean = rio.open(ofile).read(1)\n",
    "    else:\n",
    "        with rio.open(file) as src:\n",
    "            data = src.read(1)\n",
    "            kwargs = src.profile\n",
    "        \n",
    "        #orthoFile = glob(orthosDir + \"/\" + \"_\".join(os.path.basename(file).split(\"_\")[:2]) + \"*.tif\")[0]\n",
    " \n",
    "        data = np.where(data>probLimit, 1, 0)\n",
    "        \n",
    "    \n",
    "        if lcClass == \"Asphault\":\n",
    "            data_clean = cleanupAsphault(data, kwargs)\n",
    "        elif lcClass == \"Pool\":\n",
    "            #rint(\"Cleaning Pool\")\n",
    "            trainingData = getBandByDescription(orthoFile, [\"HAG\",\"NIR\",\"BLUEness\"])\n",
    "            data_clean = cleanupPool(data, trainingData)\n",
    "            #print(\"Cleaned Pool\")\n",
    "        elif lcClass == \"PondLake\":\n",
    "            trainingData = getBandByDescription(orthoFile, [\"NDVI\",\"NDPI\",\"BLUEness\"])\n",
    "            data_clean = cleanupPondLake(data, trainingData)\n",
    "        elif lcClass == \"Irrigated\":\n",
    "            trainingData = getBandByDescription(orthoFile, [\"MSAVI\"])\n",
    "            data_clean = cleanupIrrigated(data, trainingData)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown class type {lcClass}\")\n",
    "    \n",
    "        if writeOut:\n",
    "            kwargs.update(dtype=np.uint8, nodata=0)\n",
    "            with rio.open(ofile, \"w\", nbits=1, **kwargs) as dst:\n",
    "                dst.write(data_clean.astype(np.uint8), 1)\n",
    "            return ofile\n",
    "    \n",
    "    return data_clean.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BenJames\\anaconda3\\envs\\geospatial\\lib\\site-packages\\geopandas\\geodataframe.py:294: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for f in features_lst:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4846 total tile indicies\n"
     ]
    }
   ],
   "source": [
    "rasterDataDir = os.path.abspath(\"R:/ProjectData/PAG2019\")\n",
    "classifiedFiles_loc = os.path.join(rasterDataDir, \"EPCExtent_30cm/Orthos_Segmented_Classifiedv3\")\n",
    "binaryClassifiedFiles_loc = os.path.join(rasterDataDir, \"EPCExtent_30cm/Orthos_Segmented_Classifiedv3_Binary\")\n",
    "histoEqual_loc = os.path.join(rasterDataDir, \"EPCExtent_30cm/Orthos_LocalHistogramEqualized\")\n",
    "cleanBinaryDir = os.path.join(rasterDataDir, r\"EPCExtent_30cm/Orthos_Segmented_Classifiedv3_Binary_cleaned\")\n",
    "\n",
    "\n",
    "\n",
    "ortho30cmvrt_loc = os.path.join(rasterDataDir, \"EPCExtent_30cm/Orthos/EPC_30cmOrtho_2019.vrt\")\n",
    "hagvrt_loc = os.path.join(rasterDataDir, \"EPCExtent_30cm/Elevation_80cmNPS/HAG_2015/HAG_2015.vrt\")\n",
    "\n",
    "#propsDir = r\"../EPCExtent_30cm/Orthos_Segmentedv2_properties\"\n",
    "orthosDir = os.path.join(rasterDataDir, r\"EPCExtent_30cm/Orthos_Segmentedv3\")\n",
    "os.makedirs(classifiedFiles_loc, exist_ok=True)\n",
    "os.makedirs(cleanBinaryDir, exist_ok=True)\n",
    "\n",
    "tindex = gpd.read_file(\"../vectors/Ortho_5kSubIndex.gpkg\")\n",
    "#to_process = gpd.read_file(\"../temp/ToProcess.gpkg\")\n",
    "\n",
    "print(f\"{len(tindex)} total tile indicies\")\n",
    "tindex[\"OrthoFile\"] = tindex.apply(lambda r: findFile(path=r.path, row=r.row, directory=orthosDir), axis=1)\n",
    "#tindex[\"PropsFile\"] = tindex.apply(lambda r: findFile(path=r.path, row=r.row, files=propsDir), axis=1)\n",
    "\n",
    "# ignore tiles which don't have input variables created\n",
    "#tindex = tindex[(~pd.isnull(tindex.OrthoFile))]\n",
    "#print(f\"{len(tindex)} tile indicies with training data already built\")\n",
    "\n",
    "\n",
    "#prioritize central tucson and work out from there\n",
    "tindex[\"centroid\"] = tindex.geometry.centroid\n",
    "central_tile = tindex[(tindex.path == \"W1004789\") & (tindex.row == \"N449850\")]\n",
    "central_point = central_tile.centroid.values[0]\n",
    "tindex[\"DistToCenter\"] = tindex.centroid.apply(lambda c: int(c.distance(central_point)))\n",
    "tindex.sort_values(by=\"DistToCenter\", inplace=True)\n",
    "\n",
    "classifiers = {mp.split(\"_\")[0].split(\"Binary\")[-1]:mp for mp in glob(\"../Models/*20220103.sav\")}\n",
    "\n",
    "#tt = tindex[((tindex.path == \"W989789\") & (tindex.row == \"W439850\")) |\n",
    "#            ((tindex.path == \"W1004789\") & (tindex.row == \"W449850\")) |\n",
    "#            ((tindex.path == \"W1004789\") & (tindex.row == \"W344850\")) |\n",
    "#            ((tindex.path == \"W979789\") & (tindex.row == \"W419850\")) |\n",
    "#            ((tindex.path == \"W1004789\") & (tindex.row == \"W444850\")) |\n",
    "#            ((tindex.path == \"W919789\") & (tindex.row == \"W404850\")) \n",
    "#           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1409 tiles with LULC files to be created\n",
      "1352 tiles (95.95%) that need training data created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22931ea9190>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "targetDistance_miles = 25\n",
    "cleanedLULC_dir = os.path.join(rasterDataDir, r\"EPCExtent_30cm\\LULC\")\n",
    "\n",
    "tindex_target = tindex[tindex.DistToCenter <= targetDistance_miles*5280].copy()\n",
    "tindex_target[\"CleanedLULCFile\"] = tindex_target.apply(lambda r: cleanedLULC_dir+f\"/{r.path}_{r.row}_LULC2019.tif\", axis=1)\n",
    "tindex_target[\"CleanedLULCFile\"] = tindex_target[\"CleanedLULCFile\"].apply(lambda f: f if os.path.exists(f) else None)\n",
    "tindex_toDo = tindex_target[tindex_target[\"CleanedLULCFile\"].isnull()].copy()\n",
    "tindex_toDo[\"OrthoFile\"] = tindex_toDo.apply(lambda r: glob(f\"{orthosDir}/{r.path}_{r.row}_*.tif\"), axis=1)\n",
    "tindex_toDo[\"OrthoFile\"] = tindex_toDo[\"OrthoFile\"].apply(lambda f: f[0] if len(f) == 1 else None)\n",
    "print(f\"{len(tindex_toDo)} tiles with LULC files to be created\")\n",
    "print(f\"{len(tindex_toDo[tindex_toDo.OrthoFile.isnull()])} tiles ({round(100*len(tindex_toDo[tindex_toDo.OrthoFile.isnull()])/len(tindex_toDo),2)}%) that need training data created\")\n",
    "tindex_toDo.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "toDo_OrthoFiles = Parallel(n_jobs=6, verbose=5, backend=\"loky\")(delayed(segmentWindowV3)(boxrow,\n",
    "                                                                                         v3TrainingStack_dir,\n",
    "                                                                                         ortho30cmvrt_loc,\n",
    "                                                                                         hagvrt_loc,\n",
    "                                                                                         returnArray=False,\n",
    "                                                                                         writeOutStack=True,\n",
    "                                                                                         overwrite=False)\n",
    "                                                                for i, boxrow in tindex_toDo.iterrows())\n",
    "toDo_LHEFiles = Parallel(n_jobs=6, verbose=5, backend=\"loky\")(delayed(createLocalHistogramOrtho)(boxrow,\n",
    "                                                                                                 histoEqual_loc,\n",
    "                                                                                                 ortho30cmvrt_loc,\n",
    "                                                                                                 overwrite=False)\n",
    "                                                              for i, boxrow in tindex_toDo.iterrows())\n",
    "\n",
    "tindex_target[\"OrthoFile\"] = tindex_target.apply(lambda r: glob(f\"{orthosDir}/{r.path}_{r.row}_*.tif\"), axis=1)\n",
    "tindex_target[\"OrthoFile\"] = tindex_target[\"OrthoFile\"].apply(lambda f: f[0] if len(f) == 1 else None)\n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Group Parallelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20220103'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tday = datetime.now().strftime(\"%Y%m%d\")\n",
    "tday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "asphaultFiles = Parallel(n_jobs=1)(delayed(createClassifiedRaster)(pickle.load(open(classifiers[\"Asphault\"], \"rb\")),\n",
    "                                                                   ortho_file= row.OrthoFile,\n",
    "                                                                   classifiedFiles_loc= binaryClassifiedFiles_loc,\n",
    "                                                                   binaryClass= \"Asphault\",\n",
    "                                                                   suffix = tday,\n",
    "                                                                   overwrite= False) for i, row in tindex_toDo.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "poolFiles = Parallel(n_jobs=1)(delayed(createClassifiedRaster)(pickle.load(open(classifiers[\"Pool\"], \"rb\")),\n",
    "                                                               ortho_file= row.OrthoFile,\n",
    "                                                               classifiedFiles_loc= binaryClassifiedFiles_loc,\n",
    "                                                               binaryClass= \"Pool\",\n",
    "                                                               suffix = tday,\n",
    "                                                               overwrite= False) for i, row in tindex_toDo.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "imperviousFiles = Parallel(n_jobs=1)(delayed(createClassifiedRaster)(pickle.load(open(\"./Models/lightGBMBinaryImpervious_OptunaTuner_20211130.sav\", \"rb\")),\n",
    "                                                                     ortho_file= row.OrthoFile,\n",
    "                                                                     classifiedFiles_loc= binaryClassifiedFiles_loc,\n",
    "                                                                     binaryClass= \"Impervious\",\n",
    "                                                                     suffix= tday,\n",
    "                                                                     overwrite= False) for i, row in tindex_toDo.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "barrenFiles = Parallel(n_jobs=1)(delayed(createClassifiedRaster)(pickle.load(open(\"Models\\lightGBMBinaryBarren_LGBMTunerOptimum_20211130.sav\", \"rb\")),\n",
    "                                                                 ortho_file= row.OrthoFile,\n",
    "                                                                 classifiedFiles_loc= binaryClassifiedFiles_loc,\n",
    "                                                                 binaryClass= \"Barren\",\n",
    "                                                                 suffix = tday,\n",
    "                                                                 overwrite= False) for i, row in tindex_toDo.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pondLakeFiles = Parallel(n_jobs=1)(delayed(createClassifiedRaster)(pickle.load(open('./Models\\\\lightGBMBinaryPondLake_LGBMTuner_20211118.sav', \"rb\")),\n",
    "                                                                     ortho_file= row.OrthoFile,\n",
    "                                                                     classifiedFiles_loc= binaryClassifiedFiles_loc,\n",
    "                                                                     binaryClass= \"PondLake\",\n",
    "                                                                     suffix = tday,\n",
    "                                                                     overwrite= False) for i, row in tindex_toDo.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "files = Parallel(n_jobs=1)(delayed(createBinaryClassifiedRaster)(models=classifiers,\n",
    "                                                                 ortho_file=row.OrthoFile,\n",
    "                                                                 classifiedFiles_loc=classifiedFiles_loc,\n",
    "                                                                 overwrite=False,\n",
    "                                                                 roundConfidence=True) for i, row in tindex_toDo.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tindex_target[\"BinaryStackFile\"] = tindex_target.apply(lambda r: glob(f\"../EPCExtent_30cm/Orthos_Segmented_Classifiedv3/{r.path}_{r.row}_TrainingStackV3_BinaryOptunaTunerGBLM.tif\"), axis=1)\n",
    "tindex_target[\"AsphaultFile\"] = tindex_target.apply(lambda r: glob(f\"../EPCExtent_30cm/Orthos_Segmented_Classifiedv3_Binary/{r.path}_{r.row}_TrainingStackV3_AsphaultBinaryOptunaTunerGBLM_20211115.tif\"), axis=1)\n",
    "tindex_target[\"PoolFile\"] = tindex_target.apply(lambda r: glob(f\"../EPCExtent_30cm/Orthos_Segmented_Classifiedv3_Binary/{r.path}_{r.row}_TrainingStackV3_PoolBinaryOptunaTunerGBLM_20211115.tif\"), axis=1)\n",
    "tindex_target[\"PondLakeFile\"] = tindex_target.apply(lambda r: glob(f\"../EPCExtent_30cm/Orthos_Segmented_Classifiedv3_Binary/{r.path}_{r.row}_TrainingStackV3_PondLakeBinaryOptunaTunerGBLM_20211115.tif\"), axis=1)\n",
    "tindex_target[\"ImperviousFile\"] = tindex_target.apply(lambda r: glob(f\"../EPCExtent_30cm/Orthos_Segmented_Classifiedv3_Binary/{r.path}_{r.row}_TrainingStackV3_ImperviousBinaryOptunaTunerGBLM_20211115.tif\"), axis=1)\n",
    "tindex_target[\"BarrenFile\"] = tindex_target.apply(lambda r: glob(f\"../EPCExtent_30cm/Orthos_Segmented_Classifiedv3_Binary/{r.path}_{r.row}_TrainingStackV3_BarrenBinaryOptunaTunerGBLM_20211130.tif\"), axis=1)\n",
    "\n",
    "tindex_target[\"BinaryStackFile\"] = tindex_target[\"BinaryStackFile\"].apply(lambda f: f[0] if len(f) == 1 else None)\n",
    "tindex_target[\"AsphaultFile\"] = tindex_target[\"AsphaultFile\"].apply(lambda f: f[0] if len(f) == 1 else None)\n",
    "tindex_target[\"PoolFile\"] = tindex_target[\"PoolFile\"].apply(lambda f: f[0] if len(f) == 1 else None)\n",
    "tindex_target[\"PondLakeFile\"] = tindex_target[\"PondLakeFile\"].apply(lambda f: f[0] if len(f) == 1 else None)\n",
    "tindex_target[\"ImperviousFile\"] = tindex_target[\"ImperviousFile\"].apply(lambda f: f[0] if len(f) == 1 else None)\n",
    "tindex_target[\"BarrenFile\"] = tindex_target[\"BarrenFile\"].apply(lambda f: f[0] if len(f) == 1 else None)\n",
    "tindex_target = tindex_target.dropna().reset_index()\n",
    "print(tindex_target.shape)\n",
    "%chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "values_2019_lookup = {'PondLake': 1, 'Pool': 2, 'Barren': 3, 'SparseVeg': 4, 'DenseVeg': 5,\n",
    "                      'IrrigatedLand': 6, 'Asphault': 7, 'Impervious': 8, 'Structure': 9}\n",
    "                      \n",
    "def buildCleanLULC(rowIndex, r, outdir, overwrite=False):\n",
    "    try:\n",
    "        start = datetime.now()\n",
    "        ofile = f\"{r.path}_{r.row}_TrainingStackV3_BinaryStack_cleaned.tif\"\n",
    "        output_lulc = os.path.join(outdir, ofile)\n",
    "        if os.path.exists(output_lulc) and not overwrite:\n",
    "            return output_lulc\n",
    "        \n",
    "        with rio.open(r.OrthoFile) as src:\n",
    "            kwargs = src.profile\n",
    "            \n",
    "        asphault = getCleanedBinary(r.AsphaultFile, r.OrthoFile, cleanBinaryDir, \"Asphault\", writeOut=False)\n",
    "        pool = getCleanedBinary(r.PoolFile, r.OrthoFile, cleanBinaryDir, \"Pool\", writeOut=False)\n",
    "        pondLake = getCleanedBinary(r.PondLakeFile, r.OrthoFile, cleanBinaryDir, \"PondLake\", probLimit=0.5, writeOut=False)\n",
    "        \n",
    "        binaryFile = r.BinaryStackFile\n",
    "        \n",
    "        irrigatedLand = getBandByDescription(binaryFile, bandDesc=\"IrrigatedLand\")\n",
    "        irrigatedLand = cleanupIrrigated(irrigatedLand, 1, getBandByDescription(r.OrthoFile, [\"MSAVI\"]))\n",
    "        structures = cleanupStructures(getBandByDescription(binaryFile, bandDesc=\"Structure\"), 1)\n",
    "        sparseVeg = cleanupSparseVeg(getBandByDescription(binaryFile, bandDesc=\"SparseVeg\"), 1)\n",
    "        denseVeg = cleanupDenseVeg(getBandByDescription(binaryFile, bandDesc=\"DenseVeg\"), 1)\n",
    "        \n",
    "        lulc_array = np.zeros(asphault.shape, dtype=np.uint8)\n",
    "        lulc_array[structures == 1] = values_2019_lookup[\"Structure\"]\n",
    "        lulc_array[pool == 1] = values_2019_lookup[\"Pool\"]\n",
    "        lulc_array[pondLake == 1] = values_2019_lookup[\"PondLake\"]\n",
    "        lulc_array[irrigatedLand == 1] = values_2019_lookup[\"IrrigatedLand\"]\n",
    "        lulc_array[sparseVeg == 1] = values_2019_lookup[\"SparseVeg\"]\n",
    "        lulc_array[asphault == 1] = values_2019_lookup[\"Asphault\"]\n",
    "        lulc_array[denseVeg == 1] = values_2019_lookup[\"DenseVeg\"]\n",
    "        \n",
    "        with rio.open(r.ImperviousFile) as src:\n",
    "            impervious = src.read(1)\n",
    "            impervious = cleanupImpervious(impervious, getBandByDescription(r.OrthoFile, [\"MSAVI\"]))\n",
    "        with rio.open(r.BarrenFile) as src:\n",
    "            barren = src.read(1)\n",
    "            \n",
    "        # stack barren and impervious and pick class with best confidence (highest/argmax)\n",
    "        barrenImp = np.stack([impervious, barren])\n",
    "        bestIndex = np.argmax(barrenImp, axis=0)\n",
    "        lulc_array[(lulc_array==0) & (bestIndex==0)] = values_2019_lookup[\"Impervious\"]\n",
    "        lulc_array[(lulc_array==0) & (bestIndex==1)] = values_2019_lookup[\"Barren\"]\n",
    "        \n",
    "        # cleanup Barren and Impervious\n",
    "        redNessNorm = calcNessNorm(r.OrthoFile, target_band=\"RED\")\n",
    "        blueNessNorm = calcNessNorm(r.OrthoFile, target_band=\"BLUE\")\n",
    "        rbNessDiff = redNessNorm - blueNessNorm\n",
    "        rgDiff = calcRGDiff(r.OrthoFile)\n",
    "        #  These cutoffs are kinda arbitrary based on limited sampling\n",
    "        lulc_array[(lulc_array==values_2019_lookup[\"Impervious\"]) & (rbNessDiff>=0.1) & (rgDiff>=2500)] = values_2019_lookup[\"Barren\"]\n",
    "        lulc_array[(lulc_array==values_2019_lookup[\"Impervious\"]) & (rbNessDiff>=0.06) & (rgDiff<2500)] = values_2019_lookup[\"Barren\"]\n",
    "        lulc_array[(lulc_array==values_2019_lookup[\"Barren\"]) & (rbNessDiff<=0.1) & (rgDiff<2500)] = values_2019_lookup[\"Impervious\"]\n",
    "        \n",
    "        \n",
    "        # burn in shadows where low values, but not pool or pond\n",
    "        lulc_array = burnShadows(lulc_array, getBandByDescription(r.OrthoFile, [\"RED\", \"GREEN\", \"BLUE\", \"NIR\"]), redNessNorm)\n",
    "        \n",
    "        kwargs.update(count=1, dtype=np.uint8, nodata=0)\n",
    "        with rio.open(output_lulc, 'w', **kwargs) as dst: \n",
    "            # write to the final file\n",
    "            dst.write(lulc_array, 1)\n",
    "            colors = {\n",
    "                1: (12,42,235, 255),\n",
    "                2: (41, 210, 219,255),\n",
    "                3: (255, 214, 117, 255),\n",
    "                4: (171, 224, 85, 255),\n",
    "                5: (12, 100, 1, 255),\n",
    "                6: (0, 192, 32, 255),\n",
    "                7: (62, 62, 62, 255),\n",
    "                8: (160, 160, 160, 255),\n",
    "                9: (160, 37, 6, 255),\n",
    "                10: (0, 0, 0, 255)\n",
    "                }\n",
    "            dst.write_colormap(1, colors)\n",
    "        print(f\"Finished with #{rowIndex} - {output_lulc}\")\n",
    "        return output_lulc\n",
    "    \n",
    "    except ValueError as e:\n",
    "        print(f\"Failed for {r.OrthoFile}\\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedLULC_dir = os.path.join(rasterDataDir, r\"EPCExtent_30cm\\LULC\")\n",
    "\n",
    "tindex_target[\"CleanedLULCFile\"] = tindex_target.apply(lambda r: cleanedLULC_dir + f\"/{r.path}_{r.row}_LULC2019.tif\", axis=1)\n",
    "tindex_target[\"CleanedLULCFile\"] = tindex_target[\"CleanedLULCFile\"].apply(lambda f: f if os.path.exists(f) else None)\n",
    "lulc_toDo = tindex_target[tindex_target[\"CleanedLULCFile\"].isnull()].copy()\n",
    "print(f\"{len(lulc_toDo)} LULCs to be build\")\n",
    "\n",
    "cleanLULCFiles = Parallel(n_jobs=10)(delayed(buildCleanLULC)(i, row, cleanBinaryDir, overwrite=True) for i, row in lulc_toDo.iterrows())\n",
    "\n",
    "tindex_target[\"CleanedLULCFile\"] = tindex_target.apply(lambda r: f\"../EPCExtent_30cm/Orthos_Segmented_Classifiedv3_Binary_cleaned/{r.path}_{r.row}_TrainingStackV3_BinaryStack_cleaned.tif\", axis=1)\n",
    "tindex_target[\"CleanedLULCFile\"] = tindex_target[\"CleanedLULCFile\"].apply(lambda f: f if os.path.exists(f) else None)\n",
    "tindex_target = tindex_target[~tindex_target[\"CleanedLULCFile\"].isnull()].copy()\n",
    "\n",
    "%chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.mask import mask\n",
    "\n",
    "railroads = gpd.read_file(\"../OtherData/PC_Railroad/railroad.shp\").unary_union\n",
    "railroads = railroads.buffer(20)\n",
    "washes = gpd.read_file(\"../OtherData/Major_Washes_in_Eastern_Pima_County/Major_Washes_in_Eastern_Pima_County.shp\").unary_union\n",
    "bridges = gpd.read_file(\"../OtherData/BurnIns_2019.gpkg\", layer=\"Bridges\").unary_union\n",
    "minesLand = gpd.read_file(\"../OtherData/BurnIns_2019.gpkg\", layer=\"Barren-Mines\").unary_union\n",
    "\n",
    "\n",
    "def getGeometryMask(geometry, raster):\n",
    "    if geometry.type == \"Polygon\":\n",
    "        geometry = [geometry]\n",
    "    tds, tds_trans = mask(raster, geometry, all_touched=False, crop=False, filled=False)\n",
    "    \n",
    "    return ~tds.mask\n",
    "\n",
    "def burnInValuesOverLULC(rowIndex, row):\n",
    "    \n",
    "    lulc_file = row.CleanedLULCFile\n",
    "    ortho_file = row.OrthoFile\n",
    "    \n",
    "    hag = getBandByDescription(ortho_file, \"HAG\")\n",
    "    with rio.open(lulc_file) as src:\n",
    "        lulc = src.read(1)\n",
    "        kwargs = src.profile\n",
    "        bnds = src.bounds\n",
    "        railroad_mask = getGeometryMask(railroads, src)[0]\n",
    "        washes_mask = getGeometryMask(washes, src)[0]\n",
    "        bridges_mask = getGeometryMask(bridges, src)[0]\n",
    "        mines_mask = getGeometryMask(minesLand, src)[0]\n",
    "    print(lulc.shape)\n",
    "    print(mines_mask.shape)\n",
    "    \n",
    "    \n",
    "    # burn in railroads where not structure or road\n",
    "    lulc[(railroad_mask) & \n",
    "         (lulc != values_2019_lookup[\"Structure\"]) &\n",
    "         (lulc != values_2019_lookup[\"Asphault\"])] = values_2019_lookup[\"Barren\"]\n",
    "    \n",
    "    #burn in bridges as asphault\n",
    "    lulc[(bridges_mask)] = values_2019_lookup[\"Asphault\"]\n",
    "    #burn in Major Wash polygons as barren where major wash, 6ft above ground and classified as impervious or structure\n",
    "    lulc[(washes_mask) \n",
    "         & (hag < 10) \n",
    "         & ((lulc == values_2019_lookup[\"Impervious\"]) | (lulc == values_2019_lookup[\"Structure\"]))] = values_2019_lookup[\"Barren\"]\n",
    "    \n",
    "    lulc[(lulc!=0)\n",
    "             & (mines_mask==1)\n",
    "             & (lulc != values_2019_lookup[\"PondLake\"])\n",
    "             & (lulc != values_2019_lookup[\"Pool\"])\n",
    "             & (lulc != values_2019_lookup[\"SparseVeg\"])\n",
    "             & (lulc != values_2019_lookup[\"DenseVeg\"])] = values_2019_lookup[\"Barren\"]\n",
    "    \n",
    "    \n",
    "    with rio.open(lulc_file, \"r+\") as dst:\n",
    "        dst.write(lulc, 1)\n",
    "        \n",
    "    print(f\"Finished burn in for #{rowIndex} - {row.path}, {row.row}\")\n",
    "    \n",
    "    return lulc_file\n",
    "\n",
    "\n",
    "relevantTiles = tindex_target[(tindex_target.intersects(railroads))\n",
    "                   | (tindex_target.intersects(washes))\n",
    "                   | (tindex_target.intersects(bridges))\n",
    "                   | (tindex_target.intersects(minesLand))].reset_index(drop=True)\n",
    "\n",
    "cleanedRelevantTiles = Parallel(n_jobs=10)(delayed(burnInValuesOverLULC)(i, row) for i, row in relevantTiles.iterrows())\n",
    "\n",
    "%chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import chime\n",
    "\n",
    "tindex_target = tindex.dropna().reset_index()\n",
    "\n",
    "lulcVRT_loc = \"../EPCExtent_30cm/Cleaned2019LULC.vrt\"\n",
    "outTiff_loc = \"../EPCExtent_30cm/EPC_LULC_2019.tif\"\n",
    "lulcVRT = gdal.BuildVRT(lulcVRT_loc, tt.CleanedLULCFile.values.tolist())\n",
    "del lulcVRT\n",
    "\n",
    "start = datetime.now()\n",
    "lulcVRT_loc = \"../EPCExtent_30cm/Cleaned2019LULC.vrt\"\n",
    "outTiff_loc = \"../EPCExtent_30cm/EPC_LULC_2019.tif\"\n",
    "translateOptions = gdal.TranslateOptions(creationOptions=[\"NUM_THREADS=ALL_CPUS\",\n",
    "                                                          \"TILED=YES\",\n",
    "                                                          \"COMPRESS=LZW\"])\n",
    "lulc2019Tiff = gdal.Translate(outTiff_loc, lulcVRT_loc, options=translateOptions)\n",
    "del lulc2019Tiff\n",
    "end = datetime.now()\n",
    "print(f\"Translate took {end-start}\")\n",
    "start_addo = datetime.now()\n",
    "gdaladdo_cmd  = f\"gdaladdo -ro --config COMPRESS_OVERVIEW LZW {outTiff_loc}\"\n",
    "end_addo = datetime.now()\n",
    "print(f\"Overviews took {end_addo-start_addo}\")\n",
    "chime.theme(\"zelda\")\n",
    "chime.success()\n",
    "Translate took 0:26:12.219672\n",
    "print(os.path.abspath(tindex[(tindex.path==\"W994789\") &(tindex.row==\"W454850\")].OrthoFile.values[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
