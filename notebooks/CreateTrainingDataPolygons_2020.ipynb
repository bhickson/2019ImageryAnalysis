{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chime extension is already loaded. To reload it, use:\n",
      "  %reload_ext chime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BenJames\\anaconda3\\envs\\geospatial\\lib\\site-packages\\geopandas\\geodataframe.py:294: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for f in features_lst:\n"
     ]
    }
   ],
   "source": [
    "%load_ext chime\n",
    "\n",
    "import rasterio as rio\n",
    "from rasterio.windows import from_bounds, transform\n",
    "from rasterio.features import shapes\n",
    "from rasterio.mask import mask\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "from shapely.geometry import Point, box, Polygon\n",
    "import math\n",
    "from rasterio.features import shapes\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib import pyplot\n",
    "from utils import *\n",
    "\n",
    "rasterDataDir = os.path.abspath(r\"R:/ProjectData/PAG2019\")\n",
    "\n",
    "landcover2015 = r\"C:/Users/BenJames/Documents/PAG-Data/DirtRoads/pima_landcover_noroads/pima_landcover_noroads.img\"\n",
    "ortho2019Seg_loc = os.path.join(rasterDataDir, r\"EPCExtent_30cm/Orthos_Segmentedv3/Ortho2019SegmentedV3.vrt\")\n",
    "landcoverExtent_loc = r\"../Vectors/LULC2015_Footprint.gpkg\"\n",
    "#ortho30cmvrt_loc = os.path.join(rasterDataDir, r\"EPCExtent_30cm/Orthos/EPC_30cmOrtho_2019.vrt\")\n",
    "\n",
    "with rio.open(landcover2015) as src:\n",
    "    bnds = src.bounds\n",
    "    landcover2015_extent = box(*bnds)\n",
    "\n",
    "subIndex_loc = \"../Vectors/Ortho_5kSubIndex.gpkg\"\n",
    "targetboxes_loc = \"../Vectors/targetBoxes2019.gpkg\"\n",
    "targetboxes = gpd.read_file(targetboxes_loc)\n",
    "tileIndex = gpd.read_file(subIndex_loc)\n",
    "landcoverTileIndex = gpd.read_file(landcoverExtent_loc)\n",
    "\n",
    "hag_tindex_loc = \"../vectors/HAG2015_tindex.gpkg\"\n",
    "hag_tindex = gpd.read_file(hag_tindex_loc)\n",
    "targetboxes = targetboxes[targetboxes.within(hag_tindex.unary_union)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"PondsLakes\", \"Pool\", \"Dense Vegetation\", \"Sparse Vegetation\", \"Barren\", \"Irrigated Lands/Turf\",  \"Asphault\", \"Other Impervious\", \"Structure\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Water - Deep and/or turbid water. Ponds & Lakes\n",
    "- Pools - Clear and shallow water with concrete substrate. Backyard pools and public pools.\n",
    "- Dense Vegetation - High vegetation index value - Trees, bushes\n",
    "- Sparse Vegetation - Low vegetation index values - shrubs, cacti\n",
    "- Irrigated Lands/Turf - ground-level, high vi value - Grasses, fields, agriculture\n",
    "- Barren - ground-level, dirt or rock\n",
    "- Asphault - generally newer asphault associated with roads\n",
    "- Other Impervious - concrete, dirty asphault, older asphault\n",
    "- Structures - impervious surfaces elevated off the ground - generally fixed built structures. Could be cars/trailers/RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delOSMColumns(df, exception):\n",
    "    mandatory = ['osm_id', 'osm_way_id', 'name', 'type', \"other_tags\", \"geometry\"]\n",
    "    mandatory.append(exception)\n",
    "    for col in df.columns.tolist():\n",
    "        if col not in mandatory:\n",
    "            del df[col]\n",
    "    return df\n",
    "\n",
    "\n",
    "def pullTagValues(other_tags, target_tag):\n",
    "    return None if other_tags is None or target_tag not in other_tags else \",\".join([tag.split(\"=>\")[1].replace('\"','') for tag in other_tags.split(\",\") if target_tag in tag])\n",
    "\n",
    "\n",
    "def getFeaturesInBounds(landcover_ras, bnds, landcoverValue=None, msaviUpperLimit=None, msaviLowerLimit=None):\n",
    "    polys = []\n",
    "    try:\n",
    "        with rio.open(landcover_ras) as src:\n",
    "            twin = from_bounds(bnds[0], bnds[1], bnds[2], bnds[3], transform=src.transform)\n",
    "            wtrans = transform(twin, src.transform)\n",
    "            nd = src.nodata\n",
    "            window_array = src.read(1, window=twin)\n",
    "\n",
    "        allMask = np.ones(window_array.shape).astype(bool)\n",
    "\n",
    "        if msaviUpperLimit or msaviLowerLimit:\n",
    "            with rio.open(ortho2019Seg_loc) as src:\n",
    "                twin = from_bounds(*bnds, transform=src.transform)\n",
    "                msavi_array = src.read(5, window=twin, out_shape=window_array.shape)\n",
    "                if (msaviUpperLimit <= 255) & (msavi_array.dtype == np.uint16):\n",
    "                    #print(\"converting limits to uint16\")\n",
    "                    msaviUpperLimit = (msaviUpperLimit/np.iinfo(np.uint8).max)*np.iinfo(np.uint16).max\n",
    "                    msaviLowerLimit = (msaviLowerLimit/np.iinfo(np.uint8).max)*np.iinfo(np.uint16).max\n",
    "                vegMask = (msavi_array <= msaviUpperLimit) & (msavi_array > msaviLowerLimit)\n",
    "        else:\n",
    "            vegMask = None\n",
    "\n",
    "        if landcoverValue:\n",
    "            landcoverMask = window_array == landcoverValue\n",
    "        else:\n",
    "            landcoverMask == None\n",
    "\n",
    "        if landcoverMask is not None:\n",
    "            allMask = landcoverMask & allMask\n",
    "        if vegMask is not None:\n",
    "            allMask = vegMask & allMask\n",
    "\n",
    "        targetFeatures = shapes(window_array, allMask, transform=wtrans)\n",
    "        targetFeatures = [feat for feat in targetFeatures]\n",
    "\n",
    "        if len(targetFeatures) != 0:\n",
    "            for feat in targetFeatures:\n",
    "                polys.append(Polygon(feat[0][\"coordinates\"][0]))\n",
    "    except Exception as e:\n",
    "        print(f\"Error on bounds {bnds}. Returning empty list\\n{e}\")\n",
    "        \n",
    "    return polys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in ../vectors/osmPolygons_regional.gpkg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BenJames\\anaconda3\\envs\\geospatial\\lib\\site-packages\\geopandas\\geodataframe.py:294: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for f in features_lst:\n"
     ]
    }
   ],
   "source": [
    "osmPolygons_regional_loc = \"../vectors/osmPolygons_regional.gpkg\"\n",
    "if not os.path.exists(osmPolygons_regional_loc):\n",
    "    print(f\"Creating {osmPolygons_regional_loc}...\")\n",
    "    osmPoly_loc = r\"../Vectors/arizona-latest.osm.20201215.gpkg\"\n",
    "    osm_polygons = gpd.read_file(osmPoly_loc, layer=\"multipolygons\").to_crs(\"epsg:2868\")\n",
    "    osm_polygons.geometry = osm_polygons.geometry.buffer(0)\n",
    "    osm_polygons = osm_polygons[osm_polygons.intersects(landcoverTileIndex.unary_union)]\n",
    "    osm_polygons.to_file(osmPolygons_regional_loc)\n",
    "else:\n",
    "    print(f\"Reading in {osmPolygons_regional_loc}...\")\n",
    "    osm_polygons = gpd.read_file(osmPolygons_regional_loc)\n",
    "    \n",
    "%chime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------\n",
    "# PondsLakes & Pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in ../vectors/osmwater_2020.gpkg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BenJames\\anaconda3\\envs\\geospatial\\lib\\site-packages\\geopandas\\geodataframe.py:294: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for f in features_lst:\n"
     ]
    }
   ],
   "source": [
    "from rasterio.plot import show\n",
    "# Get Water# natural, man-made, leisure (pool), landuse (basin, reservoir)\n",
    "osm_water = \"../vectors/osmwater_2020.gpkg\"\n",
    "\n",
    "if not os.path.exists(osm_water):\n",
    "    print(f\"Creating {osm_water}\")\n",
    "    osm_polyWater = osm_polygons[osm_polygons.natural == 'water'].copy().reset_index()\n",
    "    osm_polyWater = delOSMColumns(osm_polyWater, \"natural\")\n",
    "    osm_polyWater[\"water\"] = osm_polyWater.other_tags.apply(lambda ot: pullTagValues(ot, \"water\"))\n",
    "    display(osm_polyWater.head())\n",
    "\n",
    "    osm_polyWater[\"intermittent\"] = osm_polyWater.other_tags.apply(lambda ot: pullTagValues(ot, \"intermittent\"))\n",
    "    osm_polyWater = osm_polyWater[(~pd.isnull(osm_polyWater.water)) & (osm_polyWater.intermittent != \"yes\")\n",
    "                                 & (osm_polyWater.water.str.lower() != \"river\") & (osm_polyWater.water.str.lower() != \"wash\")]\n",
    "    \n",
    "    osm_polyWater.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if \"review\" not in osm_polyWater.columns.tolist():\n",
    "        osm_polyWater[\"review\"] = None\n",
    "\n",
    "    for i, row in osm_polyWater.copy().iterrows():\n",
    "        if row.review is not None:\n",
    "            continue\n",
    "        buffered = row.geometry.centroid.buffer(100)\n",
    "        bndbox = buffered.bounds\n",
    "        with rio.open(ortho30cmvrt_loc) as src:\n",
    "            winb = from_bounds(bndbox[0], bndbox[1], bndbox[2], bndbox[3], transform=src.transform)\n",
    "            raster = src.read(window=winb)\n",
    "        if 0 not in raster.shape:    \n",
    "            show(raster[:3])\n",
    "            result = input(f\"Enter eval for {i} of {len(osm_polyWater)} (if water enter 0, to review enter 1, if not water enter 666)\\n\")\n",
    "        else:\n",
    "            result = 666\n",
    "\n",
    "        osm_polyWater.at[i, 'review'] = result\n",
    "\n",
    "    osm_polyWater.to_file(osm_water, driver=\"GPKG\")\n",
    "    print(f\"Wrote out to {osm_water}\")\n",
    "else:\n",
    "    print(f\"Reading in {osm_water}\")\n",
    "    osm_polyWater = gpd.read_file(osm_water)\n",
    "    osm_polyWater = osm_polyWater[osm_polyWater.review!='666']\n",
    "        \n",
    "%chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ../vectors/poolsPolys.gpkg\n",
      "Reading in ../vectors/water_2015.gpkg\n"
     ]
    }
   ],
   "source": [
    "water_2015_loc = r\"../vectors/water_2015.gpkg\"\n",
    "pondsLakes_2015_loc = r\"../vectors/pondsLakes_2015.gpkg\"\n",
    "poolsPoly_loc  = r\"../vectors/poolsPolys.gpkg\"\n",
    "\n",
    "water_val_2015 = 1\n",
    "if not os.path.exists(poolsPoly_loc):\n",
    "    osmPools_2020 = osm_polyWater[(osm_polyWater.review == '0') & (osm_polyWater.water == \"pool\")]\n",
    "    print(f\"Creating {poolsPoly_loc}\")\n",
    "    if not os.path.exists(water_2015_loc):\n",
    "        print(f\"Creating {water_2015_loc}\")\n",
    "        t1 = datetime.now()\n",
    "        water_polys = Parallel(n_jobs=10, verbose=10)(delayed(getFeaturesInBounds)(landcover, row.geometry.bounds, landcoverValue=water_val_2015) for i, row in tileIndex.iterrows())\n",
    "        %chime\n",
    "        print(datetime.now()-t1)\n",
    "        allPolys = list(itertools.chain.from_iterable(water_polys))\n",
    "        water_2015 = gpd.GeoDataFrame(geometry=allPolys, crs=\"epsg:2868\")\n",
    "        water_2015[\"Area\"] = water_2015.geometry.area\n",
    "        water_2015.to_file(water_2015_loc, driver=\"GPKG\")\n",
    "    else:\n",
    "        print(f\"Reading in {water_2015_loc}\")\n",
    "        water_2015 = gpd.read_file(water_2015_loc)\n",
    "\n",
    "    \n",
    "    non_osmPondsLakes = water_2015[~water_2015.intersects(osm_polyWater.unary_union)].copy()\n",
    "    # merge adjacent geometries\n",
    "    non_osmPondsLakes.geometry = non_osmPondsLakes.geometry.buffer(3)#.explode().buffer(-3)\n",
    "    non_osmPondsLakes[\"Type\"] = \"water\"\n",
    "    non_osmPondsLakes = non_osmPondsLakes.dissolve(by=\"Type\")\n",
    "    non_osmPondsLakes = gpd.GeoDataFrame(geometry = [g for g in non_osmPondsLakes.geometry.values[0]], crs = non_osmPondsLakes.crs)\n",
    "    non_osmPondsLakes.geometry = non_osmPondsLakes.geometry.buffer(-3)\n",
    "    non_osmPondsLakes[\"Area\"] = non_osmPondsLakes.geometry.area\n",
    "\n",
    "    pools_2015 = non_osmPondsLakes[(non_osmPondsLakes.Area < 500) & (non_osmPondsLakes.Area > 200)].copy()\n",
    "\n",
    "    pools = pd.concat([osmPools_2020, pools_2015])\n",
    "    \n",
    "    pools.to_file(poolsPoly_loc, driver=\"GPKG\")\n",
    "else:\n",
    "    print(f\"Reading in {poolsPoly_loc}\")\n",
    "    pools = gpd.read_file(poolsPoly_loc)\n",
    "\n",
    "%chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ../vectors/pondsLakesPolys.gpkg...\n",
      "Reading in ../vectors/pondsLakes_2015.gpkg\n"
     ]
    }
   ],
   "source": [
    "pondsLakesPoly_loc = r\"../vectors/pondsLakesPolys.gpkg\"\n",
    "\n",
    "if not os.path.exists(pondsLakesPoly_loc):\n",
    "    print(f\"Creating {pondsLakesPoly_loc}...\")\n",
    "    \n",
    "    if not os.path.exists(pondsLakes_2015_loc):\n",
    "        pondsLakes_2015 = non_osmPondsLakes[non_osmPondsLakes.area>3000]\n",
    "        pondsLakes_2015.reset_index(drop=True, inplace=True)\n",
    "        if \"review\" not in pondsLakes_2015.columns.tolist():\n",
    "            print(\"HERE\")\n",
    "            pondsLakes_2015[\"review\"] = None\n",
    "\n",
    "        for i, row in pondsLakes_2015.copy().iterrows():\n",
    "            if row.review is not None:\n",
    "                continue\n",
    "            buffered = row.geometry.centroid.buffer(100)\n",
    "            bndbox = buffered.bounds\n",
    "            with rio.open(ortho30cmvrt_loc) as src:\n",
    "                winb = from_bounds(bndbox[0], bndbox[1], bndbox[2], bndbox[3], transform=src.transform)\n",
    "                raster = src.read(window=winb)\n",
    "            if 0 not in raster.shape:    \n",
    "                plt = show(raster[:3])\n",
    "                result = input(f\"Enter eval for {i} of {len(osm_polyWater)} (if water enter 0, to review enter 1, if not water enter 666)\\n\")\n",
    "            else:\n",
    "                result = 666\n",
    "            pondsLakes_2015.at[i, 'review'] = result\n",
    "\n",
    "        pondsLakes_2015 = pondsLakes_2015[pondsLakes_2015.review=='0']\n",
    "        pondsLakes_2015.to_file(pondsLakes_2015_loc, driver=\"GPKG\")\n",
    "    else:\n",
    "        print(f\"Reading in {pondsLakes_2015_loc}\")\n",
    "        pondsLakes_2015 = gpd.read_file(pondsLakes_2015_loc)\n",
    "\n",
    "    osmPondsLakes_2020 = osm_polyWater[(osm_polyWater.review == '0') & (osm_polyWater.water != \"pool\")]\n",
    "\n",
    "    pondsLakes = pd.concat([osmPondsLakes_2020, pondsLakes_2015])\n",
    "    del pondsLakes[\"Area\"]\n",
    "    pondsLakes[\"Area\"] = pondsLakes.geometry.area\n",
    "    pondsLakes.to_file(pondsLakesPoly_loc, driver=\"GPKG\")\n",
    "else:\n",
    "    print(f\"Reading in {pondsLakesPoly_loc}...\")\n",
    "    pondsLakes = gpd.read_file(pondsLakesPoly_loc)\n",
    "    \n",
    "%chime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ../vectors/osmBuildings_2020.gpkg\n",
      "['office' 'university' 'yes' 'school' 'hospital' 'industrial' 'church'\n",
      " 'apartments' 'commercial' 'roof' 'hotel' 'stadium' 'house' 'dormitory'\n",
      " 'residential' 'shed' 'train_station' 'public' 'retail' 'carport'\n",
      " 'college' 'kindergarten' 'parking' 'bridge' 'storage_tank'\n",
      " 'central_office' 'terrace' 'garages' 'garage' 'ruins' 'Commercial'\n",
      " 'detached' 'manufacture' 'barn' 'grandstand' 'stable' 'warehouse'\n",
      " 'greenhouse' 'collapsed' 'ses' 'hangar' 'bunker' 'water_tower'\n",
      " 'government' 'static_caravan' 'service' 'construction'\n",
      " 'semidetached_house' 'pavilion']\n",
      "(141425, 9)\n",
      "Creating ../vectors/structures_2015.gpkg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  52 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=10)]: Done 125 out of 125 | elapsed:    4.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:16.219002\n"
     ]
    }
   ],
   "source": [
    "structuresPoly_loc = r\"../vectors/StructuresPoly.gpkg\"\n",
    "\n",
    "structures_2015_loc = r\"../vectors/structures_2015.gpkg\"\n",
    "osmBuildings_loc = \"../vectors/osmBuildings_2020.gpkg\"\n",
    "\n",
    "if not os.path.exists(structuresPoly_loc):\n",
    "    if not os.path.exists(osmBuildings_loc):\n",
    "        print(f\"Creating {osmBuildings_loc}\")\n",
    "        osm_polyBuildings = osm_polygons[~(pd.isnull(osm_polygons.building))].copy().reset_index()\n",
    "        osm_polyBuildings = delOSMColumns(osm_polyBuildings, \"building\")\n",
    "        osm_polyBuildings[\"buildingTag\"] = osm_polyBuildings.other_tags.apply(lambda ot: pullTagValues(ot, \"building\"))\n",
    "        osm_polyBuildings[\"area\"] = osm_polyBuildings.geometry.area\n",
    "        osm_polyBuildings.to_file(osmBuildings_loc, driver=\"GPKG\")\n",
    "    else:\n",
    "        print(f\"Reading in {osmBuildings_loc}\")\n",
    "        osm_polyBuildings = gpd.read_file(osmBuildings_loc)\n",
    "\n",
    "    print(osm_polyBuildings.building.unique())\n",
    "    # remove parking (elevated outdoor may unnecessarily confuse model), ruins, bunkers, collapsed, construction, stable, bridge\n",
    "    bad_buildings = [\"parking\", \"ruins\", \"bunker\", \"collapsed\", \"construction\", \"stable\", \"bridge\"]\n",
    "    osm_polyBuildings = osm_polyBuildings[~osm_polyBuildings[\"building\"].isin(bad_buildings)]\n",
    "    print(osm_polyBuildings.shape)\n",
    "\n",
    "    structureVal = 7\n",
    "\n",
    "    if not os.path.exists(structures_2015_loc):\n",
    "        print(f\"Creating {structures_2015_loc}\")\n",
    "        t1 = datetime.now()\n",
    "\n",
    "        structures_polys = Parallel(n_jobs=10, verbose=5)(delayed(getFeaturesInBounds)(landcover, row.geometry.bounds, landcoverValue=structureVal, msaviUpperLimit=None, msaviLowerLimit=None) for i, row in targetboxes.iterrows())\n",
    "\n",
    "        allPolys = list(itertools.chain.from_iterable(structures_polys))\n",
    "        structures_2015 = gpd.GeoDataFrame(geometry=allPolys, crs=\"epsg:2868\")\n",
    "        structures_2015[\"area\"] = structures_2015.geometry.area\n",
    "        structures_2015.to_file(structures_2015_loc, driver=\"GPKG\")\n",
    "        print(datetime.now()-t1)\n",
    "    else:\n",
    "        print(f\"Reading in {structures_2015_loc}\")\n",
    "        structures_2015 = gpd.read_file(structures_2015_loc)\n",
    "\n",
    "    # osm buildings are offset (different imagery), so filter out anything below 1500 square feet\n",
    "    osm_polyBuildings = osm_polyBuildings[osm_polyBuildings.area>1500]\n",
    "    # filtering spatial join much faster than usual intersect\n",
    "    osmBuildings_non2015 = gpd.sjoin(osm_polyBuildings, structures_2015, op=\"intersects\", how=\"left\")\n",
    "    osmBuildings_non2015 = osmBuildings_non2015[pd.isnull(osmBuildings_non2015.index_right)]\n",
    "\n",
    "    structuresPoly = pd.concat([structures_2015, osmBuildings_non2015])\n",
    "    structuresPoly[\"area\"] = structuresPoly.geometry.area\n",
    "    structuresPoly.to_file(structuresPoly_loc, driver=\"GPKG\")\n",
    "else:\n",
    "    print(f\"Reading in {structuresPoly_loc}...\")\n",
    "    structuresPoly = gpd.read_file(structuresPoly_loc)\n",
    "\n",
    "%chime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Veg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ../vectors/TrainingData/denseVeg_2015.gpkg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  52 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=10)]: Done  99 out of  99 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:06:38.155543\n"
     ]
    }
   ],
   "source": [
    "denseVeg_2015_loc = r\"../vectors/TrainingData/denseVeg_2015.gpkg\"\n",
    "treesShrubs_val = 2\n",
    "\n",
    "if not os.path.exists(denseVeg_2015_loc):\n",
    "    print(f\"Creating {denseVeg_2015_loc}\")\n",
    "    t1 = datetime.now()\n",
    "\n",
    "    denseVeg_polys = Parallel(n_jobs=10, verbose=5)(delayed(getFeaturesInBounds)(landcover2015, row.geometry.bounds, landcoverValue=treesShrubs_val, msaviUpperLimit=255, msaviLowerLimit=135) for i, row in targetboxes.iterrows())\n",
    "\n",
    "    allPolys = list(itertools.chain.from_iterable(denseVeg_polys))\n",
    "    denseVeg_2015 = gpd.GeoDataFrame(geometry=allPolys, crs=\"epsg:2868\")\n",
    "    denseVeg_2015[\"area\"] = denseVeg_2015.geometry.area\n",
    "    denseVeg_2015.to_file(denseVeg_2015_loc, driver=\"GPKG\")\n",
    "    print(datetime.now()-t1)\n",
    "else:\n",
    "    print(f\"Reading in {denseVeg_2015_loc}\")\n",
    "    denseVeg_2015 = gpd.read_file(denseVeg_2015_loc)\n",
    "\n",
    "%chime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARSE VEG AND BARREN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BenJames\\anaconda3\\envs\\geospatial\\lib\\site-packages\\geopandas\\geodataframe.py:294: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for f in features_lst:\n"
     ]
    }
   ],
   "source": [
    "ruralRegions_loc = r\"../vectors/DesertBarrenVegetation_2015.gpkg\"\n",
    "rural_regions = gpd.read_file(ruralRegions_loc)\n",
    "rural_regions = rural_regions[rural_regions.within(hag_tindex.unary_union)]\n",
    "\n",
    "tileIndexBuff = tileIndex.copy()\n",
    "tileIndexBuff[\"geometry\"] = tileIndexBuff[\"geometry\"].apply(lambda g: box(*g.buffer(-50).bounds))\n",
    "\n",
    "rural_regions = gpd.overlay(rural_regions, tileIndexBuff, how=\"union\")\n",
    "rural_regions = rural_regions[~pd.isnull(rural_regions[\"Area\"])]\n",
    "\n",
    "rural_regions.to_file(ruralRegions_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import box\n",
    "from rasterio import features\n",
    "from rasterio.windows import transform as wtransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "barrenSparse_2015Urban_loc = r\"../vectors/BarrenPolys_MidTown2015.gpkg\"\n",
    "def barrenFrom2015(polygon):\n",
    "    with rio.open(\"C:/Users/BenJames/Documents/PAG-Data/DirtRoads/pima_landcover_noroads/pima_landcover_noroads.img\") as src:\n",
    "        win = from_bounds(*polygon.bounds, src.transform)\n",
    "        win_trans = wtransform(win, src.transform)\n",
    "        a = src.read(1, window=win)\n",
    "        mask = a == 5\n",
    "        shapes = [Polygon(poly[0][\"coordinates\"][0]) for poly in features.shapes(a, mask=mask, transform=win_trans)]\n",
    "        shapes_gdf = gpd.GeoDataFrame({\"geometry\":shapes}, geometry=\"geometry\", crs=src.crs)\n",
    "        \n",
    "    return shapes_gdf\n",
    "        \n",
    "        \n",
    "inPoly = box(984278,436946,1009295,456063)\n",
    "barrenPolys = barrenFrom2015(inPoly)\n",
    "barrenPolys.to_file(barrenSparse_2015Urban_loc, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePoints(df, totalPointsDF, minPerFeature):\n",
    "    \n",
    "    df[\"Area\"] = df.geometry.area\n",
    "    remainderPoints = totalPointsDF - (minPerFeature * len(df))\n",
    "    total_area = df.Area.sum()\n",
    "    #df[\"POT\"] = df[\"Area\"].apply(lambda a: a/total_area)\n",
    "    #df[\"NumPoints\"] = df[\"POT\"].apply(lambda pot: int(minPerFeature+(pot*remainderPoints)))\n",
    "\n",
    "    allPoints = []\n",
    "    for i, row in df.iterrows():\n",
    "        bnds = row.geometry.bounds\n",
    "        featurePoints = []\n",
    "        while (len(featurePoints) < row.NumPoints) :\n",
    "            #print(f\"WHILE {len(featurePoints), row.NumPoints}\")\n",
    "            x = random.uniform(bnds[0], bnds[2])\n",
    "            y = random.uniform(bnds[1], bnds[3])\n",
    "            point = Point(x,y)\n",
    "            if point.intersects(row.geometry):\n",
    "                featurePoints.append(point)\n",
    "        allPoints += featurePoints\n",
    "    return allPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "3500000\n",
      "4000000\n",
      "4500000\n",
      "5000000\n",
      "5500000\n",
      "6000000\n",
      "6500000\n",
      "7000000\n",
      "7500000\n",
      "8000000\n",
      "8500000\n",
      "9000000\n",
      "9500000\n",
      "3:36:29.633577\n",
      "6747567 2682605 569726\n"
     ]
    }
   ],
   "source": [
    "barrenSparseV_loc = \"../OtherData/TrainingData/barrenSparseVPoints.gpkg\"\n",
    "\n",
    "if not os.path.exists(barrenSparseV_loc):\n",
    "    # masking raster with polygon and turning into shapes takes way to long. Since we know these area contain only one of three values denseV, sparseV, or barren, justsparseVegPoints_loc and filter those\n",
    "    print(f\"Creating {barrenSparseV_loc}...\")\n",
    "\n",
    "    totalPoints = 10 * 1000000\n",
    "    minPoints = 5000\n",
    "    barrenSparsePoints = generatePoints(rural_regions, totalPoints, minPoints)\n",
    "    print(f\"Created {len(barrenSparsePoints)} random points\")\n",
    "    \n",
    "    subSize = 10000\n",
    "    t1 = datetime.now()\n",
    "    allValues=[]\n",
    "    with rio.open(ortho2019Seg_loc) as src:\n",
    "        for i in range(0, len(barrenSparsePoints), subSize):\n",
    "            if i % 500000 == 0 and i != 0:\n",
    "                print(i)\n",
    "            pointsSubset = barrenSparsePoints[i:i+subSize]\n",
    "            xys = [(point.x, point.y) for point in pointsSubset]\n",
    "            values = [value[0] for value in src.sample(xys, indexes=5)]\n",
    "            allValues += values\n",
    "\n",
    "    t2 = datetime.now()\n",
    "    print(t2-t1)\n",
    "\n",
    "    barrenSparseV = gpd.GeoDataFrame({\"MSAVI\":allValues}, geometry=barrenSparsePoints, crs=\"epsg:2868\")\n",
    "    barrenSparseV.to_file(barrenSparseV_loc, driver=\"GPKG\")\n",
    "    \n",
    "\n",
    "else:\n",
    "    print(f\"Reading in {barrenSparseV_loc}...\")\n",
    "    barrenSparseV = gpd.read_file(barrenSparseV_loc)\n",
    "\n",
    "%chime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________\n",
    "# Irrigated Land\n",
    "\n",
    "2015 irrigated classification with MSAVI values greater than the 135 cutoff (dense veg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ../vectors/irrigatedLand_2015.gpkg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  52 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=10)]: Done  99 out of  99 | elapsed:  4.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:04:33.527592\n",
      "22109376.0\n"
     ]
    }
   ],
   "source": [
    "irrigatedLand_2015_loc = r\"../vectors/irrigatedLand_2015.gpkg\"\n",
    "irrigatedVal = 3\n",
    "\n",
    "if not os.path.exists(irrigatedLand_2015_loc):\n",
    "    print(f\"Creating {irrigatedLand_2015_loc}\")\n",
    "    targetboxes[\"geometry\"] = targetboxes[\"geometry\"].apply(lambda g: box(*g.buffer(-50).bounds))\n",
    "    t1 = datetime.now()\n",
    "\n",
    "    irrigated_polys = Parallel(n_jobs=10, verbose=5)(delayed(getFeaturesInBounds)(landcover2015, row.geometry.bounds, irrigatedVal, msaviUpperLimit=255, msaviLowerLimit=135) for i, row in targetboxes.iterrows())\n",
    "\n",
    "    allPolys = list(itertools.chain.from_iterable(irrigated_polys))\n",
    "    irrigated_2015 = gpd.GeoDataFrame(geometry=allPolys, crs=\"epsg:2868\")\n",
    "    irrigated_2015[\"Area\"] = irrigated_2015.geometry.area\n",
    "    \n",
    "    irrigated_2015[\"Area\"] = irrigated_2015.geometry.area\n",
    "    irrigated_2015.to_file(irrigatedLand_2015_loc, driver=\"GPKG\")\n",
    "    print(datetime.now()-t1)\n",
    "else:\n",
    "    print(f\"Reading in {irrigatedLand_2015_loc}\")\n",
    "    irrigated_2015 = gpd.read_file(irrigatedLand_2015_loc)\n",
    "\n",
    "print(irrigated_2015[\"Area\"].sum())\n",
    "%chime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________\n",
    "# Asphault\n",
    "\n",
    "Drop points along osm road lines to ensure they fall on asphault\n",
    "Buffer osm roads to create polygons, drop points within polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in ../vectors/asphaultPoly.gpkg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BenJames\\anaconda3\\envs\\geospatial\\lib\\site-packages\\geopandas\\geodataframe.py:294: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for f in features_lst:\n"
     ]
    }
   ],
   "source": [
    "censusPlaces_loc = r\"Q:\\Projects\\2019ImageryAnalysis\\vectors\\tl_2021_04_place.shp\"\n",
    "osm_roads_loc = \"../vectors/osm_arizona_20210406_lines_pima_roads.gpkg\"\n",
    "asphaultPoly_loc = \"../vectors/asphaultPoly.gpkg\"\n",
    "target_place = gpd.read_file(censusPlaces_loc).to_crs(tileIndex.crs)\n",
    "target_place = target_place[target_place.NAME.isin([\"Tucson\", \"Oro Valley\", \"Casas Adobes\", \"Drexel Heights\", \"Vail\", \"Green Valley\", \"Sahuarita\"])]\n",
    "urbanboxes = tileIndex[(tileIndex.intersects(target_place.unary_union)) & (tileIndex.within(landcoverTileIndex.unary_union))].sample(n=75)\n",
    "#urbanboxes = gpd.read_file(\"../vectors/UrbanBoxes.gpkg\")\n",
    "\n",
    "buffer_size = 5\n",
    "\n",
    "if not os.path.exists(asphaultPoly_loc):\n",
    "    print(f\"Creating {asphaultPoly_loc}...\")\n",
    "    osm_roads_all = gpd.read_file(osm_roads_loc)\n",
    "    roadboxes = pd.concat([targetboxes, urbanboxes])\n",
    "    roadboxes[\"geometry\"] = roadboxes[\"geometry\"].apply(lambda g: box(*g.buffer(-50).bounds))\n",
    "    bad_roads = [\"abandonded\", \"bridleway\", \"construction\", \"elevator\", \"footway\", \"living_street\", \"path\", \"pedestrian\", \"proposed\", \"steps\",\"unclassified\"]\n",
    "    osm_roads = osm_roads_all[\n",
    "        (osm_roads_all.other_tags.str.contains(\"asphault\")) \n",
    "        | (osm_roads_all.other_tags.str.contains(\"parking_aisle\"))\n",
    "        | (\n",
    "            (~osm_roads_all.highway.isin(bad_roads))\n",
    "        )\n",
    "        | (\n",
    "            (osm_roads_all.highway == \"residential\")\n",
    "            & (~pd.isnull(osm_roads_all.name)) \n",
    "            & (osm_roads_all.intersects(urbanboxes.to_crs(osm_roads_all.crs).unary_union))\n",
    "        )\n",
    "    ].copy()\n",
    "    #osm_roads.to_file(\"../pavedRoads_20211228.gpkg\", driver=\"GPKG\")\n",
    "    osm_roads.to_crs(roadboxes.crs, inplace=True)\n",
    "    osm_roads[\"geometry\"] = osm_roads.buffer(buffer_size)\n",
    "    roads_targets = gpd.clip(osm_roads, roadboxes)\n",
    "    roads_targets = gpd.overlay(roads_targets, roadboxes[roadboxes.intersects(roads_targets.unary_union)], how=\"union\")\n",
    "    roads_targets = roads_targets[~pd.isnull(roads_targets[\"osm_id\"])]\n",
    "    roads_targets = roads_targets.dissolve(by=[\"name\", \"path\", \"row\"]).reset_index().explode().reset_index(drop=True)\n",
    "    roads_targets[\"Area\"] = roads_targets.geometry.area\n",
    "    roads_targets.to_file(asphaultPoly_loc, driver=\"GPKG\")\n",
    "else:\n",
    "    print(f\"Reading in {asphaultPoly_loc}...\")\n",
    "    road_targets = gpd.read_file(asphaultPoly_loc)\n",
    "\n",
    "%chime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "num_asphault = 2.5 * 1000000\n",
    "minPoints = 1\n",
    "remainderPoints = num_asphault - (minPoints * len(roads_targets))\n",
    "total_area = roads_targets.Area.sum()\n",
    "roads_targets[\"POT\"] = roads_targets[\"Area\"].apply(lambda a: a/total_area)\n",
    "roads_targets[\"NumPoints\"] = roads_targets[\"POT\"].apply(lambda pot: int(minPoints+(pot*remainderPoints)))\n",
    "\n",
    "allPoints = []\n",
    "for i, row in roads_targets.iterrows():\n",
    "    if i%1000 == 0 and i != 0:\n",
    "        print(f\"index:{i} of {len(roads_targets)}\")\n",
    "    bnds = row.geometry.bounds\n",
    "    featurePoints = []\n",
    "    while len(featurePoints) < row.NumPoints:\n",
    "        x = random.uniform(bnds[0], bnds[2])\n",
    "        y = random.uniform(bnds[1], bnds[3])\n",
    "        point = Point(x,y)\n",
    "        if point.intersects(row.geometry):\n",
    "            featurePoints.append(point)\n",
    "    allPoints += featurePoints\n",
    "print(len(allPoints))\n",
    "\n",
    "asphaultPoints = gpd.GeoDataFrame(geometry=allPoints, crs=roads_targets.crs)\n",
    "asphaultPoints.to_file(\"../OtherData/TrainingData/asphaultPoints.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "%chime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________\n",
    "# Impervious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BenJames\\anaconda3\\envs\\geospatial\\lib\\site-packages\\geopandas\\geodataframe.py:294: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for f in features_lst:\n",
      "C:\\Users\\BenJames\\anaconda3\\envs\\geospatial\\lib\\site-packages\\geopandas\\geodataframe.py:294: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for f in features_lst:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "amexSidewalks_loc = \"../vectors/Amex_ADAAssetsRawCalcs_COT.gpkg\"\n",
    "cotSidewalks = gpd.read_file(amexSidewalks_loc, layer=\"Amex_ADAAssetsRawCalcs_COT_Sidewalks\")\n",
    "cotSidewalks = cotSidewalks[cotSidewalks[\"Material\"].str.lower() == \"concrete\"]\n",
    "cotSidewalks = gpd.overlay(cotSidewalks[cotSidewalks.intersects(targetboxes.unary_union)], tileIndexBuff, how=\"union\")\n",
    "cotSidewalks = cotSidewalks[cotSidewalks.Material.notnull()]\n",
    "\n",
    "impervious_loc = \"../vectors/Impervious.gpkg\"\n",
    "imperviousPoly_loc = \"../vectors/ImperviousPoly.gpkg\"\n",
    "imperviousPoly = gpd.read_file(impervious_loc, layer=\"ImperviousPoly\")\n",
    "imperviousPoly[\"Area\"] = imperviousPoly.geometry.area\n",
    "imperviousPoly[\"Area\"].sum()/4\n",
    "\n",
    "tileIndex = gpd.read_file(subIndex_loc)\n",
    "tileIndex[\"geometry\"] = tileIndex[\"geometry\"].apply(lambda g: box(*g.buffer(-50).bounds))\n",
    "imperviousPoly = gpd.overlay(imperviousPoly, tileIndex, how=\"union\")\n",
    "imperviousPoly = imperviousPoly[imperviousPoly.Area.notnull()]\n",
    "\n",
    "impervious = pd.concat([imperviousPoly, cotSidewalks])\n",
    "impervious = impervious[tileIndex.columns].dissolve(by=[\"path\", \"row\"], as_index=False)\n",
    "\n",
    "impervious.to_file(imperviousPoly_loc, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSnappedPointsLocation(geometry, rasterBnds, rasterRes):\n",
    "    \"\"\" Returns set of upper-right snapped pixel locations in set as (x, y)\"\"\"\n",
    "\n",
    "    geom_left, geom_bottom, geom_right, geom_top = geometry.bounds\n",
    "    \n",
    "    pix_diff_x_left = (geom_left - rasterBnds.left) / rasterRes % 1\n",
    "    pix_diff_y_bottom = (geom_bottom - rasterBnds.bottom) / rasterRes % 1\n",
    "    pix_diff_x_right = (geom_right - rasterBnds.right) / rasterRes % 1\n",
    "    pix_diff_y_top = (geom_top - rasterBnds.top) / rasterRes % 1\n",
    "    \n",
    "    geom_left -= pix_diff_x_left\n",
    "    geom_right += 1-pix_diff_x_left\n",
    "    geom_bottom -= pix_diff_y_bottom\n",
    "    geom_top += 1-pix_diff_y_top\n",
    "    \n",
    "    sizex = ceil((geom_right - geom_left)/rasterRes)\n",
    "    sizey = ceil((geom_top - geom_bottom)/rasterRes)\n",
    "    \n",
    "    points = []\n",
    "    for x in range(0, sizex):\n",
    "        xp = geom_left + (x*rasterRes) + (rasterRes/2)\n",
    "        for y in range(0, sizey):\n",
    "            yp = geom_bottom + (y*rasterRes) + (rasterRes/2)\n",
    "            points.append(Point(xp,yp))\n",
    "    \n",
    "    return [point for point in points if point.intersects(geometry)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rio.open(ortho30cmvrt_loc) as src:\n",
    "    rasbnds = src.bounds\n",
    "    reso = src.res[0]\n",
    "    \n",
    "impervious_points = [getSnappedPointsLocation(geom, rasbnds, reso) for geom in impervious.geometry]\n",
    "impervious_points = list(itertools.chain.from_iterable(impervious_points))\n",
    "\n",
    "impervious_pnts = gpd.GeoDataFrame(geometry=impervious_points, crs = \"epsg:2868\")\n",
    "impervious_pnts.to_file(\"../OtherData/TrainingData/ImperviousPoints.gpkg\", driver=\"GPKG\")\n",
    "%chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31275316.208916515"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subSize = 10000\n",
    "t1 = datetime.now()\n",
    "allValues=[]\n",
    "\n",
    "with rio.open(ortho2019Seg_loc) as src:\n",
    "    for i in range(0, len(allPoints), subSize):\n",
    "        if i%1000000 == 0:\n",
    "            print(i)\n",
    "        pointsSubset = allPoints[i:i+subSize]\n",
    "        xys = [(point.x, point.y) for point in pointsSubset]\n",
    "        values = [value[0] for value in src.sample(xys, indexes=5)]\n",
    "        allValues += values\n",
    "        \n",
    "        \n",
    "t2 = datetime.now()\n",
    "print(t2-t1)\n",
    "\n",
    "barrenSparsV = gpd.GeoDataFrame({\"MSAVI\":allValues}, geometry=allPoints, crs=\"epsg:2868\")\n",
    "barrenSparsV.to_file(\"../OtherData/TrainingData/barrenSparsV.gpkg\", driver=\"GPKG\")\n",
    "barrenSparsV.head()\n",
    "barrenPts = barrenSparsV[barrenSparsV.MSAVI <= 115]\n",
    "sparsePts = barrenSparsV[(barrenSparsV.MSAVI > 115) & (barrenSparsV.MSAVI <= 135)]\n",
    "densePts = barrenSparsV[barrenSparsV.MSAVI > 135]\n",
    "\n",
    "print(len(barrenPts), len(sparsePts), len(densePts))\n",
    "\n",
    "%chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['motorway_link', 'motorway', 'secondary', 'residential',\n",
       "       'tertiary', 'service', 'track', 'track_grade4', 'path',\n",
       "       'unclassified', 'footway', 'trunk', 'track_grade2', 'cycleway',\n",
       "       'trunk_link', 'track_grade3', 'pedestrian', 'primary_link',\n",
       "       'secondary_link', 'track_grade5', 'living_street', 'primary',\n",
       "       'tertiary_link', 'track_grade1', 'steps', 'unknown', 'bridleway'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_roads = [\"service\",\"footway\",\"pedestrian\",\"living_street\",\"steps\",\"unknown\",\"bridleway\"]\n",
    "osm_roads = osm_roads[(~osm_roads.fclass.isin(bad_roads)) | (~osm_roads.fclass.str.contains(\"track\"))]\n",
    "osm_roads.fclass.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
