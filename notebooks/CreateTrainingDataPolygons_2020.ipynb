{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BenJames\\anaconda3\\envs\\geospatial\\lib\\site-packages\\geopandas\\geodataframe.py:294: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for f in features_lst:\n"
     ]
    }
   ],
   "source": [
    "import rasterio as rio\n",
    "from rasterio.windows import from_bounds, transform\n",
    "from rasterio.features import shapes\n",
    "from rasterio.mask import mask\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "from shapely.geometry import Point, box, Polygon\n",
    "import math\n",
    "from rasterio.features import shapes\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "landcover = r\"C:/Users/BenJames/Documents/PAG-Data/DirtRoads/pima_landcover_noroads/pima_landcover_noroads.img\"\n",
    "orthoSeg_loc = r\"../EPCExtent_30cm/Orthos_Segmented/Ortho2019Segmented.vrt\"\n",
    "\n",
    "if not os.path.exists(landcover):\n",
    "    landcover = \"/media/ben/56A08937A0891E9D/Users/BenJames/Documents/PAG-Data/DirtRoads/pima_landcover_noroads/pima_landcover_noroads.img\"\n",
    "with rio.open(landcover) as src:\n",
    "    bnds = src.bounds\n",
    "    landcover_extent = box(*bnds)\n",
    "\n",
    "boxesdf_loc = \"../EPCExtent_30cm/Ortho_5kSubIndex.gpkg\"\n",
    "targetboxes_loc = \"../OtherData/targetBoxes_20201208.gpkg\"\n",
    "targetboxes = gpd.read_file(targetboxes_loc)\n",
    "boxesdf = gpd.read_file(boxesdf_loc)\n",
    "\n",
    "hag_tindex_loc = \"../EPCExtent_30cm/Elevation_80cmNPS/HAG_2015/HAG_tindex.gpkg\"\n",
    "hag_tindex = gpd.read_file(hag_tindex_loc)\n",
    "\n",
    "targetboxes = targetboxes[targetboxes.within(hag_tindex.unary_union)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"PondsLakes\", \"Pool\", \"Dense Vegetation\", \"Sparse Vegetation\", \"Barren\", \"Irrigated Lands/Turf\",  \"Asphault\", \"Other Impervious\", \"Structure\"]\n",
    "ortho_vrt = r\"M:/PAG2019/EPCExtent_30cm/Orthos/EPC_30cmOrtho_2019.vrt\"\n",
    "orthoSeg_loc = r\"../EPCExtent_30cm/Orthos_Segmented/Ortho2019Segmented.vrt\"\n",
    "if not os.path.exists(ortho_vrt):\n",
    "    ortho_vrt = \"../EPCExtent_30cm/Orthos/EPC_30cmOrtho_2019.vrt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Water - Deep and/or turbid water. Ponds & Lakes\n",
    "- Pools - Clear and shallow water with concrete substrate. Backyard pools and public pools.\n",
    "- Dense Vegetation - High vegetation index value - Trees, bushes\n",
    "- Sparse Vegetation - Low vegetation index values - shrubs, cacti\n",
    "- Irrigated Lands/Turf - ground-level, high vi value - Grasses, fields, agriculture\n",
    "- Barren - ground-level, dirt or rock\n",
    "- Asphault - generally newer asphault associated with roads\n",
    "- Other Impervious - concrete, dirty asphault, older asphault\n",
    "- Structures - impervious surfaces elevated off the ground - generally fixed built structures. Could be cars/trailers/RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delOSMColumns(df, exception):\n",
    "    mandatory = ['osm_id', 'osm_way_id', 'name', 'type', \"other_tags\", \"geometry\"]\n",
    "    mandatory.append(exception)\n",
    "    for col in df.columns.tolist():\n",
    "        if col not in mandatory:\n",
    "            del df[col]\n",
    "    return df\n",
    "\n",
    "\n",
    "def pullTagValues(other_tags, target_tag):\n",
    "    return None if other_tags is None or target_tag not in other_tags else \",\".join([tag.split(\"=>\")[1].replace('\"','') for tag in other_tags.split(\",\") if target_tag in tag])\n",
    "\n",
    "\n",
    "def getFeaturesInBounds(landcover_ras, bnds, landcoverValue=None, msaviUpperLimit=None, msaviLowerLimit=None):\n",
    "    polys = []\n",
    "    #try:\n",
    "    with rio.open(landcover_ras) as src:\n",
    "        twin = from_bounds(bnds[0], bnds[1], bnds[2], bnds[3], transform=src.transform)\n",
    "        wtrans = transform(twin, src.transform)\n",
    "        nd = src.nodata\n",
    "        window_array = src.read(1, window=twin)\n",
    "        wtrans = transform(twin, src.transform)\n",
    "        winW = window_array.shape[1]\n",
    "        winH = window_array.shape[0]\n",
    "\n",
    "    allMask = np.ones(window_array.shape).astype(bool)\n",
    "    \n",
    "    if msaviUpperLimit or msaviLowerLimit:\n",
    "        with rio.open(orthoSeg_loc) as src:\n",
    "            twin = from_bounds(bnds[0], bnds[1], bnds[2], bnds[3], transform=src.transform)\n",
    "            msavi_array = src.read(5, window=twin, out_shape=window_array.shape)\n",
    "            vegMask = (msavi_array <= msaviUpperLimit) & (msavi_array > msaviLowerLimit)\n",
    "    else:\n",
    "        vegMask = None\n",
    "\n",
    "    \n",
    "    if landcoverValue:\n",
    "        landcoverMask = window_array == landcoverValue\n",
    "    else:\n",
    "        landcoverMask == None\n",
    "    \n",
    "    if landcoverMask is not None:\n",
    "        allMask = landcoverMask & allMask\n",
    "    if vegMask is not None:\n",
    "        allMask = vegMask & allMask\n",
    "\n",
    "    targetFeatures = shapes(window_array, allMask, transform=wtrans)\n",
    "    targetFeatures = [feat for feat in targetFeatures]\n",
    "\n",
    "    if len(targetFeatures) != 0:\n",
    "        for feat in targetFeatures:\n",
    "            polys.append(Polygon(feat[0][\"coordinates\"][0]))\n",
    "    #except Exception as e:\n",
    "    #    print(f\"Error on bounds {bnds}. Returning empty list\\n{e}\")\n",
    "        \n",
    "    return polys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ../OtherData/osmPolygons_regional.gpkg...\n"
     ]
    }
   ],
   "source": [
    "osmPolygons_regional_loc = \"../OtherData/osmPolygons_regional.gpkg\"\n",
    "if not os.path.exists(osmPolygons_regional_loc):\n",
    "    print(f\"Creating {osmPolygons_regional_loc}...\")\n",
    "    osmPoly_loc = r\"../OtherData/arizona-latest.osm.20201215.gpkg\"\n",
    "    osm_polygons = gpd.read_file(osmPoly_loc, layer=\"multipolygons\").to_crs(\"epsg:2868\")\n",
    "    osm_polygons = osm_polygons[osm_polygons.intersects(landcover_extent)]\n",
    "    osm_polygons.to_file(osmPolygons_regional_loc)\n",
    "else:\n",
    "    print(f\"Reading in {osmPolygons_regional_loc}...\")\n",
    "    osm_polygons = gpd.read_file(osm_polygons)\n",
    "    \n",
    "%chime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------\n",
    "# PondsLakes & Pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in ../OtherData/TrainingData/osmwater_2020.gpkg\n"
     ]
    }
   ],
   "source": [
    "# Get Water# natural, man-made, leisure (pool), landuse (basin, reservoir)\n",
    "osm_water = \"../OtherData/TrainingData/osmwater_2020.gpkg\"\n",
    "\n",
    "if not os.path.exists(osm_water):\n",
    "    print(f\"Creating {osm_water}\")\n",
    "    osm_polyWater = osm_polygons[osm_polygons.natural == 'water'].copy().reset_index()\n",
    "    osm_polyWater = delOSMColumns(osm_polyWater, \"natural\")\n",
    "    osm_polyWater[\"water\"] = osm_polyWater.other_tags.apply(lambda ot: pullTagValues(ot, \"water\"))\n",
    "    display(osm_polyWater.head())\n",
    "\n",
    "    osm_polyWater[\"intermittent\"] = osm_basins.other_tags.apply(lambda ot: pullTagValues(ot, \"intermittent\"))\n",
    "    osm_polyWater = osm_polyWater[(~pd.isnull(osm_polyWater.water)) & (osm_polyWater.intermittent != \"yes\")\n",
    "                                 & (osm_polyWater.water.str.lower() != \"river\") & (osm_polyWater.water.str.lower() != \"wash\")]\n",
    "    osm_polyWater\n",
    "\n",
    "    if \"review\" not in osm_polyWater.columns.tolist():\n",
    "        osm_polyWater[\"review\"] = None\n",
    "\n",
    "    for i, row in osm_polyWater.copy().iterrows():\n",
    "        if row.review is not None:\n",
    "            continue\n",
    "        buffered = row.geometry.centroid.buffer(100)\n",
    "        bndbox = buffered.bounds\n",
    "        with rio.open(ortho_vrt) as src:\n",
    "            winb = from_bounds(bndbox[0], bndbox[1], bndbox[2], bndbox[3], transform=src.transform)\n",
    "            raster = src.read(window=winb)\n",
    "        if 0 not in raster.shape:    \n",
    "            show(raster[:3])\n",
    "            result = input(\"Enter eval\")\n",
    "        else:\n",
    "            result = 666\n",
    "\n",
    "        osm_polyWater.at[i, 'review'] = result\n",
    "\n",
    "    osm_polyWater.to_file(osm_water, driver=\"GPKG\")\n",
    "    print(f\"Wrote out to {osm_water}\")\n",
    "else:\n",
    "    print(f\"Reading in {osm_water}\")\n",
    "    osm_polyWater = gpd.read_file(osm_water)\n",
    "    osm_polyWater = osm_polyWater[osm_polyWater.review!='666']\n",
    "        \n",
    "%chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ../OtherData/TrainingData/poolsPolys.gpkg\n",
      "Reading in ../OtherData/TrainingData/water_2015.gpkg\n"
     ]
    }
   ],
   "source": [
    "water_2015_loc = r\"../OtherData/TrainingData/water_2015.gpkg\"\n",
    "pondsLakes_2015_loc = r\"../OtherData/TrainingData/pondsLakes_2015.gpkg\"\n",
    "poolsPoly_loc  = r\"../OtherData/TrainingData/poolsPolys.gpkg\"\n",
    "\n",
    "if not os.path.exists(poolsPoly_loc):\n",
    "    osmPools_2020 = osm_polyWater[(osm_polyWater.review == '0') & (osm_polyWater.water == \"pool\")]\n",
    "    print(f\"Creating {poolsPoly_loc}\")\n",
    "    if not os.path.exists(water_2015_loc):\n",
    "        print(f\"Creating {water_2015_loc}\")\n",
    "        t1 = datetime.now()\n",
    "        water_polys = Parallel(n_jobs=10, verbose=10)(delayed(getFeaturesInBounds)(landcover, row.geometry.bounds, landcoverValue=water_val) for i, row in boxesdf.iterrows())\n",
    "        %chime\n",
    "        print(datetime.now()-t1)\n",
    "        allPolys = list(itertools.chain.from_iterable(water_polys))\n",
    "        water_2015 = gpd.GeoDataFrame(geometry=allPolys, crs=\"epsg:2868\")\n",
    "        water_2015[\"Area\"] = water_2015.geometry.area\n",
    "        water_2015.to_file(water_2015_loc, driver=\"GPKG\")\n",
    "    else:\n",
    "        print(f\"Reading in {water_2015_loc}\")\n",
    "        water_2015 = gpd.read_file(water_2015_loc)\n",
    "\n",
    "    \n",
    "    non_osmPondsLakes = water_2015[~water_2015.intersects(osmPondsLakes_2020.unary_union)].copy()\n",
    "    # merge adjacent geometries\n",
    "    non_osmPondsLakes.geometry = non_osmPondsLakes.geometry.buffer(3)#.explode().buffer(-3)\n",
    "    non_osmPondsLakes[\"Type\"] = \"water\"\n",
    "    non_osmPondsLakes = non_osmPondsLakes.dissolve(by=\"Type\")\n",
    "    non_osmPondsLakes = gpd.GeoDataFrame(geometry = [g for g in non_osmPondsLakes.geometry.values[0]], crs = non_osmPondsLakes.crs)\n",
    "    non_osmPondsLakes.geometry = non_osmPondsLakes.geometry.buffer(-3)\n",
    "    non_osmPondsLakes[\"Area\"] = non_osmPondsLakes.geometry.area\n",
    "\n",
    "    pools_2015 = non_osmPondsLakes[(non_osmPondsLakes.Area < 500) & (non_osmPondsLakes.Area > 200)].copy()\n",
    "\n",
    "    pools = pd.concat([osmPools_2020, pools_2015])\n",
    "    \n",
    "    pools.to_file(poolsPoly_loc, driver=\"GPKG\")\n",
    "else:\n",
    "    print(f\"Reading in {poolsPoly_loc}\")\n",
    "    pools = gpd.read_file(poolsPoly_loc)\n",
    "\n",
    "%chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ../OtherData/TrainingData/pondsLakesPolys.gpkg...\n",
      "Reading in ../OtherData/TrainingData/pondsLakes_2015.gpkg\n"
     ]
    }
   ],
   "source": [
    "pondsLakesPoly_loc = r\"../OtherData/TrainingData/pondsLakesPolys.gpkg\"\n",
    "\n",
    "if not os.path.exists(pondsLakesPoly_loc):\n",
    "    print(f\"Creating {pondsLakesPoly_loc}...\")\n",
    "    \n",
    "    if not os.path.exists(pondsLakes_2015_loc):\n",
    "        pondsLakes_2015 = non_osmPondsLakes[non_osmPondsLakes.area>3000]\n",
    "\n",
    "        if \"review\" not in pondsLakes_2015.columns.tolist():\n",
    "            print(\"HERE\")\n",
    "            non_osmPondsLakes[\"review\"] = None\n",
    "\n",
    "        for i, row in pondsLakes_2015.copy().iterrows():\n",
    "            if row.review is not None:\n",
    "                continue\n",
    "            buffered = row.geometry.centroid.buffer(100)\n",
    "            bndbox = buffered.bounds\n",
    "            with rio.open(ortho_vrt) as src:\n",
    "                winb = from_bounds(bndbox[0], bndbox[1], bndbox[2], bndbox[3], transform=src.transform)\n",
    "                raster = src.read(window=winb)\n",
    "            if 0 not in raster.shape:    \n",
    "                plt = show(raster[:3])\n",
    "                display.display(plt)#, ax=ax))\n",
    "                result = input(f\"Enter eval for {i}\")\n",
    "                display.clear_output(wait=True)\n",
    "            else:\n",
    "                result = 666\n",
    "            fig.clear()\n",
    "            pondsLakes_2015.at[i, 'review'] = result\n",
    "\n",
    "        pondsLakes_2015 = pondsLakes_2015[pondsLakes_2015.review=='0']\n",
    "        pondsLakes_2015.to_file(pondsLakes_2015_loc, driver=\"GPKG\")\n",
    "    else:\n",
    "        print(f\"Reading in {pondsLakes_2015_loc}\")\n",
    "        pondsLakes_2015 = gpd.read_file(pondsLakes_2015_loc)\n",
    "\n",
    "    osmPondsLakes_2020 = osm_polyWater[(osm_polyWater.review == '0') & (osm_polyWater.water != \"pool\")]\n",
    "\n",
    "    pondsLakes = pd.concat([osmPondsLakes_2020, pondsLakes_2015])\n",
    "    del pondsLakes[\"area\"]\n",
    "    pondsLakes[\"Area\"] = pondsLakes.geometry.area\n",
    "    pondsLakes.to_file(pondsLakesPoly_loc, driver=\"GPKG\")\n",
    "else:\n",
    "    print(f\"Reading in {pondsLakesPoly_loc}...\")\n",
    "    pondsLakes = gpd.read_file(pondsLakesPoly_loc)\n",
    "    \n",
    "%chime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in ../OtherData/TrainingData/StructuresPoly.gpkg...\n"
     ]
    }
   ],
   "source": [
    "structuresPoly_loc = r\"../OtherData/TrainingData/StructuresPoly.gpkg\"\n",
    "\n",
    "structures_2015_loc = r\"../OtherData/TrainingData/structures_2015.gpkg\"\n",
    "osmBuildings_loc = \"../OtherData/TrainingData/osmBuildings_2020.gpkg\"\n",
    "\n",
    "if not os.path.exists(structuresPoly_loc):\n",
    "    if not os.path.exists(osmBuildings_loc):\n",
    "        print(f\"Creating {osmBuildings_loc}\")\n",
    "        osm_polyBuildings = osm_polygons[~(pd.isnull(osm_polygons.building))].copy().reset_index()\n",
    "        osm_polyBuildings = delOSMColumns(osm_polyBuildings, \"building\")\n",
    "        osm_polyBuildings[\"buildingTag\"] = osm_polyBuildings.other_tags.apply(lambda ot: pullTagValues(ot, \"building\"))\n",
    "        osm_polyBuildings[\"area\"] = osm_polyBuildings.geometry.area\n",
    "        osm_polyBuildings.to_file(osmBuildings_loc, driver=\"GPKG\")\n",
    "    else:\n",
    "        print(f\"Reading in {osmBuildings_loc}\")\n",
    "        osm_polyBuildings = gpd.read_file(osmBuildings_loc)\n",
    "\n",
    "    print(osm_polyBuildings.building.unique())\n",
    "    # remove parking (elevated outdoor may unnecessarily confuse model), ruins, bunkers, collapsed, construction, stable, bridge\n",
    "    bad_buildings = [\"parking\", \"ruins\", \"bunker\", \"collapsed\", \"construction\", \"stable\", \"bridge\"]\n",
    "    osm_polyBuildings = osm_polyBuildings[~osm_polyBuildings[\"building\"].isin(bad_buildings)]\n",
    "    print(osm_polyBuildings.shape)\n",
    "\n",
    "    structureVal = 7\n",
    "\n",
    "    if not os.path.exists(structures_2015_loc):\n",
    "        print(f\"Creating {structures_2015_loc}\")\n",
    "        t1 = datetime.now()\n",
    "\n",
    "        structures_polys = Parallel(n_jobs=10, verbose=5)(delayed(getFeaturesInBounds)(landcover, row.geometry.bounds, landcoverValue=structureVal, msaviUpperLimit=None, msaviLowerLimit=None) for i, row in targetboxes.iterrows())\n",
    "\n",
    "        allPolys = list(itertools.chain.from_iterable(structures_polys))\n",
    "        structures_2015 = gpd.GeoDataFrame(geometry=allPolys, crs=\"epsg:2868\")\n",
    "        structures_2015[\"area\"] = structures_2015.geometry.area\n",
    "        structures_2015.to_file(structures_2015_loc, driver=\"GPKG\")\n",
    "        print(datetime.now()-t1)\n",
    "    else:\n",
    "        print(f\"Reading in {structures_2015_loc}\")\n",
    "        structures_2015 = gpd.read_file(structures_2015_loc)\n",
    "\n",
    "    # osm buildings are offset (different imagery), so filter out anything below 1500 square feet\n",
    "    osm_polyBuildings = osm_polyBuildings[osm_polyBuildings.area>1500]\n",
    "    # filtering spatial join much faster than usual intersect\n",
    "    osmBuildings_non2015 = gpd.sjoin(osm_polyBuildings, structures_2015, op=\"intersects\", how=\"left\")\n",
    "    osmBuildings_non2015 = osmBuildings_non2015[pd.isnull(osmBuildings_non2015.index_right)]\n",
    "\n",
    "    structuresPoly = pd.concat([structures_2015, osmBuildings_non2015])\n",
    "    structuresPoly[\"area\"] = structuresPoly.geometry.area\n",
    "    structuresPoly.to_file(structuresPoly_loc, driver=\"GPKG\")\n",
    "else:\n",
    "    print(f\"Reading in {structuresPoly_loc}...\")\n",
    "    structuresPoly = gpd.read_file(structuresPoly_loc)\n",
    "\n",
    "%chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>osm_id</th>\n",
       "      <th>osm_way_id</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>building</th>\n",
       "      <th>other_tags</th>\n",
       "      <th>buildingTag</th>\n",
       "      <th>area_left</th>\n",
       "      <th>index_right</th>\n",
       "      <th>area_right</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>952.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((995017.592 494544.201, 995017.592 49...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    area osm_id osm_way_id  name  type building other_tags buildingTag  \\\n",
       "3  952.0   None       None  None  None     None       None        None   \n",
       "\n",
       "   area_left index_right area_right  \\\n",
       "3        NaN        None       None   \n",
       "\n",
       "                                            geometry  \n",
       "3  POLYGON ((995017.592 494544.201, 995017.592 49...  "
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tstruct = structuresPoly.iloc[3:4]\n",
    "tstruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Veg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in ../OtherData/TrainingData/denseVeg_2015.gpkg\n"
     ]
    }
   ],
   "source": [
    "denseVeg_2015_loc = r\"../OtherData/TrainingData/denseVeg_2015.gpkg\"\n",
    "treesShrubs_val = 2\n",
    "\n",
    "if not os.path.exists(denseVeg_2015_loc):\n",
    "    print(f\"Creating {denseVeg_2015_loc}\")\n",
    "    t1 = datetime.now()\n",
    "\n",
    "    denseVeg_polys = Parallel(n_jobs=10, verbose=5)(delayed(getFeaturesInBounds)(landcover, row.geometry.bounds, landcoverValue=treesShrubs_val, msaviUpperLimit=255, msaviLowerLimit=135) for i, row in targetboxes.iterrows())\n",
    "\n",
    "    allPolys = list(itertools.chain.from_iterable(denseVeg_polys))\n",
    "    denseVeg_2015 = gpd.GeoDataFrame(geometry=allPolys, crs=\"epsg:2868\")\n",
    "    denseVeg_2015[\"area\"] = denseVeg_2015.geometry.area\n",
    "    denseVeg_2015.to_file(denseVeg_2015_loc, driver=\"GPKG\")\n",
    "    print(datetime.now()-t1)\n",
    "else:\n",
    "    print(f\"Reading in {denseVeg_2015_loc}\")\n",
    "    denseVeg_2015 = gpd.read_file(denseVeg_2015_loc)\n",
    "\n",
    "%chime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARSE VEG AND BARREN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruralRegions_loc = r\"../OtherData/TrainingData/DesertBarrenVegetation_2015.gpkg\"\n",
    "rural_regions = gpd.read_file(ruralRegions_loc)\n",
    "rural_regions = rural_regions[rural_regions.within(hag_tindex.unary_union)]\n",
    "\n",
    "boxesdf = gpd.read_file(boxesdf_loc)\n",
    "boxesdf[\"geometry\"] = boxesdf[\"geometry\"].apply(lambda g: box(*g.buffer(-50).bounds))\n",
    "\n",
    "rural_regions = gpd.overlay(rural_regions, boxesdf, how=\"union\")\n",
    "rural_regions = rural_regions[~pd.isnull(rural_regions[\"Area\"])]\n",
    "\n",
    "rural_regions.to_file(ruralRegions_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import box\n",
    "from rasterio import features\n",
    "from rasterio.windows import transform as wtransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rio.open(\"C:/Users/BenJames/Documents/PAG-Data/DirtRoads/pima_landcover_noroads/pima_landcover_noroads.img\") as src:\n",
    "        win = from_bounds(*inPoly.bounds, src.transform)\n",
    "        win_trans = wtransform(win, src.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "barrenSparse_2015Urban_loc = r\"../OtherData/TrainingData/BarrenPolys_MidTown2015.gpkg\"\n",
    "def barrenFrom2015(polygon):\n",
    "    with rio.open(\"C:/Users/BenJames/Documents/PAG-Data/DirtRoads/pima_landcover_noroads/pima_landcover_noroads.img\") as src:\n",
    "        win = from_bounds(*polygon.bounds, src.transform)\n",
    "        win_trans = wtransform(win, src.transform)\n",
    "        a = src.read(1, window=win)\n",
    "        mask = a == 5\n",
    "        shapes = [Polygon(poly[0][\"coordinates\"][0]) for poly in features.shapes(a, mask=mask, transform=win_trans)]\n",
    "        shapes_gdf = gpd.GeoDataFrame({\"geometry\":shapes}, geometry=\"geometry\", crs=src.crs)\n",
    "        \n",
    "    return shapes_gdf\n",
    "        \n",
    "        \n",
    "inPoly = box(984278,436946,1009295,456063)\n",
    "barrenPolys = barrenFrom2015(inPoly)\n",
    "barrenPolys.to_file(barrenSparse_2015Urban_loc, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePoints(df, totalPointsDF, minPerFeature):\n",
    "    \n",
    "    df[\"Area\"] = df.geometry.area\n",
    "    remainderPoints = totalPointsDF - (minPerFeature * len(df))\n",
    "    total_area = df.Area.sum()\n",
    "    #df[\"POT\"] = df[\"Area\"].apply(lambda a: a/total_area)\n",
    "    #df[\"NumPoints\"] = df[\"POT\"].apply(lambda pot: int(minPerFeature+(pot*remainderPoints)))\n",
    "\n",
    "    allPoints = []\n",
    "    for i, row in df.iterrows():\n",
    "        bnds = row.geometry.bounds\n",
    "        featurePoints = []\n",
    "        while (len(featurePoints) < row.NumPoints) :\n",
    "            #print(f\"WHILE {len(featurePoints), row.NumPoints}\")\n",
    "            x = random.uniform(bnds[0], bnds[2])\n",
    "            y = random.uniform(bnds[1], bnds[3])\n",
    "            point = Point(x,y)\n",
    "            if point.intersects(row.geometry):\n",
    "                featurePoints.append(point)\n",
    "        allPoints += featurePoints\n",
    "    return allPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "3500000\n",
      "4000000\n",
      "4500000\n",
      "5000000\n",
      "5500000\n",
      "6000000\n",
      "6500000\n",
      "7000000\n",
      "7500000\n",
      "8000000\n",
      "8500000\n",
      "9000000\n",
      "9500000\n",
      "3:36:29.633577\n",
      "6747567 2682605 569726\n"
     ]
    }
   ],
   "source": [
    "barrenSparseV_loc = \"../OtherData/TrainingData/barrenSparseVPoints.gpkg\"\n",
    "\n",
    "if not os.path.exists(barrenSparseV_loc):\n",
    "    # masking raster with polygon and turning into shapes takes way to long. Since we know these area contain only one of three values denseV, sparseV, or barren, justsparseVegPoints_loc and filter those\n",
    "    print(f\"Creating {barrenSparseV_loc}...\")\n",
    "\n",
    "    totalPoints = 10 * 1000000\n",
    "    minPoints = 5000\n",
    "    barrenSparsePoints = generatePoints(rural_regions, totalPoints, minPoints)\n",
    "    print(f\"Created {len(barrenSparsePoints)} random points\")\n",
    "    \n",
    "    subSize = 10000\n",
    "    t1 = datetime.now()\n",
    "    allValues=[]\n",
    "    with rio.open(orthoSeg_loc) as src:\n",
    "        for i in range(0, len(barrenSparsePoints), subSize):\n",
    "            if i % 500000 == 0 and i != 0:\n",
    "                print(i)\n",
    "            pointsSubset = barrenSparsePoints[i:i+subSize]\n",
    "            xys = [(point.x, point.y) for point in pointsSubset]\n",
    "            values = [value[0] for value in src.sample(xys, indexes=5)]\n",
    "            allValues += values\n",
    "\n",
    "    t2 = datetime.now()\n",
    "    print(t2-t1)\n",
    "\n",
    "    barrenSparseV = gpd.GeoDataFrame({\"MSAVI\":allValues}, geometry=barrenSparsePoints, crs=\"epsg:2868\")\n",
    "    barrenSparseV.to_file(barrenSparseV_loc, driver=\"GPKG\")\n",
    "    \n",
    "\n",
    "else:\n",
    "    print(f\"Reading in {barrenSparseV_loc}...\")\n",
    "    barrenSparseV = gpd.read_file(barrenSparseV_loc)\n",
    "\n",
    "%chime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________\n",
    "# Irrigated Land\n",
    "\n",
    "2015 irrigated classification with MSAVI values greater than the 135 cutoff (dense veg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in ../OtherData/TrainingData/irrigatedLand_2015.gpkg\n",
      "17309496.0\n"
     ]
    }
   ],
   "source": [
    "irrigatedLand_2015_loc = r\"../OtherData/TrainingData/irrigatedLand_2015.gpkg\"\n",
    "irrigatedVal = 3\n",
    "\n",
    "if not os.path.exists(irrigatedLand_2015_loc):\n",
    "    print(f\"Creating {irrigatedLand_2015_loc}\")\n",
    "    targetboxes[\"geometry\"] = targetboxes[\"geometry\"].apply(lambda g: box(*g.buffer(-50).bounds))\n",
    "    t1 = datetime.now()\n",
    "\n",
    "    #irrigated_polys = Parallel(n_jobs=10, verbose=5)(delayed(getFeaturesInBounds)(landcover, row.geometry.bounds, irrigatedVal, msaviUpperLimit=255, msaviLowerLimit=135) for i, row in targetboxes.iterrows())\n",
    "\n",
    "    #allPolys = list(itertools.chain.from_iterable(irrigated_polys))\n",
    "    #irrigated_2015 = gpd.GeoDataFrame(geometry=allPolys, crs=\"epsg:2868\")\n",
    "    irrigated_2015[\"Area\"] = irrigated_2015.geometry.area\n",
    "    \n",
    "    irrigated_2015[\"Area\"] = irrigated_2015.geometry.area\n",
    "    irrigated_2015.to_file(irrigatedLand_2015_loc, driver=\"GPKG\")\n",
    "    print(datetime.now()-t1)\n",
    "else:\n",
    "    print(f\"Reading in {irrigatedLand_2015_loc}\")\n",
    "    irrigated_2015 = gpd.read_file(irrigatedLand_2015_loc)\n",
    "\n",
    "print(irrigated_2015[\"Area\"].sum())\n",
    "%chime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________\n",
    "# Asphault\n",
    "\n",
    "Drop points along osm road lines to ensure they fall on asphault\n",
    "Buffer osm roads to create polygons, drop points within polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in ../OtherData/TrainingData/asphaultPoly_new.gpkg...\n"
     ]
    }
   ],
   "source": [
    "osm_roads_loc = \"../OtherData/arizona-latest-20200507/gis_osm_roads_free_1.shp\"\n",
    "asphaultPoly_loc = \"../OtherData/TrainingData/asphaultPoly.gpkg\"\n",
    "urbanboxes = gpd.read_file(\"../OtherData/UrbanBoxes.gpkg\")\n",
    "\n",
    "buffer_size = 5\n",
    "\n",
    "if not os.path.exists(asphaultPoly_loc):\n",
    "    print(f\"Creating {asphaultPoly_loc}...\")\n",
    "    osm_roads_all = gpd.read_file(osm_roads_loc)\n",
    "    roadboxes = pd.concat([targetboxes, urbanboxes])\n",
    "    roadboxes[\"geometry\"] = roadboxes[\"geometry\"].apply(lambda g: box(*g.buffer(-50).bounds))\n",
    "    bad_roads = [\"path\", \"residential\", \"cycleway\", \"service\",\"footway\",\"pedestrian\",\"living_street\",\"steps\",\"unknown\",\"bridleway\", \"unclassified\"]\n",
    "    osm_roads = osm_roads_all[((~osm_roads_all.fclass.isin(bad_roads)) & (~osm_roads_all.fclass.str.contains(\"track\"))) | ((osm_roads_all.fclass == \"residential\") & (~pd.isnull(osm_roads_all.name)) & (osm_roads_all.intersects(urbanboxes.to_crs(osm_roads_all.crs).unary_union)))].copy()\n",
    "    osm_roads.to_crs(roadboxes.crs, inplace=True)\n",
    "    osm_roads[\"geometry\"] = osm_roads.buffer(buffer_size)\n",
    "    roads_targets = gpd.clip(osm_roads, roadboxes)\n",
    "    roads_targets = gpd.overlay(roads_targets, roadboxes[roadboxes.intersects(roads_targets.unary_union)], how=\"union\")\n",
    "    roads_targets = roads_targets[~pd.isnull(roads_targets[\"osm_id\"])]\n",
    "    roads_targets = roads_targets.dissolve(by=[\"name\", \"filepath\"]).reset_index().explode().reset_index(drop=True)\n",
    "    roads_targets[\"Area\"] = roads_targets.geometry.area\n",
    "    roads_targets.to_file(asphaultPoly_loc, driver=\"GPKG\")\n",
    "else:\n",
    "    print(f\"Reading in {asphaultPoly_loc}...\")\n",
    "    road_targets = gpd.read_file(asphaultPoly_loc)\n",
    "\n",
    "%chime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:1000 of 15161\n",
      "index:2000 of 15161\n",
      "index:3000 of 15161\n",
      "index:4000 of 15161\n",
      "index:5000 of 15161\n",
      "index:6000 of 15161\n",
      "index:7000 of 15161\n",
      "index:8000 of 15161\n",
      "index:9000 of 15161\n",
      "index:10000 of 15161\n",
      "index:11000 of 15161\n",
      "index:12000 of 15161\n",
      "index:13000 of 15161\n",
      "index:14000 of 15161\n",
      "index:15000 of 15161\n",
      "2492450\n"
     ]
    }
   ],
   "source": [
    "num_asphault = 2.5 * 1000000\n",
    "minPoints = 1\n",
    "remainderPoints = num_asphault - (minPoints * len(roads_targets))\n",
    "total_area = roads_targets.Area.sum()\n",
    "roads_targets[\"POT\"] = roads_targets[\"Area\"].apply(lambda a: a/total_area)\n",
    "roads_targets[\"NumPoints\"] = roads_targets[\"POT\"].apply(lambda pot: int(minPoints+(pot*remainderPoints)))\n",
    "\n",
    "allPoints = []\n",
    "for i, row in roads_targets.iterrows():\n",
    "    if i%1000 == 0 and i != 0:\n",
    "        print(f\"index:{i} of {len(roads_targets)}\")\n",
    "    bnds = row.geometry.bounds\n",
    "    featurePoints = []\n",
    "    while len(featurePoints) < row.NumPoints:\n",
    "        x = random.uniform(bnds[0], bnds[2])\n",
    "        y = random.uniform(bnds[1], bnds[3])\n",
    "        point = Point(x,y)\n",
    "        if point.intersects(row.geometry):\n",
    "            featurePoints.append(point)\n",
    "    allPoints += featurePoints\n",
    "print(len(allPoints))\n",
    "\n",
    "asphaultPoints = gpd.GeoDataFrame(geometry=allPoints, crs=roads_targets.crs)\n",
    "asphaultPoints.to_file(\"../OtherData/TrainingData/asphaultPoints.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "%chime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________\n",
    "# Impervious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "imperviousPoly_loc = \"../OtherData/TrainingData/ImperviousPoly.gpkg\"\n",
    "impervious = gpd.read_file(imperviousPoly_loc)\n",
    "impervious[\"Area\"] = impervious.geometry.area\n",
    "impervious[\"Area\"].sum()/4\n",
    "\n",
    "boxesdf = gpd.read_file(boxesdf_loc)\n",
    "boxesdf[\"geometry\"] = boxesdf[\"geometry\"].apply(lambda g: box(*g.buffer(-50).bounds))\n",
    "impervious = gpd.overlay(impervious, boxesdf, how=\"union\")\n",
    "impervious = impervious[~pd.isnull(impervious[\"Area\"])]\n",
    "\n",
    "impervious.to_file(imperviousPoly_loc, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSnappedPointsLocation(geometry, rasterBnds, rasterRes):\n",
    "    \"\"\" Returns set of upper-right snapped pixel locations in set as (x, y)\"\"\"\n",
    "\n",
    "    geom_left, geom_bottom, geom_right, geom_top = geometry.bounds\n",
    "    \n",
    "    pix_diff_x_left = (geom_left - rasterBnds.left) / rasterRes % 1\n",
    "    pix_diff_y_bottom = (geom_bottom - rasterBnds.bottom) / rasterRes % 1\n",
    "    pix_diff_x_right = (geom_right - rasterBnds.right) / rasterRes % 1\n",
    "    pix_diff_y_top = (geom_top - rasterBnds.top) / rasterRes % 1\n",
    "    \n",
    "    geom_left -= pix_diff_x_left\n",
    "    geom_right += 1-pix_diff_x_left\n",
    "    geom_bottom -= pix_diff_y_bottom\n",
    "    geom_top += 1-pix_diff_y_top\n",
    "    \n",
    "    sizex = ceil((geom_right - geom_left)/rasterRes)\n",
    "    sizey = ceil((geom_top - geom_bottom)/rasterRes)\n",
    "    \n",
    "    points = []\n",
    "    for x in range(0, sizex):\n",
    "        xp = geom_left + (x*rasterRes) + (rasterRes/2)\n",
    "        for y in range(0, sizey):\n",
    "            yp = geom_bottom + (y*rasterRes) + (rasterRes/2)\n",
    "            points.append(Point(xp,yp))\n",
    "    \n",
    "    return [point for point in points if point.intersects(geometry)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rio.open(ortho_vrt) as src:\n",
    "    rasbnds = src.bounds\n",
    "    reso = src.res[0]\n",
    "    \n",
    "impervious_points = [getSnappedPointsLocation(geom, rasbnds, reso) for geom in impervious.geometry]\n",
    "impervious_points = list(itertools.chain.from_iterable(impervious_points))\n",
    "\n",
    "impervious_pnts = gpd.GeoDataFrame(geometry=impervious_points, crs = \"epsg:2868\")\n",
    "impervious_pnts.to_file(\"../OtherData/TrainingData/ImperviousPoints.gpkg\", driver=\"GPKG\")\n",
    "%chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31275316.208916515"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subSize = 10000\n",
    "t1 = datetime.now()\n",
    "allValues=[]\n",
    "orthoSeg_loc = r\"../EPCExtent_30cm/Orthos_Segmented/Ortho2019Segmented.vrt\"\n",
    "with rio.open(orthoSeg_loc) as src:\n",
    "    for i in range(0, len(allPoints), subSize):\n",
    "        if i%1000000 == 0:\n",
    "            print(i)\n",
    "        pointsSubset = allPoints[i:i+subSize]\n",
    "        xys = [(point.x, point.y) for point in pointsSubset]\n",
    "        values = [value[0] for value in src.sample(xys, indexes=5)]\n",
    "        allValues += values\n",
    "        \n",
    "        \n",
    "t2 = datetime.now()\n",
    "print(t2-t1)\n",
    "\n",
    "barrenSparsV = gpd.GeoDataFrame({\"MSAVI\":allValues}, geometry=allPoints, crs=\"epsg:2868\")\n",
    "barrenSparsV.to_file(\"../OtherData/TrainingData/barrenSparsV.gpkg\", driver=\"GPKG\")\n",
    "barrenSparsV.head()\n",
    "barrenPts = barrenSparsV[barrenSparsV.MSAVI <= 115]\n",
    "sparsePts = barrenSparsV[(barrenSparsV.MSAVI > 115) & (barrenSparsV.MSAVI <= 135)]\n",
    "densePts = barrenSparsV[barrenSparsV.MSAVI > 135]\n",
    "\n",
    "print(len(barrenPts), len(sparsePts), len(densePts))\n",
    "\n",
    "%chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['motorway_link', 'motorway', 'secondary', 'residential',\n",
       "       'tertiary', 'service', 'track', 'track_grade4', 'path',\n",
       "       'unclassified', 'footway', 'trunk', 'track_grade2', 'cycleway',\n",
       "       'trunk_link', 'track_grade3', 'pedestrian', 'primary_link',\n",
       "       'secondary_link', 'track_grade5', 'living_street', 'primary',\n",
       "       'tertiary_link', 'track_grade1', 'steps', 'unknown', 'bridleway'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_roads = [\"service\",\"footway\",\"pedestrian\",\"living_street\",\"steps\",\"unknown\",\"bridleway\"]\n",
    "osm_roads = osm_roads[(~osm_roads.fclass.isin(bad_roads)) | (~osm_roads.fclass.str.contains(\"track\"))]\n",
    "osm_roads.fclass.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
