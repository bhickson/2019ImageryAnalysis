{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPC EXTENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "from shapely.geometry import Polygon\n",
    "import numpy as np\n",
    "\n",
    "xmin,ymin,xmax,ymax =  tindex.total_bounds\n",
    "width = 5000\n",
    "height = 5000\n",
    "rows = int(np.ceil((ymax-ymin) /  height))\n",
    "cols = int(np.ceil((xmax-xmin) / width))\n",
    "XleftOrigin = xmin\n",
    "XrightOrigin = xmin + width\n",
    "YtopOrigin = ymax\n",
    "YbottomOrigin = ymax- height\n",
    "polygons = []\n",
    "for i in range(cols):\n",
    "    Ytop = YtopOrigin\n",
    "    Ybottom =YbottomOrigin\n",
    "    for j in range(rows):\n",
    "        polygons.append(Polygon([(XleftOrigin, Ytop), (XrightOrigin, Ytop), (XrightOrigin, Ybottom), (XleftOrigin, Ybottom)])) \n",
    "        Ytop = Ytop - height\n",
    "        Ybottom = Ybottom - height\n",
    "    XleftOrigin = XleftOrigin + width\n",
    "    XrightOrigin = XrightOrigin + width\n",
    "\n",
    "grid = gpd.GeoDataFrame({'geometry':polygons}, crs=tindex.crs)\n",
    "grid = grid[grid.intersects(tindex.unary_union)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "tindex_loc = \"../EPCExtent_30cm/Elevation_80cmNPS/EPCtindex.gpkg\"\n",
    "if not os.path.exists(tindex_loc):\n",
    "    create_tindex_cmd = f\"\"\"find ../EPCExtent_30cm/Elevation_80cmNPS/RGB/Full/ -iname \"*.laz\" | pdal tindex create --fast_boundary ../EPCExtent_30cm/Elevation_80cmNPS/EPCtindex.gpkg -f \"GPKG\" --t_srs \"EPSG:2868\" --stdin --lyr_name pdal\"\"\"\n",
    "    os.system(create_tindex_cmd)\n",
    "tindex = gpd.read_file(tindex_loc)\n",
    "\n",
    "# create subtile index since single LAZ files are almost a Gig\n",
    "nrows = 3\n",
    "ncols = 3\n",
    "polygons = []\n",
    "origin_files = []\n",
    "indexNums = []\n",
    "for i, row in tindex.iterrows():\n",
    "    geom = row.geometry\n",
    "    xmin,ymin,xmax,ymax = geom.bounds\n",
    "    width = xmax-xmin\n",
    "    height=ymax-ymin\n",
    "    \n",
    "    target_width = width/ncols\n",
    "    target_height=height/nrows\n",
    "\n",
    "    index=0\n",
    "    for y in range(nrows):\n",
    "        ymin_s = ymin + (target_height*y)\n",
    "        ymax_s = ymin + (target_height*(y+1))\n",
    "        for x in range(ncols):\n",
    "            index+=1\n",
    "            xmin_s = xmin + (target_width*x)\n",
    "            xmax_s = xmin + (target_width*(x+1))\n",
    "            indexNums.append(index)\n",
    "            origin_files.append(os.path.abspath(row.location))\n",
    "            polygons.append(Polygon([(xmin_s, ymax_s), (xmax_s, ymax_s), (xmax_s, ymin_s), (xmin_s, ymin_s)]))\n",
    "            \n",
    "subTileIndex = gpd.GeoDataFrame({'originFile':origin_files, \"subIndex\":indexNums,'geometry':polygons}, crs=tindex.crs)\n",
    "subTileIndex.to_file(\"../EPCExtent_30cm/Elevation_80cmNPS/subTileIndex.gpkg\", driver=\"GPKG\")\n",
    "subTileIndex.head(10)\n",
    "\n",
    "subTileSample = subTileIndex.sample(int(len(subTileIndex)*0.03))\n",
    "subTileSample.to_file(\"../EPCExtent_30cm/Elevation_80cmNPS/subTileSample.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "subTileIndex['quads'] = subTileIndex.originFile.apply(lambda f: os.path.basename(f)[:-4])\n",
    "# make sure that quads used in the downtown area are included in our sample set\n",
    "downtown_quads = [\"E0960_N470\", \"E0960_N490\", \"E0980_N430\", \"E0980_N450\", \"E0980_N470\", \"E0980_N490\", \"E1000_N450\", \"E1000_N470\", \"E1020_N450\", \"E1020_N470\"]\n",
    "must_quads = subTileIndex[subTileIndex.quads.isin(downtown_quads)]\n",
    "must_quads = must_quads.sample(int(len(must_quads)*0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning DTM Generations for 326 QQuads\n",
      "\n",
      "Starting on Central Tucson. 14 total tiles\n",
      "Starting combo 1 of 360\n",
      "Starting at 2020-07-10 15:55:58.195908 with params\n",
      "\tScalar:  0\n",
      "\tSlope: 0.001\n",
      "\tThreshold: 0.1\n",
      "\tWindows Size:100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   9 | elapsed:  2.7min remaining:  3.3min\n",
      "[Parallel(n_jobs=4)]: Done   6 out of   9 | elapsed:  5.0min remaining:  2.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0:07:27.559134\n",
      "Starting combo 2 of 360\n",
      "Starting at 2020-07-10 16:03:25.755612 with params\n",
      "\tScalar:  0\n",
      "\tSlope: 0.001\n",
      "\tThreshold: 0.1\n",
      "\tWindows Size:200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   9 out of   9 | elapsed:  7.5min finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   9 | elapsed:  3.9min remaining:  4.9min\n",
      "/home/ben/anaconda3/envs/geospatial/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:703: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdal\n",
    "import gdal\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import rasterio as rio\n",
    "from joblib import Parallel, delayed\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "temp_dir = os.path.join(os.path.abspath(\"../\"), 'temp')\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "json_pipe_base = \"\"\" { \"pipeline\": [ %s ] } \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Scalar: Scaling value\n",
    "    Increase this parameter for terrains with lots of height variation.\n",
    "    \n",
    "Slope: slope parameter\n",
    "    Which is a measure of “slope tolerance”. Increase this parameter for terrains with lots of \n",
    "    height variation. Should be set to something higher than 0.1 and not higher than 1.2.\n",
    "    \n",
    "Threshold:  Elevation threshold\n",
    "    Set this parameter to the minimum height (in meters) that you expect non-ground objects to be.\n",
    "    \n",
    "Window: Window radius parameter\n",
    "    Corresponds to the size of the largest feature (building, trees, etc.) to be removed.\n",
    "    Should be set to a value higher than 10.\n",
    "    \n",
    "Threshold option has the biggest impact on results (per OpenDroneMap docs)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "filterCrop = \"\"\"\n",
    "    {\n",
    "        \"type\":\"filters.crop\",\n",
    "        \"bounds\":\"([%s, %s],[%s, %s])\",\n",
    "        \"a_srs\":\"EPSG:2868\"   \n",
    "    }\"\"\"\n",
    "filterGround = \"\"\"\n",
    "    {\n",
    "        \"type\":\"filters.range\",\n",
    "        \"limits\":\"Classification[2:2]\"\n",
    "    }\"\"\"\n",
    "writeLAZ = \"\"\"\n",
    "    {\n",
    "        \"type\":\"writers.las\",\n",
    "        \"compression\":\"laszip\",\n",
    "        \"filename\":\"%s\"\n",
    "    }\"\"\"\n",
    "writeTIFF = \"\"\"\n",
    "    {\n",
    "        \"type\":\"writers.gdal\",\n",
    "        \"filename\": \"%s\",\n",
    "        \"resolution\": %s,\n",
    "        \"window_size\": %s,\n",
    "        \"nodata\": 0,\n",
    "        \"output_type\": \"idw\",\n",
    "        \"gdalopts\":\"TILED=YES, COMPRESS=LZW\"\n",
    "    }\"\"\"\n",
    "\n",
    "\n",
    "laz_folder = r\"../EPCExtent_30cm/Elevation_80cmNPS/RGB/Full\"\n",
    "\n",
    "dtm_folder = r\"../EPCExtent_30cm/Elevation_80cmNPS/DTM80cm\"\n",
    "ground_folder = r\"../EPCExtent_30cm/Elevation_80cmNPS/RGB/ground\"\n",
    "os.makedirs(dtm_folder, exist_ok=True)\n",
    "os.makedirs(ground_folder, exist_ok=True)\n",
    "\n",
    "inout_files = {}\n",
    "\n",
    "for f in os.listdir(laz_folder):\n",
    "    if f.endswith(\".laz\"):\n",
    "        infile = os.path.join(laz_folder, f)\n",
    "        outfile = os.path.join(dtm_folder, f.replace(\".laz\", \"_DTM80cm.tif\"))\n",
    "        inout_files[infile] = outfile\n",
    "        \n",
    "        \n",
    "def createDTM(row, laz_folder, dtm_folder, params, overwrite=False):\n",
    "    infile = row.originFile\n",
    "    fname = os.path.basename(infile)\n",
    "    #suffix = f\"Sc{params['scalar']}Sl{params['slope']}Th{params['threshold']}WS{params['win_size']}\"\n",
    "    suffix = os.path.basename(dtm_folder)\n",
    "    \n",
    "    #outlaz = os.path.join(laz_folder, fname.replace(\".laz\", f\"_{row.subIndex}_{suffix}.laz\"))\n",
    "    outtiff = os.path.join(dtm_folder, fname.replace(\".laz\", f\"_{row.subIndex}_{suffix}.tif\"))\n",
    "    \n",
    "    if os.path.exists(outtiff) and overwrite == False:\n",
    "        return\n",
    "    \n",
    "    filterSMRF = \"\"\"\n",
    "    {\n",
    "        \"type\": \"filters.smrf\",\n",
    "        \"returns\":\"first,last,intermediate,only\",\n",
    "        \"cell\":%s,\n",
    "        \"scalar\": %s,\n",
    "        \"slope\": %s,\n",
    "        \"threshold\": %s,\n",
    "        \"window\": %s     \n",
    "    }\"\"\"\n",
    "    \n",
    "    geom = row.geometry.buffer(params['win_size'])\n",
    "    xmin,ymin,xmax,ymax = geom.bounds\n",
    "    \n",
    "    #if os.path.exists(outlaz) and not overwrite:\n",
    "    #    return\n",
    "        #pipeline_json = \"\"\" \"%s\" \"\"\" % outlaz\n",
    "    if not os.path.exists(outtiff):\n",
    "        pipeline_json = \"\"\" \"%s\" \"\"\" % infile\n",
    "        pipeline_json += ',' + filterCrop % (xmin,xmax,ymin,ymax)\n",
    "        pipeline_json += ',' + filterSMRF % (params['resolution'],params['scalar'], params['slope'], params['threshold'], params['win_size'])\n",
    "        #pipeline_json += ',' + filterPMF % (8, 0.15, 40, 50, 1.5)\n",
    "        pipeline_json += ',' + filterGround\n",
    "        #pipeline_json += ',' + writeLAZ % (outlaz)\n",
    "        pipeline_json += ',' + writeTIFF % (outtiff, params['resolution'], 0)\n",
    "    else:\n",
    "        return outtiff\n",
    "        \n",
    "        \n",
    "    pipeline_json = json_pipe_base % pipeline_json.replace(\"\\\\\",\"/\")\n",
    "    \n",
    "    pipeline = pdal.Pipeline(pipeline_json)\n",
    "    pipeline.loglevel = 8\n",
    "\n",
    "    start = datetime.now()\n",
    "    try:\n",
    "        pipeline.execute()\n",
    "    except:\n",
    "        print(f\"Failed for {outtiff}\")\n",
    "    end = datetime.now()\n",
    "\n",
    "    print(f\"{datetime.now()}\\t-\\tFinished with {os.path.basename(outtiff)} - {end-start} elapsed\")\n",
    "    \n",
    "    return outtiff\n",
    "    \n",
    "print(\"\\nBeginning DTM Generations for {} QQuads\\n\".format(len(inout_files)))\n",
    "\n",
    "res = round(0.8/0.3048,5)\n",
    "\n",
    "# default scalar is 1.25\n",
    "#scalars = (0, 0.5, 1, 1.5)\n",
    "scalars = (0, 0.25, 0.5, 1, 1.5, 3)\n",
    "# default slope is 0.15\n",
    "#slopes = (0.01,0.10,0.25)\n",
    "slopes = (0.001, 0.005, 0.01, 0.10, 0.25)\n",
    "# default threshold is 0.5\n",
    "#thresholds = (0.5, 1, 1.5)\n",
    "thresholds = (0.1, 0.25, 0.5, 1, 1.5, 3)\n",
    "#default window is 18\n",
    "#windows = [100]\n",
    "windows = [100,200]\n",
    "\n",
    "tindex_2015_loc = \"/media/ben/PAG2015Elev/PAG2015DEM_index.gpkg\"\n",
    "tindex_2015 = gpd.read_file(tindex_2015_loc).to_crs(\"epsg:2868\")\n",
    "\n",
    "tindex_loc = \"../EPCExtent_30cm/Elevation_80cmNPS/EPCtindex.gpkg\"\n",
    "if not os.path.exists(tindex_loc):\n",
    "    create_tindex_cmd = f\"\"\"find ../EPCExtent_30cm/Elevation_80cmNPS/RGB/Full/ -iname \"*.laz\" | pdal tindex create --fast_boundary ../EPCExtent_30cm/Elevation_80cmNPS/EPCtindex.gpkg -f \"GPKG\" --t_srs \"EPSG:2868\" --stdin --lyr_name pdal\"\"\"\n",
    "    os.system(create_tindex_cmd)\n",
    "tindex = gpd.read_file(tindex_loc)\n",
    "\n",
    "subTileIndex_loc = \"../EPCExtent_30cm/Elevation_80cmNPS/subTileIndex.gpkg\"\n",
    "if not os.path.exists(subTileIndex_loc):\n",
    "    # create subtile index since single LAZ files are almost a Gig\n",
    "    nrows = 3\n",
    "    ncols = 3\n",
    "    polygons = []\n",
    "    origin_files = []\n",
    "    indexNums = []\n",
    "    for i, row in tindex.iterrows():\n",
    "        geom = row.geometry\n",
    "        xmin,ymin,xmax,ymax = geom.bounds\n",
    "        width = xmax-xmin\n",
    "        height=ymax-ymin\n",
    "\n",
    "        target_width = width/ncols\n",
    "        target_height=height/nrows\n",
    "\n",
    "        index=0\n",
    "        for y in range(nrows):\n",
    "            ymin_s = ymin + (target_height*y)\n",
    "            ymax_s = ymin + (target_height*(y+1))\n",
    "            for x in range(ncols):\n",
    "                index+=1\n",
    "                xmin_s = xmin + (target_width*x)\n",
    "                xmax_s = xmin + (target_width*(x+1))\n",
    "                indexNums.append(index)\n",
    "                origin_files.append(os.path.abspath(row.location))\n",
    "                polygons.append(Polygon([(xmin_s, ymax_s), (xmax_s, ymax_s), (xmax_s, ymin_s), (xmin_s, ymin_s)]))\n",
    "\n",
    "    subTileIndex = gpd.GeoDataFrame({'originFile':origin_files, \"subIndex\":indexNums,'geometry':polygons}, crs=tindex.crs)\n",
    "    subTileIndex.to_file(subTileIndex_loc, driver=\"GPKG\")\n",
    "subTileIndex = gpd.read_file(subTileIndex_loc)\n",
    "\n",
    "#make sure sub tiles intersect 2015 data for comparison\n",
    "subTileIndex = subTileIndex[subTileIndex.within(tindex_2015.unary_union.buffer(-500))]\n",
    "\n",
    "#sample_subtiles\n",
    "subTileSample_loc = \"../EPCExtent_30cm/Elevation_80cmNPS/subTileSample.gpkg\"\n",
    "if not os.path.exists(subTileSample_loc):\n",
    "    print(\"Creating new sub tile sample set\")\n",
    "    subTileSample = subTileIndex.sample(int(len(subTileIndex)*0.03))\n",
    "    subTileSample.to_file(subTileSample_loc, driver=\"GPKG\")\n",
    "subTileSample = gpd.read_file(subTileSample_loc)\n",
    "    \n",
    "centralTucsonSample_loc = \"../EPCExtent_30cm/Elevation_80cmNPS/centralTucson_sample.gpkg\"\n",
    "if not os.path.exists(centralTucsonSample_loc):\n",
    "    subTileIndex['quads'] = subTileIndex.originFile.apply(lambda f: os.path.basename(f)[:-4])\n",
    "    # make sure that quads used in the downtown area are included in our sample set\n",
    "    downtown_quads = [\"E0960_N470\", \"E0960_N490\", \"E0980_N430\", \"E0980_N450\", \"E0980_N470\", \"E0980_N490\", \"E1000_N450\", \"E1000_N470\", \"E1020_N450\", \"E1020_N470\"]\n",
    "    centralTucsonSample = subTileIndex[subTileIndex.quads.isin(downtown_quads)]\n",
    "    centralTucsonSample = centralTucsonSample.sample(int(len(centralTucsonSample)*0.1))\n",
    "    centralTucsonSample.to_file(centralTucsonSample_loc, driver=\"GPKG\")\n",
    "centralTucsonSample = gpd.read_file(centralTucsonSample_loc)\n",
    "\n",
    "total_possibilities = len(scalars) * len(slopes) * len(thresholds) * len(windows)\n",
    "combo_count = 0\n",
    "\n",
    "for name,df in {\"Central Tucson\" : centralTucsonSample, \"Regional Sample\" : subTileSample}.items():\n",
    "    print(f\"Starting on {name}. {len(name)} total tiles\")\n",
    "    for sci, sc in enumerate(scalars):\n",
    "        for sli, sl in enumerate(slopes):\n",
    "            for ti, thresh in enumerate(thresholds):\n",
    "                for wi, win in enumerate(windows):\n",
    "                    \n",
    "                    combo_count += 1\n",
    "                    print(f\"Starting combo {combo_count} of {total_possibilities}\")\n",
    "                    \n",
    "                    parameters=dict(\n",
    "                        scalar=sc,\n",
    "                        slope=sl,\n",
    "                        threshold=thresh,\n",
    "                        win_size=win,\n",
    "                        resolution=res\n",
    "                    )\n",
    "                    suffix = f\"Sc{parameters['scalar']}Sl{parameters['slope']}Th{parameters['threshold']}WS{parameters['win_size']}\"\n",
    "                    out_DTMFolder = os.path.join(dtm_folder,suffix)\n",
    "                    out_groundFolder = os.path.join(ground_folder, suffix)\n",
    "                    os.makedirs(out_DTMFolder, exist_ok=True)\n",
    "                    os.makedirs(out_groundFolder, exist_ok=True)\n",
    "                    \n",
    "                    t1 = datetime.now()\n",
    "                    print(f\"Starting at {t1} with params\\n\\tScalar:  {sc}\\n\\tSlope: {sl}\\n\\tThreshold: {thresh}\\n\\tWindows Size:{win}\")\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        json_pipes = Parallel(n_jobs=4, verbose=5, backend=\"loky\")(delayed(\n",
    "                            createDTM)(row, out_groundFolder, out_DTMFolder, parameters, overwrite=False) for i, row in df.iterrows())\n",
    "\n",
    "                    except:\n",
    "                        json_pipes = Parallel(n_jobs=4, verbose=5, backend=\"loky\")(delayed(\n",
    "                            createDTM)(row, out_groundFolder, out_DTMFolder, parameters, overwrite=False) for i, row in df.iterrows())\n",
    "\n",
    "                    print(f\"Finished in {datetime.now()-t1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting at {t1} with params\\n\\tScalar:  {sc}\\n\\tSlope: {sl}\\n\\tThreshold: {thresh}\")\n",
    "                \n",
    "for i, row in centralTucsonSample.iterrows():\n",
    "    json = createDTM(row, ground_folder, dtm_folder, parameters, overwrite=True)\n",
    "    break\n",
    "print(f\"Finished in {datetime.now()-t1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_loc = \"../EPCExtent_30cm/Elevation_80cmNPS/HAG_NED_80cm_DTG\"\n",
    "distance_files = glob(f\"{dist_loc}/*.tif\")\n",
    "\n",
    "params=parameters.copy()\n",
    "parameters.update(scalar=1.5, slope=0.15, threshold=0.5)\n",
    "\n",
    "dtg_breaks = (20,40,100,200,500)\n",
    "params = parameters.copy()\n",
    "\n",
    "infile = row.originFile\n",
    "fname = os.path.basename(infile)\n",
    "suffix = f\"Sc{params['scalar']}Sl{params['slope']}Th{params['threshold']}\"\n",
    "print(suffix)\n",
    "outlaz = os.path.join(ground_folder, fname.replace(\".laz\", f\"_{row.subIndex}_{suffix}.laz\"))\n",
    "outtiff = os.path.join(dtm_folder, fname.replace(\".laz\", f\"_{row.subIndex}_{suffix}.tif\"))\n",
    "\n",
    "for f in distance_files:\n",
    "    if fname.replace(\".laz\",\"\") in f:\n",
    "        dtg_file = f\n",
    "        break\n",
    "\n",
    "dimension = \"GpsTime\"\n",
    "filterColorization = \"\"\"{\n",
    "      \"type\":\"filters.colorization\",\n",
    "      \"dimensions\":\"%s\",\n",
    "      \"raster\":\"%s\"\n",
    "}\"\"\"\n",
    "filterSMRF = \"\"\"\n",
    "{\n",
    "    \"type\": \"filters.smrf\",\n",
    "    \"returns\":\"first,last,intermediate,only\",\n",
    "    \"cell\":%s,\n",
    "    \"scalar\": %s,\n",
    "    \"slope\": %s,\n",
    "    \"threshold\": %s,\n",
    "    \"ignore\":\"%s\",\n",
    "    \"window\": %s     \n",
    "}\"\"\"\n",
    "filterAssign = \"\"\"\n",
    "{\n",
    "      \"type\":\"filters.assign\",\n",
    "      \"assignment\": \"%s[:]=0\"\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "geom = row.geometry.buffer(100)\n",
    "xmin,ymin,xmax,ymax = geom.bounds\n",
    "\n",
    "overwrite=True\n",
    "if os.path.exists(outlaz) and not overwrite:\n",
    "    print(\"ok\")\n",
    "    #pipeline_json = \"\"\" \"%s\" \"\"\" % outlaz\n",
    "else:\n",
    "    pipeline_json = \"\"\" \"%s\" \"\"\" % infile\n",
    "    #crop input file to bounds of subsection\n",
    "    pipeline_json += ',' + filterCrop % (xmin,xmax,ymin,ymax)\n",
    "    #assign distance to ground values to existing dimension. Using GpsTime\n",
    "    pipeline_json += ',' + filterAssign % dimension\n",
    "    #extract values of DTG raster to laz\n",
    "    pipeline_json += ',' + filterColorization % (dimension, dtg_file)\n",
    "    \n",
    "    pipeline_json += ',' + filterSMRF % (params['resolution'],params['scalar'], params['slope'], params['threshold'], f\"{dimension}![:10]\", 15)\n",
    "    # add SMRF filters for each window size\n",
    "    for i in range(len(dtg_breaks)-1):\n",
    "        dtg_break = dtg_breaks[i]\n",
    "        low = dtg_break\n",
    "        if i != len(dtg_breaks) -2:\n",
    "            high = dtg_breaks[i+1]\n",
    "        else:\n",
    "            high = \"\"\n",
    "        rng = f\"{low}:{high}\"\n",
    "                                         \n",
    "        bs = 10 if dtg_break == 20 else 20\n",
    "        win_size = high + bs if dtg_breaks[-2] != dtg_break else low + bs\n",
    "        if i == len(dtg_breaks):\n",
    "            break\n",
    "        pipeline_json += ',' + filterSMRF % (params['resolution'],params['scalar'], params['slope'], params['threshold'], f\"{dimension}![{rng}]\", win_size+20)\n",
    "        #pipeline_json += ',' + filterPMF % (8, 0.15, 40, 50, 1.5)\n",
    "    pipeline_json += ',' + filterGround\n",
    "    #pipeline_json += ',' + writeLAZ % (outlaz)\n",
    "\n",
    "if not os.path.exists(outtiff):\n",
    "    pipeline_json += ',' + writeTIFF % (outtiff, params['resolution'],200)\n",
    "\n",
    "pipeline_json = json_pipe_base % pipeline_json.replace(\"\\\\\",\"/\")\n",
    "\n",
    "pipeline = pdal.Pipeline(pipeline_json)\n",
    "pipeline.loglevel = 8\n",
    "\n",
    "start = datetime.now()\n",
    "try:\n",
    "    pipeline.execute()\n",
    "except:\n",
    "    print(f\"Failed for {outlaz}\")\n",
    "end = datetime.now()\n",
    "\n",
    "print(f\"{datetime.now()}\\t-\\tFinished with {os.path.basename(outlaz)} - {end-start} elapsed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"smrfMod.json\", \"w\") as dst:\n",
    "    dst.write(pipeline_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = parameters.copy()\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "    \n",
    "for i, row in subTileIndex.iterrows():\n",
    "    infile = row.originFile\n",
    "    file = os.path.basename(infile)\n",
    "    outlaz = os.path.join(ground_folder, file.replace(\".laz\", f\"_{row.subIndex}.laz\"))\n",
    "    outtiff = os.path.join(dtm_folder, file.replace(\".laz\", f\"_{row.subIndex}.tif\"))\n",
    "    inout_files[infile] = outfile\n",
    "\n",
    "            \n",
    "    t1=datetime.now()\n",
    "    \n",
    "    #json_pipe = createDTM(infile, outfile, subset_bounds, parameters, overwrite=False)\n",
    "    if not os.path.exists(outfile):\n",
    "        try:\n",
    "            print(f\"Starting on {outfile} at {t1}\")\n",
    "            json_pipes = Parallel(n_jobs=5, verbose=20, backend=\"loky\", max_nbytes=None)(delayed(\n",
    "                createDTM)(infile, outfile, subbounds, i, parameters, overwrite=False) for i, subbounds in enumerate(subset_bounds))\n",
    "            t2=datetime.now()\n",
    "            print(f\"Created file {outfile} in {t2-t1}\")\n",
    "        except:\n",
    "            print(f\"Failed for {outfile}. Error\")\n",
    "            json_pipes = Parallel(n_jobs=2, verbose=20, backend=\"loky\", max_nbytes=None)(delayed(\n",
    "                createDTM)(infile, outfile, subbounds, i, parameters, overwrite=False) for i, subbounds in enumerate(subset_bounds))\n",
    "            t2=datetime.now()\n",
    "            print(f\"Created file {outfile} in {t2-t1}\")\n",
    "           \n",
    "    \n",
    "    \n",
    "print(\"\\n\\n...FINISHED\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4/25 of a tile at window 30 uses up to 85% - tile took 25.5min, cell = 1\n",
    "- 4/25 of a tile at window 10 uses up to 85% - tile took 19.3min, cell = 1\n",
    "- 4/25 of a tile at window 10 uses up to 85% - tile took 22min, cell = 1\n",
    "\n",
    "- 5/25 of a tile at window 30 uses up to 85% - tile took 7.5 min, cell = 2.62\n",
    "- 2/4 of tile at window 30 uses up to 50% - tile took 4.9min, cell = 2.62\n",
    "- 3/4 of tile at window 30 uses up to 78% - tile took 4.8min, cell = 2.62\n",
    "- 4/8 of tile at window 30 uses up to 78% - tile took 3.7min, cell = 2.62\n",
    "- 5/9 of tile at window 30 uses up to 83% - tile took 3.6min, cell = 2.62\n",
    "- 5/10 of tile at window 30 uses up to 90% - tile took 3.5min, cell = 2.62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files = [\"0300\", \"0219\", \"0218\", \"0025\", \"0024\", \"0022\"]\n",
    "\n",
    "crop_pipe = \"\"\"{ \"pipeline\": [ \n",
    "    \"%s\",\n",
    "\t{\n",
    "        \"type\":\"filters.crop\",\n",
    "        \"bounds\":\"([979919.82, 990079.75],[499900.12, 510060.05])\"\n",
    "    },\n",
    "    {\n",
    "        \"type\":\"writers.las\",\n",
    "        \"compression\":\"laszip\",\n",
    "        \"filename\":\"./ground/%s\"\n",
    "    }   \n",
    " ] }\"\"\"\n",
    "merge_pipe = \"\"\"{ \"pipeline\": [\n",
    "    %s\n",
    "    {\n",
    "        \"type\":\"filters.merge\"\n",
    "    },\n",
    "    {\n",
    "        \"type\":\"filters.outlier\",\n",
    "        \"method\":\"statistical\",\n",
    "        \"mean_k\":12,\n",
    "        \"multiplier\":2.2\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"type\":\"filters.range\",\n",
    "        \"limits\":\"Classification[:6], Classification[8:]\"\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"filters.smrf\",\n",
    "        \"cell\": 25,\n",
    "        \"dir\": \"P:/GISLibrary/OrthoPhotos/Ortho2017/UrbanExtent_15cm/Elevation/RGB/temp\",\n",
    "        \"returns\":\"first,last,intermediate,only\",\n",
    "        \"ignore\": \"Classification[7:7]\",\n",
    "        \"scalar\": 1.0,\n",
    "        \"slope\": 0.18,\n",
    "        \"threshold\": 1.5,\n",
    "        \"window\": 30     \n",
    "    },\n",
    "    {\n",
    "        \"type\":\"filters.range\",\n",
    "        \"limits\":\"Classification[2:2]\"\n",
    "    },\n",
    "    {\n",
    "        \"type\":\"writers.las\",\n",
    "        \"filename\": \"%s\",\n",
    "        \"compression\":\"laszip\"\n",
    "    }\n",
    " ] }\"\"\"\n",
    "\n",
    "ofiles = \"\"\n",
    "for f in raw_files:\n",
    "    fname =  \"St_\" + f + \".las\"\n",
    "    fpath = os.path.join(r\"G:\\GISLibrary\\OrthoPhotos\\Ortho2015\\Final_Shipped_USGS_Feb2016\\QL2\\Raw_Point_Cloud_16bit\", fname)\n",
    "    opath = os.path.join(r\"P:\\GISLibrary\\OrthoPhotos\\Ortho2017\\UrbanExtent_15cm\\Elevation\\RGB\\ground\", fname.replace(\".las\",\".laz\"))\n",
    "    pipeline_json = crop_pipe % (fpath.replace(\"\\\\\",\"/\"), opath.replace(\"\\\\\", \"/\"))\n",
    "    #print(pipeline_json)\n",
    "    \n",
    "    pipeline = pdal.Pipeline(pipeline_json)\n",
    "    pipeline.loglevel = 8\n",
    "    start = datetime.now()\n",
    "    #pipeline.execute()\n",
    "    end = datetime.now()\n",
    "    print(f\"{end} \\t- Crop of {fname} done in {end-start}\")\n",
    "    ofiles += '\"' + opath.replace('\\\\','/') + '\", '\n",
    "            \n",
    "        \n",
    "ofiles = ofiles[:-1]\n",
    "ofile = r\"P:/GISLibrary/OrthoPhotos/Ortho2017/UrbanExtent_15cm/Elevation/RGB/ground/2015QL2UnClassified_985503.laz\"\n",
    "pipeline_json = merge_pipe % (ofiles, ofile)\n",
    "\n",
    "pipeline = pdal.Pipeline(pipeline_json)\n",
    "start = datetime.now()\n",
    "pipeline.execute()\n",
    "end = datetime.now()\n",
    "print(f\"{end} \\t- Merge of files done in {end-start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_pipe = \"\"\"{ \"pipeline\": [\n",
    "    \"%s\",\n",
    "    {\n",
    "        \"type\": \"filters.smrf\",\n",
    "        \"cell\": 25,\n",
    "        \"dir\": \"P:/GISLibrary/OrthoPhotos/Ortho2017/UrbanExtent_15cm/Elevation/RGB/temp\",\n",
    "        \"returns\":\"last,only\",\n",
    "        \"ignore\": \"Classification[7:7]\",\n",
    "        \"scalar\": 1.0,\n",
    "        \"slope\": 0.15,\n",
    "        \"threshold\": 1,\n",
    "        \"window\": 30     \n",
    "    },\n",
    "    {\n",
    "        \"type\":\"filters.range\",\n",
    "        \"limits\":\"Classification[2:2]\"\n",
    "    },\n",
    "    {\n",
    "        \"type\":\"writers.las\",\n",
    "        \"filename\": \"%s\",\n",
    "        \"compression\":\"laszip\"\n",
    "    }\n",
    " ] }\"\"\"\n",
    "\n",
    "ifile = r\"P:/GISLibrary/OrthoPhotos/Ortho2017/UrbanExtent_15cm/Elevation/RGB/ground/2015QL2UnClassified_985503.laz\"\n",
    "ofile = r\"P:/GISLibrary/OrthoPhotos/Ortho2017/UrbanExtent_15cm/Elevation/RGB/ground/2015QL2UnClassified_SMRF_985503.laz\"\n",
    "pipeline_json = merge_pipe % (ifile, ofile)\n",
    "\n",
    "pipeline = pdal.Pipeline(pipeline_json)\n",
    "start = datetime.now()\n",
    "pipeline.execute()\n",
    "end = datetime.now()\n",
    "print(f\"{end} \\t- Merge of files done in {end-start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pdal.Pipeline(info_pipe % (inlas, x, y, 1))\n",
    "pipeline.loglevel = 1\n",
    "start = datetime.now()\n",
    "pipeline.execute()\n",
    "end = datetime.now()\n",
    "md_dict = json.loads(pipeline.metadata)\n",
    "elevation = md_dict['metadata']['filters.info']['points']['point']['Z']\n",
    "print(f\"{elevation}\\t-\\tFinished - {end-start} elapsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(pipeline.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
