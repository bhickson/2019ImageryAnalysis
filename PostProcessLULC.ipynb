{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup/PostProcess LULC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext chime\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "import chime\n",
    "\n",
    "import rasterio as rio\n",
    "from rasterio.mask import mask\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from skimage.morphology import binary_closing, binary_opening, square, remove_small_holes, disk, square, remove_small_objects\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "import gdal\n",
    "\n",
    "from skimage.filters.rank import modal, mean_bilateral\n",
    "from datetime import datetime, time\n",
    "from time import sleep\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import nbimporter\n",
    "#from KeyFunctions import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "### Cleanup\n",
    "1. Smooth out roads. Binary closing 2-3 times with square selem\n",
    "1. Remove dangles from structures. Binary opening 2-3 times with square selem\n",
    "1. Fill holes in strucure class?\n",
    "1. Fill holes in asphault class?\n",
    "1. Fill holes in impervious class?\n",
    "1. FIll holes in pool class?\n",
    "1. Within x distance of roads, replace all non-ashault or non-dense veg with asphault\n",
    "1. ID shadows?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothValue(array, value, selem=square(5)):\n",
    "    onlys = array == value\n",
    "    for i in range(2):\n",
    "        onlys = binary_closing(onlys, selem)\n",
    "    array[onlys] = value\n",
    "    #array[~asphault] = 255\n",
    "    return array\n",
    "\n",
    "\n",
    "def smoothStructures(array):\n",
    "    structure = array == 9\n",
    "    array[structure] = 255\n",
    "    for i in range(3):\n",
    "        structure = binary_opening(structure, square(3))\n",
    "        structure = binary_closing(structure, square(5))\n",
    "    \n",
    "    array[structure] = 9\n",
    "    array = fillHolesInClass(array, 9, 90)\n",
    "    array = np.where(structure==1, 9, array)\n",
    "    \n",
    "    #for i in range(5):\n",
    "    #    print(f\"{i} iteration\"\n",
    "    #non_structure_mode = modal(array, square(21), mask=array==9)\n",
    "    #array = np.where(array==255, non_structure_mode, array)\n",
    "\n",
    "    return array\n",
    "\n",
    "\n",
    "def fillHolesInClass(array, class_num, size_max):\n",
    "    class_bool = array == class_num\n",
    "    array[class_bool] = 255\n",
    "    filled = remove_small_holes(class_bool, area_threshold=size_max, connectivity=1)\n",
    "    array[filled] = class_num\n",
    "    return array\n",
    "    \n",
    "\n",
    "def removeClassSmaller(array, class_num, min_size):\n",
    "    non = np.where(array==class_num, 255, array)\n",
    "    removed = remove_small_objects(array==class_num, min_size=min_size, connectivity=1)\n",
    "    array_rem = np.where(removed==0, non, array)\n",
    "    return array_rem\n",
    "\n",
    "\n",
    "def getGeometryMask(geometry, raster):\n",
    "    if geometry.type == \"Polygon\":\n",
    "        geometry = [geometry]\n",
    "    tds, tds_trans = mask(raster, geometry, all_touched=False, crop=False, filled=False)\n",
    "    \n",
    "    return ~tds.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanClassifiedFile(path, row, lulc_dir, ortho_dir, overwrite=False):\n",
    "    try:\n",
    "        lulc_file = findFile(path,row, lulc_dir)\n",
    "        ortho_file = findFile(path, row, ortho_dir)\n",
    "    except:\n",
    "        print(f\"Couldn't find necessary files for {path}_{row}\")\n",
    "        return None\n",
    "    \n",
    "    postProcess_dir = r\"../EPCExtent_30cm/Orthos_Segmented_Classified_cleaned\"\n",
    "    postProcess_dir = lulc_dir + \"_cleaned\"\n",
    "    os.makedirs(postProcess_dir, exist_ok=True)\n",
    "    \n",
    "    print(postProcess_dir)\n",
    "    \n",
    "    ofile = os.path.join(postProcess_dir, os.path.basename(lulc_file).replace(\".tif\",\"_cleaned.tif\"))\n",
    "    if os.path.exists(ofile) and not overwrite:\n",
    "        return ofile\n",
    "    \n",
    "    with rio.open(lulc_file) as src:\n",
    "        lulc = src.read(1)\n",
    "        kwargs = src.profile\n",
    "        bnds = src.bounds\n",
    "        railroad_mask = getGeometryMask(railroads, src)[0]\n",
    "        washes_mask = getGeometryMask(washes, src)[0]\n",
    "        pondsLakes_mask = getGeometryMask(pondsLakes, src)[0]\n",
    "        #pavedRoads_mask = getGeometryMask(pavedRoads, src)[0]\n",
    "\n",
    "    with rio.open(ortho_file) as src:\n",
    "        \n",
    "        descs = src.descriptions\n",
    "        data = src.read()\n",
    "\n",
    "        bands = {desc:data[ib] for ib, desc in enumerate(descs)}\n",
    "\n",
    "        #This is a training dataset not created with the others, but may be in the classifier. Create and add\n",
    "        if \"RGBNmean\" not in descs:\n",
    "            print(\"Adding RGBNmean\")\n",
    "            rgbnMean = np.nanmean(data[:4], axis=0).astype(data.dtype)\n",
    "            bands[\"RGBNmean\"] = rgbnMean\n",
    "\n",
    "        #features = {fn:bands[fn] for fn in feature_names}# if fn in descs}\n",
    "\n",
    "    \n",
    "    kwargs.update(compress=\"lzw\", nodata=255)\n",
    "    \n",
    "    with rio.open(ofile, \"w\", **kwargs) as dst:\n",
    "        out = lulc.copy()\n",
    "        #out[((out==4) | (out==5)) & (hag>=6)] == 5\n",
    "        out[(out==9) & (bands[\"HAG\"]<5)] = 8 # if classified as structure, but less than 5 feet high, reclass to impervious\n",
    "        #out[(redness>=20) & (greenness<=12) & (blueness<=5) & (lulc!=9) & (roads_distance<=3)] = 3 # good for red bare earth\n",
    "        #out[(out==2) & (blueness<45) & (nirness>-70)] = 255 # remove pools that aren't very blue AND does not have low NIRness value\n",
    "        #out[(out==2) & (ndpi < 0.25)] = 255 # if classified as pool but low pool index, set to 255\n",
    "        #out[(out==1) & (ndpi < 0.5)] = 2 # if classified as pond/Lake, but high pool index, classify to pool\n",
    "        #out[(out==255) & (msavi>=115) & (greenness>=12) & (mean_vals<=75) ] = 6 # set very green veg with high index and low brightness to irrigated\n",
    "        \n",
    "        out[(bands[\"REDness\"]>=35336) & (bands[\"GREENness\"]<=34307) & (bands[\"BLUEness\"]<=33409) & (lulc!=9) & (bands[\"DPR\"]<=3)] = 3 # good for red bare earth\n",
    "        out[(out==33023) & (bands[\"BLUEness\"]<38547) & (bands[\"NIRness\"]>23769)] = 255 # remove pools that aren't very blue AND does not have low NIRness value\n",
    "        out[(out==2) & (bands[\"NDPI\"] < 40959)] = 255 # if classified as pool but low pool index, set to 255\n",
    "        out[(out==1) & (bands[\"NDPI\"] < 49151)] = 2 # if classified as pond/Lake, but high pool index, classify to pool\n",
    "    \n",
    "        \n",
    "        out[(out==255) & (bands[\"MSAVI\"]>=29556) & (bands[\"GREENness\"]>=34307) & (bands[\"RGBNmean\"]<=19273) ] = 6 # set very green veg with high index and low brightness to irrigated\n",
    "        \n",
    "        #if sparse veg but in road, reclass to dense veg (tree overhang)\n",
    "        out[(bands[\"DPR\"]<2) & (out==4)] = 5\n",
    "        \n",
    "        #burn in roads as asphault where not classified as dense veg (tree's overhang)\n",
    "        out[(bands[\"DPR\"]==0) & (lulc!=5)] = 7\n",
    "        \n",
    "        # burn in shadows where low values, but not pool or pond\n",
    "        #out[(mean_vals<40) & (out!=1) & (lulc!=2)] = 10\n",
    "        out[(bands[\"RGBNmean\"]<10282) & (out!=1) & (lulc!=2)] = 10\n",
    "               \n",
    "        #burn in Major Wash polygons\n",
    "        out[(washes_mask) & (out == 8)] = 3\n",
    "        \n",
    "        # burn in railroads where not structure or road\n",
    "        out[(railroad_mask) & (out != 9) & (out != 7)] = 3\n",
    "        \n",
    "        # burn in known pondsLakes that have been set to no data (255)\n",
    "        out[pondsLakes_mask] = 1\n",
    "        \n",
    "        # if distance from roads greater than (100?ft), all impervious and asphault to barren\n",
    "        out[(out==8) & (bands[\"DPR\"]>100)] = 3\n",
    "        out[(out==7) & (bands[\"DPR\"]>100)] = 3\n",
    "        \n",
    "        # working top high (trees) to ground\n",
    "        out = smoothValue(out, 5, selem=disk(2)) # smooth dense veg (ideally trees)\n",
    "        out = smoothStructures(out) # smooth structures\n",
    "        out = smoothValue(out, 4, selem=disk(2)) # smooth sparse veg\n",
    "        out = smoothValue(out, 7, selem=square(5)) # smooth asphault\n",
    "        # fill holes in asphault smaller than 150?\n",
    "        out = fillHolesInClass(out, 7, 150)\n",
    "        out = smoothValue(out, 8, selem=square(5)) # smooth concrete/impervious\n",
    "        out = smoothValue(out, 3, selem=square(5)) # smooth bare ground\n",
    "        out = smoothValue(out, 6, selem=square(5)) # smooth sparse veg\n",
    "        out = smoothValue(out, 2, selem=disk(2)) # smooth pools\n",
    "        # drop ponds/lakes smaller than x?\n",
    "        out = removeClassSmaller(out, 1, 10000)\n",
    "        out = smoothValue(out, 1, selem=disk(5)) # smooth ponds\n",
    "        \n",
    "        out = smoothValue(out, 5, selem=disk(2)) # smooth dense veg\n",
    "\n",
    "        if out.max() == 0:\n",
    "            print(f\"Bad output for {ofile}\")\n",
    "            return ofile\n",
    "\n",
    "        # asphault smaller than x gets changed to impervious?\n",
    "\n",
    "        #use modal or majority (?) rank filter to fill in nodata (255) values. Exclude structures (9), ponds/lakes (1), Use cross selem to get more angular fille\n",
    "        \n",
    "        dst.write(out, 1)\n",
    "        \n",
    "        colors = {\n",
    "            1: (12,42,235, 255),\n",
    "            2: (41, 210, 219,255),\n",
    "            3: (255, 214, 117, 255),\n",
    "            4: (171, 224, 85, 255),\n",
    "            5: (12, 100, 1, 255),\n",
    "            6: (0, 192, 32, 255),\n",
    "            7: (62, 62, 62, 255),\n",
    "            8: (160, 160, 160, 255),\n",
    "            9: (160, 37, 6, 255),\n",
    "            10: (0, 0, 0, 255),\n",
    "            255: (255,1,166, 255)\n",
    "        }\n",
    "        \n",
    "        dst.write_colormap(1, colors)\n",
    "        \n",
    "        print(f\"Wrote out to {ofile}\")\n",
    "\n",
    "    return ofile  \n",
    "    %chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BenJames\\anaconda3\\envs\\geospatial\\lib\\site-packages\\geopandas\\geodataframe.py:294: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for f in features_lst:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "orthos_loc = \"../EPCExtent_30cm/Orthos_Segmentedv3\"\n",
    "lulcs_loc = \"../EPCExtent_30cm/Orthos_Segmented_Classifiedv3\"\n",
    "\n",
    "\n",
    "lulc_finished = glob(lulcs_loc + \"/*.tif\")\n",
    "paths_and_rows = [os.path.basename(file).split(\"_\")[:2] for file in lulc_finished]\n",
    "\n",
    "railroads = gpd.read_file(\"../OtherData/PC_Railroad/railroad.shp\").unary_union\n",
    "railroads = railroads.buffer(20)\n",
    "washes = gpd.read_file(\"../OtherData/Major_Washes_in_Eastern_Pima_County/Major_Washes_in_Eastern_Pima_County.shp\").unary_union\n",
    "pondsLakes = gpd.read_file(\"../OtherData/TrainingData/pondsLakesPolys.gpkg\").unary_union\n",
    "%chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#pavedRoads = gpd.read_file(\"../OtherData/OSMRoadsPimaPaved_20201124.gpkg\").buffer(10).unary_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../EPCExtent_30cm/Orthos_Segmentedv3_cleaned\n",
      "Adding RGBNmean\n",
      "Wrote out to ../EPCExtent_30cm/Orthos_Segmentedv3_cleaned\\W1004789_W449850_TrainingStackV3_classLGBNewishGBLM_cleaned.tif\n",
      "../EPCExtent_30cm/Orthos_Segmentedv3_cleaned\n",
      "Adding RGBNmean\n",
      "Wrote out to ../EPCExtent_30cm/Orthos_Segmentedv3_cleaned\\W989789_W439850_TrainingStackV3_classLGBNewishGBLM_cleaned.tif\n"
     ]
    }
   ],
   "source": [
    "for pr in paths_and_rows:\n",
    "    cleanClassifiedFile(path=pr[0], row=pr[1], lulc_dir=lulcs_loc, ortho_dir=orthos_loc, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 files to process\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   1 out of   1 | elapsed:    2.0s finished\n"
     ]
    }
   ],
   "source": [
    "cleaned_tifs = Parallel(n_jobs=8, verbose=5)(delayed(cleanClassifiedFile)(path=pr[0], row=pr[1], lulc_dir=lulcs_loc, ortho_dir=orthos_loc, overwrite=False) for pr in paths_and_rows)\n",
    "%chime\n",
    "\n",
    "print(f\"{len(paths_and_rows)} files to process\")\n",
    "\n",
    "cleaned_tifs = [f for f in cleaned_tifs if f != None]\n",
    "print(cleaned_tifs)\n",
    "\n",
    "#vrt = buildVRT(lulcs_loc, \"EPC_30cmOrthoSegmented_Classified_cleaned.vrt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BenJames\\anaconda3\\envs\\geospatial\\lib\\site-packages\\geopandas\\geodataframe.py:294: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for f in features_lst:\n"
     ]
    }
   ],
   "source": [
    "import shapely\n",
    "from shapely.geometry import *\n",
    "import pandas as pd\n",
    "\n",
    "tindex = gpd.read_file(\"../EPCExtent_30cm/Ortho_5kSubIndex.gpkg\")\n",
    "\n",
    "tindex[\"LULCFile\"] = tindex.apply(lambda r: findFile(path=r.path, row=r.row, directory=\"../EPCExtent_30cm/Orthos_Segmented_Classified\"), axis=1)\n",
    "\n",
    "# ignore tiles which don't have input variables created\n",
    "tindex = tindex[(~pd.isnull(tindex.LULCFile))]\n",
    "\n",
    "#prioritize central tucson and work out from there\n",
    "tindex[\"centroid\"] = tindex.geometry.centroid\n",
    "central_tile = tindex[(tindex.path == \"W1004789\") & (tindex.row == \"W449850\")]\n",
    "central_point = central_tile.centroid.values[0]\n",
    "\n",
    "tindex[\"DistToCenter\"] = tindex.centroid.apply(lambda c: int(c.distance(central_point)))\n",
    "tindex.sort_values(by=\"DistToCenter\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed: 16.0min\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed: 27.4min\n",
      "[Parallel(n_jobs=8)]: Done 632 tasks      | elapsed: 42.2min\n",
      "[Parallel(n_jobs=8)]: Done 866 tasks      | elapsed: 59.6min\n",
      "[Parallel(n_jobs=8)]: Done 1136 tasks      | elapsed: 80.7min\n",
      "[Parallel(n_jobs=8)]: Done 1442 tasks      | elapsed: 104.1min\n",
      "[Parallel(n_jobs=8)]: Done 1784 tasks      | elapsed: 132.6min\n",
      "[Parallel(n_jobs=8)]: Done 2005 out of 2005 | elapsed: 150.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4011 files to process\n",
      "Created EPC_30cmOrthoSegmented_Classified_cleaned.vrt\n"
     ]
    }
   ],
   "source": [
    "target_files = tindex.iloc[0:int(len(tindex)/2)]\n",
    "target_files.shape\n",
    "cleaned_tifs = Parallel(n_jobs=8, verbose=5)(delayed(cleanClassifiedFile)(path=row.path, row=row.row, lulc_dir=lulcs_loc, ortho_dir=orthos_loc, overwrite=False) for i, row in target_files.iterrows())\n",
    "%chime\n",
    "\n",
    "print(f\"{len(paths_and_rows)} files to process\")\n",
    "\n",
    "cleaned_tifs = [f for f in cleaned_tifs if f != None]\n",
    "outVRT = os.path.join(\"../EPCExtent_30cm/Orthos_Segmented_Classified_cleaned\", \"EPC_30cmOrthoSegmented_Classified_cleaned.vrt\")\n",
    "vrt = gdal.BuildVRT(outVRT, cleaned_tifs)\n",
    "del vrt\n",
    "print(f\"Created {outVRT}\")\n",
    "finished()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:   46.7s\n",
      "[Parallel(n_jobs=4)]: Done 280 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done 640 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=4)]: Done 874 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=4)]: Done 1144 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=4)]: Done 1450 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=4)]: Done 1792 tasks      | elapsed:  9.0min\n",
      "C:\\Users\\BenJames\\anaconda3\\envs\\geospatial\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:703: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=4)]: Done 2170 tasks      | elapsed: 32.4min\n",
      "[Parallel(n_jobs=4)]: Done 2584 tasks      | elapsed: 90.7min\n",
      "[Parallel(n_jobs=4)]: Done 3034 tasks      | elapsed: 148.8min\n",
      "[Parallel(n_jobs=4)]: Done 3520 tasks      | elapsed: 216.6min\n",
      "[Parallel(n_jobs=4)]: Done 4011 out of 4011 | elapsed: 286.4min finished\n"
     ]
    }
   ],
   "source": [
    "cleaned_tifs += Parallel(n_jobs=4, verbose=5)(delayed(cleanClassifiedFile)(path=row.path, row=row.row, lulc_dir=lulcs_loc, ortho_dir=orthos_loc, overwrite=False) for i, row in tindex.iterrows())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
